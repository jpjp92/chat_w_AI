# utils/webpage_analyzer.py
from config.imports import *
import re
import logging
import xml.etree.ElementTree as ET  # ëˆ„ë½ëœ import ì¶”ê°€
from urllib.parse import urlparse
import requests
from bs4 import BeautifulSoup

# PyPDF2 ì¡°ê±´ë¶€ import
try:
    import PyPDF2
    PDF_AVAILABLE = True
except ImportError:
    PDF_AVAILABLE = False
    print("Warning: PyPDF2ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. PDF íŒŒì¼ ì²˜ë¦¬ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")

logger = logging.getLogger(__name__)  # ë‹«ëŠ” ê´„í˜¸ ì¶”ê°€

def fetch_pdf_content(url):
    """PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    if not PDF_AVAILABLE:
        return f"PDF ì²˜ë¦¬ ê¸°ëŠ¥ì´ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. HTML ë²„ì „ì´ ìˆë‹¤ë©´ í•´ë‹¹ ë§í¬ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”."
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        # PDF ë‚´ìš© ì½ê¸°
        import io
        pdf_file = io.BytesIO(response.content)
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        
        text = ""
        # ì²« 2-3 í˜ì´ì§€ë§Œ ì½ê¸° (Abstract, Introduction ë¶€ë¶„)
        max_pages = min(3, len(pdf_reader.pages))
        
        for page_num in range(max_pages):
            page = pdf_reader.pages[page_num]
            text += page.extract_text() + "\n"
        
        return text[:5000]  # ì²˜ìŒ 5000ìë§Œ ë°˜í™˜
        
    except Exception as e:
        logger.error(f"PDF ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
        return f"PDF íŒŒì¼ì„ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}"

def is_pdf_url(url):
    """URLì´ PDF íŒŒì¼ì¸ì§€ í™•ì¸"""
    return url.lower().endswith('.pdf') or 'pdf' in url.lower()

def is_arxiv_url(url):
    """ArXiv URLì¸ì§€ í™•ì¸ (ë‹¤ì–‘í•œ í˜•íƒœ ì§€ì›)"""
    arxiv_patterns = [
        r'arxiv\.org',
        r'arxiv-export\.library\.cornell\.edu',
        r'export\.arxiv\.org'
    ]
    
    for pattern in arxiv_patterns:
        if re.search(pattern, url.lower()):
            return True
    return False

def is_pubmed_url(url):
    """PubMed URLì¸ì§€ í™•ì¸"""
    return 'pubmed.ncbi.nlm.nih.gov' in url.lower()

def fetch_arxiv_metadata(url):
    """ArXiv URLì—ì„œ ë©”íƒ€ë°ì´í„°ì™€ ì´ˆë¡ ì¶”ì¶œ"""
    try:
        # ArXiv ID ì¶”ì¶œ ë¡œì§ ê°œì„ 
        arxiv_id = url.split('/')[-1]
        
        # ë²„ì „ ì •ë³´ ì²˜ë¦¬ (v1, v2 ë“±)
        if 'v' in arxiv_id and arxiv_id.split('v')[-1].isdigit():
            arxiv_id = arxiv_id.split('v')[0]
        
        # ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±°
        arxiv_id = arxiv_id.replace('abs/', '').replace('pdf/', '').replace('.pdf', '')
        
        # ArXiv ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
        import arxiv
        search = arxiv.Search(id_list=[arxiv_id])
        
        try:
            paper = next(search.results())
        except StopIteration:
            return f"âŒ ArXiv ë…¼ë¬¸ {arxiv_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. IDë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
        
        # ì¹´í…Œê³ ë¦¬ ì •ë³´ ì¶”ê°€
        categories = ', '.join(paper.categories) if paper.categories else "ë¯¸ë¶„ë¥˜"
        
        # ì €ì ì •ë³´ (ìµœëŒ€ 5ëª…ê¹Œì§€ë§Œ í‘œì‹œ)
        authors = [str(a) for a in paper.authors[:5]]
        if len(paper.authors) > 5:
            authors.append(f"ì™¸ {len(paper.authors) - 5}ëª…")
        author_list = ', '.join(authors)
        
        # ì¶œíŒì¼ ì²˜ë¦¬
        pub_date = paper.published.strftime('%Yë…„ %mì›” %dì¼') if paper.published else "ë‚ ì§œ ì •ë³´ ì—†ìŒ"
        
        content = f"""ğŸ“„ **ArXiv ë…¼ë¬¸ ì •ë³´**

ğŸ”— **ë§í¬**: {url}
ğŸ“‹ **ArXiv ID**: {arxiv_id}
ğŸ“… **ì¶œíŒì¼**: {pub_date}
ğŸ“š **ì¹´í…Œê³ ë¦¬**: {categories}

ğŸ“– **ì œëª©**: 
{paper.title}

ğŸ‘¥ **ì €ì**: 
{author_list}

ğŸ“ **ì´ˆë¡**:
{paper.summary}

ğŸ’¡ **ì¶”ê°€ ì •ë³´**: ì´ ë…¼ë¬¸ì— ëŒ€í•´ ë” ìì„¸í•œ ì§ˆë¬¸ì„ í•˜ì‹œë©´ ì´ˆë¡ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•´ë“œë¦´ê²Œìš”!
        """
        
        return content.strip()
        
    except ImportError:
        return "âŒ ArXiv ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install arxivë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”."
    except Exception as e:
        logger.error(f"ArXiv ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
        return f"âŒ ArXiv ë…¼ë¬¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def fetch_pubmed_abstract(url):
    """PubMed URLì—ì„œ ì´ˆë¡ ì¶”ì¶œ"""
    try:
        # PMID ì¶”ì¶œ
        pmid = url.split('/')[-1].rstrip('/')
        
        # ìˆ«ìê°€ ì•„ë‹Œ ê²½ìš° ì²˜ë¦¬
        if not pmid.isdigit():
            return f"âŒ ìœ íš¨í•˜ì§€ ì•Šì€ PMIDì…ë‹ˆë‹¤: {pmid}"
        
        # PubMed API ì‚¬ìš©
        base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/"
        fetch_url = f"{base_url}efetch.fcgi"
        
        # ì´ˆë¡ ê°€ì ¸ì˜¤ê¸°
        params = {
            "db": "pubmed", 
            "id": pmid, 
            "retmode": "xml", 
            "rettype": "abstract"
        }
        
        response = requests.get(fetch_url, params=params, timeout=10)
        response.raise_for_status()
        
        # XML íŒŒì‹±í•´ì„œ ì´ˆë¡ ì¶”ì¶œ
        root = ET.fromstring(response.text)
        
        # ì œëª© ì¶”ì¶œ
        title_elem = root.find(".//ArticleTitle")
        title = title_elem.text if title_elem is not None else "ì œëª© ì—†ìŒ"
        
        # ì´ˆë¡ ì¶”ì¶œ (ì—¬ëŸ¬ AbstractTextê°€ ìˆì„ ìˆ˜ ìˆìŒ)
        abstract_elems = root.findall(".//AbstractText")
        if abstract_elems:
            abstract_parts = []
            for elem in abstract_elems:
                label = elem.get('Label', '')
                text = elem.text or ''
                if label:
                    abstract_parts.append(f"{label}: {text}")
                else:
                    abstract_parts.append(text)
            abstract = '\n'.join(abstract_parts)
        else:
            abstract = "ì´ˆë¡ ì—†ìŒ"
        
        # ì €ì ì •ë³´ ì¶”ì¶œ
        author_elems = root.findall(".//Author")
        authors = []
        for author in author_elems[:5]:  # ìµœëŒ€ 5ëª…ë§Œ
            last_name = author.findtext("LastName", "")
            first_name = author.findtext("ForeName", "")
            if last_name and first_name:
                authors.append(f"{first_name} {last_name}")
            elif last_name:
                authors.append(last_name)
        
        if len(author_elems) > 5:
            authors.append(f"ì™¸ {len(author_elems) - 5}ëª…")
        
        author_list = ', '.join(authors) if authors else "ì €ì ì •ë³´ ì—†ìŒ"
        
        content = f"""ğŸ“„ **PubMed ë…¼ë¬¸ ì •ë³´**

ğŸ”— **ë§í¬**: {url}
ğŸ“‹ **PMID**: {pmid}

ğŸ“– **ì œëª©**: 
{title}

ğŸ‘¥ **ì €ì**: 
{author_list}

ğŸ“ **ì´ˆë¡**:
{abstract}

ğŸ’¡ **ì¶”ê°€ ì •ë³´**: ì´ ë…¼ë¬¸ì— ëŒ€í•´ ë” ìì„¸í•œ ì§ˆë¬¸ì„ í•˜ì‹œë©´ ì´ˆë¡ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•´ë“œë¦´ê²Œìš”!
        """
        
        return content.strip()
        
    except requests.RequestException as e:
        logger.error(f"PubMed API ìš”ì²­ ì˜¤ë¥˜: {str(e)}")
        return f"âŒ PubMed API ìš”ì²­ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    except ET.ParseError as e:
        logger.error(f"XML íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
        return f"âŒ PubMed ì‘ë‹µ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    except Exception as e:
        logger.error(f"PubMed ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
        return f"âŒ PubMed ë…¼ë¬¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def fetch_webpage_content(url):
    """ì›¹í˜ì´ì§€ ë˜ëŠ” ë…¼ë¬¸ ë‚´ìš© ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)"""
    try:
        # URL ì •ê·œí™”
        url = normalize_arxiv_url(url)
        
        # ë…¼ë¬¸ URL íŠ¹ë³„ ì²˜ë¦¬
        if is_arxiv_url(url):
            content = fetch_arxiv_metadata(url)
            if content:
                return content
        
        if is_pubmed_url(url):
            content = fetch_pubmed_abstract(url)
            if content:
                return content
        
        # PDF íŒŒì¼ ì²˜ë¦¬ (PyPDF2ê°€ ìˆëŠ” ê²½ìš°ë§Œ)
        if is_pdf_url(url):
            content = fetch_pdf_content(url)
            if content:
                return content
        
        # ê¸°ì¡´ HTML ì²˜ë¦¬ ë¡œì§
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
        for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'advertisement']):
            tag.decompose()
        
        # ë©”ì¸ ì½˜í…ì¸  ì¶”ì¶œ (ìš°ì„ ìˆœìœ„ ìˆœ)
        main_content = (
            soup.find('main') or 
            soup.find('article') or 
            soup.find('div', class_=re.compile(r'content|main|post|article', re.I)) or
            soup.find('div', id=re.compile(r'content|main|post|article', re.I))
        )
        
        if main_content:
            text = main_content.get_text(strip=True, separator='\n')
        else:
            # ì „ì²´ bodyì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            body = soup.find('body')
            if body:
                text = body.get_text(strip=True, separator='\n')
            else:
                text = soup.get_text(strip=True, separator='\n')
        
        # í…ìŠ¤íŠ¸ ì •ë¦¬
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        clean_text = '\n'.join(lines)
        
        # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì¼ë¶€ë§Œ ë°˜í™˜
        if len(clean_text) > 8000:
            clean_text = clean_text[:8000] + "\n\n... (ë‚´ìš©ì´ ê¸¸ì–´ì„œ ì¼ë¶€ë§Œ í‘œì‹œë©ë‹ˆë‹¤)"
        
        return clean_text
        
    except requests.RequestException as e:
        logger.error(f"ì›¹í˜ì´ì§€ ìš”ì²­ ì˜¤ë¥˜: {str(e)}")
        return f"âŒ '{url}' ì›¹í˜ì´ì§€ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {str(e)}"
    except Exception as e:
        logger.error(f"ì›¹í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
        return f"âŒ '{url}' ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì˜¤ë¥˜: {str(e)}"

def normalize_arxiv_url(url):
    """ArXiv URLì„ ì •ê·œí™” (PDF -> Abstract)"""
    if is_arxiv_url(url):
        # PDF ë§í¬ì¸ ê²½ìš°, Abstract ë§í¬ë¡œ ë³€í™˜
        if '/pdf/' in url:
            url = url.replace('/pdf/', '/abs/')
        # .pdf í™•ì¥ì ì œê±°
        if url.endswith('.pdf'):
            url = url[:-4]
    
    return url

def summarize_webpage_content(url, user_query="", client=None):
    """ì›¹í˜ì´ì§€ ë‚´ìš©ì„ ìš”ì•½í•©ë‹ˆë‹¤"""
    try:
        content = fetch_webpage_content(url)
        
        # ì˜¤ë¥˜ ë©”ì‹œì§€ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ë°˜í™˜
        if content.startswith("âŒ"):
            return content
        
        # LLMì„ ì‚¬ìš©í•´ ë‚´ìš© ìš”ì•½
        if not client:
            from utils.providers import select_random_available_provider
            client, _ = select_random_available_provider()
        
        prompt = f"""ë‹¤ìŒ ì›¹í˜ì´ì§€ì˜ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.

ì›¹í˜ì´ì§€ URL: {url}
ì‚¬ìš©ì ì§ˆë¬¸: {user_query if user_query else "ì „ì²´ ë‚´ìš© ìš”ì•½"}

ì›¹í˜ì´ì§€ ë‚´ìš©:
{content}

ìš”ì•½ ì§€ì¹¨:
1. ì£¼ìš” í•µì‹¬ ë‚´ìš©ì„ 3-5ê°œ í¬ì¸íŠ¸ë¡œ ì •ë¦¬
2. ì¤‘ìš”í•œ ì •ë³´ë‚˜ ìˆ˜ì¹˜ê°€ ìˆë‹¤ë©´ í¬í•¨
3. ì‚¬ìš©ìê°€ íŠ¹ì • ì§ˆë¬¸ì„ í–ˆë‹¤ë©´ ê·¸ì— ë§ì¶° ìš”ì•½
4. ì´ëª¨ì§€ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ì—¬ ê°€ë…ì„± í–¥ìƒ
5. ì¶œì²˜ URLë„ í•¨ê»˜ ì œê³µ

í˜•ì‹:
ğŸ“„ **ì›¹í˜ì´ì§€ ìš”ì•½**

ğŸ”— **ì¶œì²˜**: {url}

ğŸ“ **ì£¼ìš” ë‚´ìš©**:
- í•µì‹¬ í¬ì¸íŠ¸ 1
- í•µì‹¬ í¬ì¸íŠ¸ 2
- í•µì‹¬ í¬ì¸íŠ¸ 3

ğŸ’¡ **ê²°ë¡ **: ê°„ë‹¨í•œ ê²°ë¡ ì´ë‚˜ í•µì‹¬ ë©”ì‹œì§€
"""

        try:
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì›¹í˜ì´ì§€ ë‚´ìš©ì„ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1500,
                temperature=0.3
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"LLM ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return f"âŒ ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        
    except Exception as e:
        logger.error(f"ì›¹í˜ì´ì§€ ìš”ì•½ ì˜¤ë¥˜: {str(e)}")
        return f"âŒ ì›¹í˜ì´ì§€ ìš”ì•½ ì¤‘ ì˜¤ë¥˜: {str(e)}"

def extract_urls_from_text(text):
    """í…ìŠ¤íŠ¸ì—ì„œ URLì„ ì¶”ì¶œí•©ë‹ˆë‹¤"""
    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
    urls = re.findall(url_pattern, text)
    return urls

def is_url_summarization_request(query):
    """URL ìš”ì•½ ìš”ì²­ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤"""
    urls = extract_urls_from_text(query)
    if urls:
        summary_keywords = ['ìš”ì•½', 'ì •ë¦¬', 'ë‚´ìš©', 'ì„¤ëª…', 'ì•Œë ¤ì¤˜', 'ë¶„ì„', 'í•´ì„', 'ë¦¬ë·°', 'ì •ë³´']
        for keyword in summary_keywords:
            if keyword in query:
                return True, urls[0]  # ì²« ë²ˆì§¸ URL ë°˜í™˜
    return False, None

def is_numbered_link_request(query, search_context):
    """ìˆœì„œ ê¸°ë°˜ ë§í¬ ìš”ì²­ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤ (ì˜ˆ: 3ë²ˆì§¸ ë§í¬ ìš”ì•½í•´ì¤˜)"""
    if not search_context or search_context.get("type") != "naver_search":
        return False, None
    
    # ìˆ«ì íŒ¨í„´ê³¼ í•œê¸€ íŒ¨í„´ì„ ë¶„ë¦¬
    number_patterns = [
        r'(\d+)ë²ˆì§¸\s*(?:ë§í¬|ê²°ê³¼|ì‚¬ì´íŠ¸)',
        r'(\d+)ë²ˆ\s*(?:ë§í¬|ê²°ê³¼|ì‚¬ì´íŠ¸)', 
        r'(\d+)\.?\s*(?:ë§í¬|ê²°ê³¼|ì‚¬ì´íŠ¸)',
    ]
    
    korean_patterns = [
        r'ì²«\s*ë²ˆì§¸\s*(?:ë§í¬|ê²°ê³¼|ì‚¬ì´íŠ¸)',
        r'ë‘\s*ë²ˆì§¸\s*(?:ë§í¬|ê²°ê³¼|ì‚¬ì´íŠ¸)',
        r'ì„¸\s*ë²ˆì§¸\s*(?:ë§í¬|ê²°ê³¼|ì‚¬ì´íŠ¸)',
        r'ë„¤\s*ë²ˆì§¸\s*(?:ë§í¬|ê²°ê³¼|ì‚¬ì´íŠ¸)',
        r'ë‹¤ì„¯\s*ë²ˆì§¸\s*(?:ë§í¬|ê²°ê³¼|ì‚¬ì´íŠ¸)'
    ]
    
    # ìˆ«ìë¥¼ í•œê¸€ë¡œ ë³€í™˜
    korean_numbers = {'ì²«': 1, 'ë‘': 2, 'ì„¸': 3, 'ë„¤': 4, 'ë‹¤ì„¯': 5}
    
    number = None
    
    # ìˆ«ì íŒ¨í„´ í™•ì¸
    for pattern in number_patterns:
        match = re.search(pattern, query, re.IGNORECASE)
        if match:
            number = int(match.group(1))
            break
    
    # í•œê¸€ ìˆ«ì íŒ¨í„´ í™•ì¸
    if number is None:
        for pattern in korean_patterns:
            match = re.search(pattern, query, re.IGNORECASE)
            if match:
                # í•œê¸€ ìˆ«ì ì²˜ë¦¬
                for korean, num in korean_numbers.items():
                    if korean in query:
                        number = num
                        break
                if number:
                    break
    
    if number is not None:
        # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ í•´ë‹¹ ìˆœì„œì˜ URL ì¶”ì¶œ
        search_result = search_context.get("result", "")
        urls = extract_urls_from_text(search_result)
        
        if urls and len(urls) >= number:
            summary_keywords = ['ìš”ì•½', 'ì •ë¦¬', 'ë‚´ìš©', 'ì„¤ëª…', 'ì•Œë ¤ì¤˜', 'ë¶„ì„']
            if any(keyword in query for keyword in summary_keywords):
                return True, urls[number - 1]  # 0ë¶€í„° ì‹œì‘í•˜ë¯€ë¡œ -1
    
    return False, None

def is_followup_question(query):
    """í›„ì† ì§ˆë¬¸ì¸ì§€ í™•ì¸í•˜ê³ , ì»¨í…ìŠ¤íŠ¸ë¥¼ ìœ ì§€í•´ì•¼ í•˜ëŠ”ì§€ ê²°ì •í•©ë‹ˆë‹¤."""
    
    # í›„ì† ì§ˆë¬¸ íŒ¨í„´
    followup_patterns = [
        r'ì´ì— ëŒ€í•´|ì´ê²ƒì— ëŒ€í•´|ê´€ë ¨í•´ì„œ|ë” ìì„¸íˆ|ìš”ì•½í•´?ì¤˜',
        r'ì„¤ëª…í•´?ì¤˜|ì•Œë ¤?ì¤˜|ì–´ë–¤|ì™œ|ì´ìœ |ë­ì•¼|ë­ì§€|ë­ì„',
        r'ì´ê²Œ ë¬´ìŠ¨|ì´ê±´ ë¬´ìŠ¨|ë¬´ìŠ¨ ì˜ë¯¸|ì˜ë¯¸ê°€ ë­|ì²« ë²ˆì§¸|ë‘ ë²ˆì§¸|ì„¸ ë²ˆì§¸',
        r'ë‹¤ì‹œ ì„¤ëª…|ë‹¤ì‹œ ì•Œë ¤ì¤˜|í•œ ë²ˆ ë”|ë” ì•Œë ¤ì¤˜|ì¶”ê°€ ì •ë³´|ì¶”ê°€ë¡œ|êµ¬ì²´ì ',
        r'ê°™ì€ ì£¼ì œ|ê³„ì†|ê·¸ë¦¬ê³ |ê·¸ ë‹¤ìŒ|ì¶”ê°€ ì§ˆë¬¸|ì—°ê´€ëœ',
        r'ë§í¬|ì‚¬ì´íŠ¸|ì›¹í˜ì´ì§€|ì´ ì£¼ì†Œ|url'
    ]
    
    # ê²€ìƒ‰ ìš”ì²­ì´ ì•„ë‹ˆê³ , í›„ì† ì§ˆë¬¸ íŒ¨í„´ê³¼ ì¼ì¹˜í•˜ë©´ ì»¨í…ìŠ¤íŠ¸ ìœ ì§€
    from utils.query_analyzer import needs_search
    if not needs_search(query):
        for pattern in followup_patterns:
            if re.search(pattern, query, re.IGNORECASE):
                return True
    
    # URLì´ í¬í•¨ëœ ê²½ìš°ë„ í›„ì† ì§ˆë¬¸ìœ¼ë¡œ ì²˜ë¦¬
    if extract_urls_from_text(query):
        return True
    
    # ê·¸ ì™¸ì—ëŠ” ìƒˆë¡œìš´ ì§ˆë¬¸ìœ¼ë¡œ ì²˜ë¦¬
    return False
