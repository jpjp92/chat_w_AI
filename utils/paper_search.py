import requests
import arxiv
import xml.etree.ElementTree as ET
import logging
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime

logger = logging.getLogger(__name__)

class PaperSearchAPI:
    def __init__(self, ncbi_key, cache_handler, cache_ttl=3600):
        self.ncbi_key = ncbi_key
        self.cache = cache_handler
        self.cache_ttl = cache_ttl
        self.pubmed_base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/"
    
    def fetch_arxiv_paper(self, paper):
        """ArXiv ë…¼ë¬¸ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        return {
            "title": paper.title,
            "authors": ", ".join(str(a) for a in paper.authors),
            "summary": paper.summary[:200],
            "entry_id": paper.entry_id,
            "pdf_url": paper.pdf_url,
            "published": paper.published.strftime('%Y-%m-%d')
        }
    
    def get_arxiv_papers(self, query, max_results=3):
        """ArXivì—ì„œ ë…¼ë¬¸ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        cache_key = f"arxiv:{query}:{max_results}"
        cached = self.cache.get(cache_key)
        if cached:
            return cached
        
        try:
            search = arxiv.Search(
                query=query, 
                max_results=max_results, 
                sort_by=arxiv.SortCriterion.SubmittedDate
            )
            
            with ThreadPoolExecutor() as executor:
                results = list(executor.map(self.fetch_arxiv_paper, search.results()))
            
            if not results:
                return "í•´ë‹¹ í‚¤ì›Œë“œë¡œ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            response = "ğŸ“š **Arxiv ë…¼ë¬¸ ê²€ìƒ‰ ê²°ê³¼** ğŸ“š\n\n"
            response += "\n\n".join([
                f"**ë…¼ë¬¸ {i}**\n\n"
                f"ğŸ“„ **ì œëª©**: {r['title']}\n\n"
                f"ğŸ‘¥ **ì €ì**: {r['authors']}\n\n"
                f"ğŸ“ **ì´ˆë¡**: {r['summary']}...\n\n"
                f"ğŸ”— **ë…¼ë¬¸ í˜ì´ì§€**: {r['entry_id']}\n\n"
                f"ğŸ“… **ì¶œíŒì¼**: {r['published']}"
                for i, r in enumerate(results, 1)
            ]) + "\n\në” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"
            
            self.cache.setex(cache_key, self.cache_ttl, response)
            return response
            
        except Exception as e:
            logger.error(f"ArXiv ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            return "ArXiv ë…¼ë¬¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ğŸ˜“"
    
    def search_pubmed(self, query, max_results=5):
        """PubMedì—ì„œ ë…¼ë¬¸ IDë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        search_url = f"{self.pubmed_base_url}esearch.fcgi"
        params = {
            "db": "pubmed",
            "term": query,
            "retmode": "json",
            "retmax": max_results,
            "api_key": self.ncbi_key
        }
        response = requests.get(search_url, params=params, timeout=3)
        return response.json()
    
    def get_pubmed_summaries(self, id_list):
        """PubMed ë…¼ë¬¸ ìš”ì•½ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        summary_url = f"{self.pubmed_base_url}esummary.fcgi"
        params = {
            "db": "pubmed",
            "id": ",".join(id_list),
            "retmode": "json",
            "api_key": self.ncbi_key
        }
        response = requests.get(summary_url, params=params, timeout=3)
        return response.json()
    
    def get_pubmed_abstract(self, id_list):
        """PubMed ë…¼ë¬¸ ì´ˆë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        fetch_url = f"{self.pubmed_base_url}efetch.fcgi"
        params = {
            "db": "pubmed",
            "id": ",".join(id_list),
            "retmode": "xml",
            "rettype": "abstract",
            "api_key": self.ncbi_key
        }
        response = requests.get(fetch_url, params=params, timeout=3)
        return response.text
    
    def extract_first_two_sentences(self, abstract_text):
        """ì´ˆë¡ì—ì„œ ì²« ë‘ ë¬¸ì¥ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        if not abstract_text or abstract_text.isspace():
            return "No abstract available"
        sentences = [s.strip() for s in abstract_text.split('.') if s.strip()]
        return " ".join(sentences[:2]) + "." if sentences else "No abstract available"
    
    def parse_abstracts(self, xml_text):
        """XMLì—ì„œ ì´ˆë¡ì„ íŒŒì‹±í•©ë‹ˆë‹¤."""
        abstract_dict = {}
        try:
            root = ET.fromstring(xml_text)
            for article in root.findall(".//PubmedArticle"):
                pmid = article.find(".//MedlineCitation/PMID").text
                abstract_elem = article.find(".//Abstract/AbstractText")
                abstract = abstract_elem.text if abstract_elem is not None else "No abstract available"
                abstract_dict[pmid] = self.extract_first_two_sentences(abstract)
        except ET.ParseError:
            return {}
        return abstract_dict
    
    def format_date(self, fordate):
        """ë‚ ì§œ í˜•ì‹ì„ í†µì¼í•©ë‹ˆë‹¤."""
        if fordate == 'No date':
            return 'ë‚ ì§œ ì—†ìŒ'
        try:
            date_obj = datetime.strptime(fordate, '%Y %b %d')
            return date_obj.strftime('%Y.%m.%d')
        except ValueError:
            return fordate
    
    def get_pubmed_papers(self, query, max_results=5):
        """PubMedì—ì„œ ë…¼ë¬¸ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        cache_key = f"pubmed:{query}:{max_results}"
        cached = self.cache.get(cache_key)
        if cached:
            return cached
        
        try:
            search_results = self.search_pubmed(query, max_results)
            pubmed_ids = search_results["esearchresult"]["idlist"]
            
            if not pubmed_ids:
                return "í•´ë‹¹ í‚¤ì›Œë“œë¡œ ì˜í•™ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            summaries = self.get_pubmed_summaries(pubmed_ids)
            abstracts_xml = self.get_pubmed_abstract(pubmed_ids)
            abstract_dict = self.parse_abstracts(abstracts_xml)
            
            response = "ğŸ©º **PubMed ë…¼ë¬¸ ê²€ìƒ‰ ê²°ê³¼** ğŸ©º\n\n"
            response += "\n\n".join([
                f"**ë…¼ë¬¸ {i}**\n\n"
                f"ğŸ†” **PMID**: {pmid}\n\n"
                f"ğŸ“– **ì œëª©**: {summaries['result'][pmid].get('title', 'No title')}\n\n"
                f"ğŸ“… **ì¶œíŒì¼**: {self.format_date(summaries['result'][pmid].get('pubdate', 'No date'))}\n\n"
                f"âœï¸ **ì €ì**: {', '.join([author.get('name', '') for author in summaries['result'][pmid].get('authors', [])])}\n\n"
                f"ğŸ“ **ì´ˆë¡**: {abstract_dict.get(pmid, 'No abstract')}\n\n"
                f"ğŸ”— **ë…¼ë¬¸ í˜ì´ì§€**: https://pubmed.ncbi.nlm.nih.gov/{pmid}/"
                for i, pmid in enumerate(pubmed_ids, 1)
            ]) + "\n\në” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"
            
            self.cache.setex(cache_key, self.cache_ttl, response)
            return response
            
        except Exception as e:
            logger.error(f"PubMed ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            return "PubMed ë…¼ë¬¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ğŸ˜“"