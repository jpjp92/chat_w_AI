# ÌôòÍ≤Ω Î≥ÄÏàò Î∞è ÏÑ§Ï†ï
from config.imports import *
from config.env import *

# Î°úÍπÖ ÏÑ§Ï†ï
logging.basicConfig(level=logging.WARNING if os.getenv("ENV") == "production" else logging.INFO)
logger = logging.getLogger("HybridChat")
logging.getLogger("streamlit").setLevel(logging.WARNING)
logging.getLogger("httpx").setLevel(logging.WARNING)

# Ï∫êÏãú ÏÑ§Ï†ï
cache = Cache("cache_directory")

class MemoryCache:
    def __init__(self):
        self.cache = {}
        self.expiry = {}
    
    def get(self, key):
        if key in self.cache and time.time() < self.expiry[key]:
            return self.cache[key]
        return cache.get(key)
    
    def setex(self, key, ttl, value):
        self.cache[key] = value
        self.expiry[key] = time.time() + ttl
        cache.set(key, value, expire=ttl)

cache_handler = MemoryCache()

# MBTI Î∞è Îã§Ï§ëÏßÄÎä• Îç∞Ïù¥ÌÑ∞
mbti_descriptions = {
    "ISTJ": "(ÌòÑÏã§Ï£ºÏùòÏûê) üèõÔ∏èüìöüßë‚Äç‚öñÔ∏è: ÏõêÏπôÏùÑ Ï§ëÏãúÌïòÎ©∞ ÍººÍººÌïú Í≥ÑÌöçÏúºÎ°ú Î™©ÌëúÎ•º Îã¨ÏÑ±!",
    "ISFJ": "(Îî∞ÎúªÌïú ÏàòÌò∏Ïûê) üõ°Ô∏èüß∏üíñ: ÌÉÄÏù∏ÏùÑ Î∞∞Î†§ÌïòÎ©∞ ÌóåÏã†Ï†ÅÏù∏ ÎèÑÏõÄÏùÑ Ï£ºÎäî ÏÑ±Í≤©!",
    "INFJ": "(Ïã†ÎπÑÎ°úÏö¥ Ï°∞Ïñ∏Ïûê) üåøüîÆüìñ: ÍπäÏùÄ ÌÜµÏ∞∞Î†•ÏúºÎ°ú ÏÇ¨ÎûåÎì§ÏóêÍ≤å ÏòÅÍ∞êÏùÑ Ï£ºÎäî Ïù¥ÏÉÅÏ£ºÏùòÏûê!",
    "INTJ": "(Ï†ÑÎûµÍ∞Ä) üß†‚ôüÔ∏èüìà: ÎØ∏ÎûòÎ•º ÏÑ§Í≥ÑÌïòÎ©∞ Î™©ÌëúÎ•º Ìñ•Ìï¥ ÎÇòÏïÑÍ∞ÄÎäî ÎßàÏä§ÌÑ∞ÎßàÏù∏Îìú!",
    "ISTP": "(ÎßåÎä• Ïû¨Ï£ºÍæº) üîßüï∂Ô∏èüèçÔ∏è: Î¨∏Ï†úÎ•º Ïã§ÏßàÏ†ÅÏúºÎ°ú Ìï¥Í≤∞ÌïòÎäî Ïã§Ïö©Ï†ÅÏù∏ Î™®ÌóòÍ∞Ä!",
    "ISFP": "(ÏòàÏà†Í∞Ä) üé®üéµü¶ã: Í∞êÏÑ±ÏùÑ ÌëúÌòÑÌïòÎ©∞ ÏûêÏú†Î°úÏö¥ ÏÇ∂ÏùÑ Ï∂îÍµ¨ÌïòÎäî ÏòàÏà†Í∞Ä!",
    "INFP": "(Ïù¥ÏÉÅÏ£ºÏùòÏûê) üååüìúüïäÔ∏è: ÎÇ¥Î©¥Ïùò Í∞ÄÏπòÎ•º Ï§ëÏãúÌïòÎ©∞ ÏÑ∏ÏÉÅÏùÑ Îçî ÎÇòÏùÄ Í≥≥ÏúºÎ°ú ÎßåÎìúÎäî Î™ΩÏÉÅÍ∞Ä!",
    "INTP": "(ÎÖºÎ¶¨Ï†ÅÏù∏ Ï≤†ÌïôÏûê) ü§îüìñ‚öôÔ∏è: Ìò∏Í∏∞Ïã¨ ÎßéÍ≥† ÎÖºÎ¶¨Ï†ÅÏúºÎ°ú ÏÑ∏ÏÉÅÏùÑ ÌÉêÍµ¨ÌïòÎäî ÏÇ¨ÏÉâÍ∞Ä!",
    "ESTP": "(Î™®ÌóòÍ∞Ä) üèéÔ∏èüî•üé§: ÏàúÍ∞ÑÏùÑ Ï¶êÍ∏∞Î©∞ ÎèÑÏ†ÑÍ≥º Î™®ÌóòÏùÑ ÏÇ¨ÎûëÌïòÎäî ÌôúÎèôÍ∞Ä!",
    "ESFP": "(ÏÇ¨ÍµêÏ†ÅÏù∏ Ïó∞ÏòàÏù∏) üé≠üé§üéä: ÏÇ¨ÎûåÎì§Í≥º Ìï®ÍªòÌïòÎ©∞ Î∂ÑÏúÑÍ∏∞Î•º ÎùÑÏö∞Îäî ÌååÌã∞Ïùò Ï§ëÏã¨!",
    "ENFP": "(ÏûêÏú†Î°úÏö¥ ÏòÅÌòº) üåàüöÄüí°: Ï∞ΩÏùòÏ†ÅÏù∏ ÏïÑÏù¥ÎîîÏñ¥Î°ú ÏÑ∏ÏÉÅÏùÑ Î∞ùÌûàÎäî Ïó¥Ï†ïÏ†ÅÏù∏ ÏòÅÌòº!",
    "ENTP": "(ÌÜ†Î°†Í∞Ä) üó£Ô∏è‚ö°‚ôüÔ∏è: ÏÉàÎ°úÏö¥ ÏïÑÏù¥ÎîîÏñ¥Î•º ÌÉêÍµ¨ÌïòÎ©∞ ÎÖºÏüÅÏùÑ Ï¶êÍ∏∞Îäî ÌòÅÏã†Í∞Ä!",
    "ESTJ": "(ÏóÑÍ≤©Ìïú Í¥ÄÎ¶¨Ïûê) üèóÔ∏èüìäüõ†Ô∏è: Ï≤¥Í≥ÑÏ†ÅÏúºÎ°ú Î™©ÌëúÎ•º Îã¨ÏÑ±ÌïòÎäî Î¶¨ÎçîÏã≠Ïùò ÎåÄÍ∞Ä!",
    "ESFJ": "(ÏπúÏ†àÌïú Ïô∏ÍµêÍ¥Ä) üíêü§óüè°: ÏÇ¨ÎûåÎì§ÏùÑ Ïó∞Í≤∞ÌïòÎ©∞ Îî∞ÎúªÌïú Í≥µÎèôÏ≤¥Î•º ÎßåÎìúÎäî Ïô∏ÍµêÍ¥Ä!",
    "ENFJ": "(Ïó¥Ï†ïÏ†ÅÏù∏ Î¶¨Îçî) üåüüé§ü´∂: ÌÉÄÏù∏ÏùÑ Ïù¥ÎÅåÎ©∞ Í∏çÏ†ïÏ†ÅÏù∏ Î≥ÄÌôîÎ•º ÎßåÎìúÎäî Ïπ¥Î¶¨Ïä§Îßà Î¶¨Îçî!",
    "ENTJ": "(ÏïºÎßùÍ∞Ä) üëëüìàüî•: Î™©ÌëúÎ•º Ìñ•Ìï¥ ÎèåÏßÑÌïòÎ©∞ ÌÅ∞ Í∑∏Î¶ºÏùÑ Í∑∏Î¶¨Îäî ÏßÄÌúòÍ¥Ä!"
}

multi_iq_descriptions = {
    "Ïñ∏Ïñ¥ÏßÄÎä•": {
        "description": "üìùüìöüì¢: ÎßêÍ≥º Í∏ÄÏùÑ ÌÜµÌï¥ ÏÉùÍ∞ÅÏùÑ ÌëúÌòÑÌïòÎäî Îç∞ ÌÉÅÏõî!\n",
        "jobs": "ÏÜåÏÑ§Í∞Ä, ÏãúÏù∏, ÏûëÍ∞Ä, ÎÖºÏÑ§ / ÎèôÌôî ÏûëÍ∞Ä, Î∞©ÏÜ°ÏûëÍ∞Ä, ÏòÅÌôîÎåÄÎ≥∏ÏûëÍ∞Ä, ÏõπÌà∞ ÏûëÍ∞Ä / ÏïÑÎÇòÏö¥ÏÑú, Î¶¨Ìè¨ÌÑ∞, ÏÑ±Ïö∞ / ÍµêÏÇ¨, ÍµêÏàò, Í∞ïÏÇ¨, ÎèÖÏÑú ÏßÄÎèÑÏÇ¨ / Ïñ∏Ïñ¥ÏπòÎ£åÏÇ¨, Ïã¨Î¶¨ÏπòÎ£åÏÇ¨, Íµ¨Ïó∞ÎèôÌôîÍ∞Ä"
    },
    "ÎÖºÎ¶¨ÏàòÌïôÏßÄÎä•": {
        "description": "üßÆüìäüß†: Î∂ÑÏÑùÏ†Å ÏÇ¨Í≥†ÏôÄ Î¨∏Ï†ú Ìï¥Í≤∞ Îä•Î†•Ïù¥ Îõ∞Ïñ¥ÎÇ®!\n",
        "jobs": "Í≥ºÌïôÏûê, Î¨ºÎ¶¨ÌïôÏûê, ÏàòÌïôÏûê / ÏùòÎ£åÍ≥µÌïô, Ï†ÑÏûêÍ≥µÌïô, Ïª¥Ìì®ÌÑ∞ Í≥µÌïô, Ìï≠Í≥µÏö∞Ï£ºÍ≥µÌïô / Ïï†ÎÑêÎ¶¨Ïä§Ìä∏, Í≤ΩÏòÅ Ïª®ÏÑ§ÌåÖ, ÌöåÍ≥ÑÏÇ¨, ÏÑ∏Î¨¥ÏÇ¨ / Ìà¨ÏûêÎ∂ÑÏÑùÍ∞Ä, M&A Ï†ÑÎ¨∏Í∞Ä / IT Ïª®ÏÑ§ÌåÖ, Ïª¥Ìì®ÌÑ∞ ÌîÑÎ°úÍ∑∏ÎûòÎ®∏, web Í∞úÎ∞ú / ÌÜµÏã† Ïã†Ìò∏Ï≤òÎ¶¨, ÌÜµÍ≥ÑÌïô, AI Í∞úÎ∞ú, Ï†ïÎ≥¥Ï≤òÎ¶¨, ÎπÖÎç∞Ïù¥ÌÑ∞ ÏóÖÎ¨¥ / ÏùÄÌñâÏõê, Í∏àÏúµÍ∏∞Í¥Ä, Í∞ïÏÇ¨, ÎπÑÌèâÍ∞Ä, ÎÖºÏÑ§ / Î≥ÄÌò∏ÏÇ¨, Î≥ÄÎ¶¨ÏÇ¨, Í≤ÄÏÇ¨, ÌåêÏÇ¨ / ÏùòÏÇ¨, Í±¥Ï∂ïÍ∞Ä, ÏÑ§Í≥ÑÏÇ¨"
    },
    "Í≥µÍ∞ÑÏßÄÎä•": {
        "description": "üé®üì∏üèõÔ∏è: Í∑∏Î¶ºÍ≥º ÎîîÏûêÏù∏ÏúºÎ°ú Í≥µÍ∞ÑÏùÑ ÏïÑÎ¶ÑÎãµÍ≤å ÌëúÌòÑ!\n",
        "jobs": "ÏÇ¨ÏßÑÏÇ¨, Ï¥¨ÏòÅÍ∏∞ÏÇ¨, ÎßåÌôîÍ∞Ä, Ïï†ÎãàÎ©îÏù¥ÏÖò, ÌôîÍ∞Ä, ÏïÑÌã∞Ïä§Ìä∏ / Í±¥Ï∂ï ÏÑ§Í≥Ñ, Ïù∏ÌÖåÎ¶¨Ïñ¥, ÎîîÏûêÏù¥ÎÑà / ÏßÄÎèÑ Ï†úÏûë, ÏóîÏßÄÎãàÏñ¥, Î∞úÎ™ÖÍ∞Ä / Ï†ÑÏûêÍ≥µÌïô, Í∏∞Í≥ÑÍ≥µÌïô, ÌÜµÏã†Í≥µÌïô, ÏÇ∞ÏóÖÍ≥µÌïô, Î°úÎ¥á Í∞úÎ∞ú / ÏòÅÌôîÍ∞êÎèÖ, Î∞©ÏÜ° ÌîºÎîî, Ìë∏ÎìúÏä§ÌÉÄÏùºÎ¶¨Ïä§Ìä∏ / Í¥ëÍ≥† Ï†úÏûë, Ïù∏ÏáÑ ÏóÖÎ¨¥"
    },
    "ÏùåÏïÖÏßÄÎä•": {
        "description": "üé∂üéßüé∏: ÏÜåÎ¶¨ÏôÄ Î¶¨Îì¨ÏùÑ ÎäêÎÅºÍ≥† Ï∞ΩÏ°∞ÌïòÎäî ÏùåÏïÖÏ†Å Ïû¨Îä•!\n",
        "jobs": "ÏùåÏïÖÍµêÏÇ¨, ÏùåÌñ•ÏÇ¨, ÏûëÍ≥°Í∞Ä, ÏûëÏÇ¨Í∞Ä, Ìé∏Í≥°Í∞Ä, Í∞ÄÏàò, ÏÑ±ÏïÖÍ∞Ä / ÏïÖÍ∏∞ Ïó∞Ï£º / ÎèôÏãúÌÜµÏó≠ÏÇ¨, ÏÑ±Ïö∞ / ÎÆ§ÏßÄÏª¨ Î∞∞Ïö∞ / Î∞úÎ†à, Î¨¥Ïö© / ÏùåÌñ• Î∂ÄÎ¨∏, Ïó∞Ïòà Í∏∞ÌöçÏÇ¨ / DJ, Í∞úÏù∏ ÏùåÏïÖ Î∞©ÏÜ°, Í∞ÄÏàò Îß§ÎãàÏßÄÎ®ºÌä∏"
    },
    "Ïã†Ï≤¥Ïö¥ÎèôÏßÄÎä•": {
        "description": "üèÄü§∏‚Äç‚ôÇÔ∏èüèÜ: Î™∏ÏùÑ ÌôúÏö©Ìï¥ Ïä§Ìè¨Ï∏†ÏôÄ ÏõÄÏßÅÏûÑÏóêÏÑú ÎëêÍ∞Å!\n",
        "jobs": "Ïô∏Í≥ºÏùòÏÇ¨, ÏπòÍ∏∞Í≥µÏÇ¨, ÌïúÏùòÏÇ¨, ÏàòÏùòÏÇ¨, Í∞ÑÌò∏ÏÇ¨, ÎåÄÏ≤¥ÏùòÌïô / Î¨ºÎ¶¨ÏπòÎ£åÏÇ¨, ÏûëÏóÖÏπòÎ£åÏÇ¨ / ÏïÖÍ∏∞ Ïó∞Ï£º, ÏÑ±ÏïÖÍ∞Ä, Í∞ÄÏàò, Î¨¥Ïö©, Ïó∞Í∑π / Ïä§Ìè¨Ï∏†, Ï≤¥Ïú°ÍµêÏÇ¨, Î™®Îç∏ / Í≤ΩÏ∞∞, Í≤ΩÌò∏Ïõê, Íµ∞Ïù∏, ÏÜåÎ∞©Í¥Ä / ÎÜçÏóÖ, ÏûÑÏóÖ, ÏàòÏÇ∞ÏóÖ, Ï∂ïÏÇ∞ÏóÖ / Í≥µÏòà, Ïï°ÏÑ∏ÏÑúÎ¶¨ Ï†úÏûë, Í∞ÄÍµ¨ Ï†úÏûë"
    },
    "ÎåÄÏù∏Í¥ÄÍ≥ÑÏßÄÎä•": {
        "description": "ü§ùüó£Ô∏èüí¨: ÏÇ¨ÎûåÎì§Í≥º ÏÜåÌÜµÌïòÎ©∞ Í¥ÄÍ≥ÑÎ•º Ïûò Îß∫Îäî Îä•Î†•!\n",
        "jobs": "Î≥ÄÌò∏ÏÇ¨, Í≤ÄÏÇ¨, ÌåêÏÇ¨, Î≤ïÎ¨¥ÏÇ¨ / ÍµêÏÇ¨, ÍµêÏàò, Í∞ïÏÇ¨ / ÌôçÎ≥¥ ÏóÖÎ¨¥, ÎßàÏºÄÌåÖ / ÏßÄÎ∞∞Ïù∏, ÎπÑÏÑú, ÏäπÎ¨¥Ïõê, ÌåêÎß§ÏóÖÎ¨¥ / Í∏∞Ïûê, Î¶¨Ìè¨ÌÑ∞, Î≥¥ÌóòÏÑúÎπÑÏä§ / Ïô∏ÍµêÍ¥Ä, Íµ≠Ï†úÍ≥µÎ¨¥Ïõê, Í≤ΩÏ∞∞ / Î≥ëÏõêÏΩîÎîîÎÑ§Ïù¥ÌÑ∞, Í∞ÑÌò∏ÏÇ¨ / Ìò∏ÌÖîÎ¶¨Ïñ¥, ÌïôÏäµÏßÄ ÍµêÏÇ¨, Ïõ®Îî©ÌîåÎûòÎÑà, ÏõÉÏùåÏπòÎ£åÏÇ¨, ÏÑ±ÏßÅÏûê"
    },
    "ÏûêÍ∏∞Ïù¥Ìï¥ÏßÄÎä•": {
        "description": "üßò‚Äç‚ôÇÔ∏èüí≠üìñ: ÏûêÏã†ÏùÑ ÍπäÏù¥ Ïù¥Ìï¥ÌïòÍ≥† ÏÑ±Ï∞∞ÌïòÎäî ÎÇ¥Î©¥Ïùò Ìûò!\n",
        "jobs": "Î≥ÄÌò∏ÏÇ¨, Í≤ÄÏÇ¨, ÌåêÏÇ¨, Î≥ÄÎ¶¨ÏÇ¨, ÌèâÎ°†Í∞Ä, ÎÖºÏÑ§ / ÍµêÏÇ¨, ÍµêÏàò, Ïã¨Î¶¨ÏÉÅÎã¥ÏÇ¨ / Ïä§Ìè¨Ï∏† Í∞êÎèÖ, ÏΩîÏπò, Ïã¨Ìåê, Ïä§Ìè¨Ï∏† Ìï¥ÏÑ§Í∞Ä / ÌòëÏÉÅÍ∞Ä, CEO, CTO, Ïª®ÏÑ§ÌåÖ, ÎßàÏºÄÌåÖ, ÌöåÏÇ¨ Í≤ΩÏòÅ / Í∏∞Ïûê, ÏïÑÎÇòÏö¥ÏÑú, ÏöîÎ¶¨ÏÇ¨, Ïã¨ÏÇ¨ÏúÑÏõê / ÏùòÏÇ¨, Ï†úÏïΩ Î∂ÑÏïº Ïó∞Íµ¨Ïõê / ÏÑ±ÏßÅÏûê, Ï≤†ÌïôÏûê, Ìà¨ÏûêÎ∂ÑÏÑùÍ∞Ä, ÏûêÏÇ∞Í¥ÄÎ¶¨ / ÏòÅÌôîÍ∞êÎèÖ, ÏûëÍ∞Ä, Í±¥Ï∂ïÍ∞Ä"
    },
    "ÏûêÏó∞ÏπúÌôîÏßÄÎä•": {
        "description": "üåøüê¶üåç: ÏûêÏó∞Í≥º ÎèôÎ¨ºÏùÑ ÏÇ¨ÎûëÌïòÎ©∞ ÌôòÍ≤ΩÏóê ÎØºÍ∞êÌïú Ïû¨Îä•!\n",
        "jobs": "ÏùòÏÇ¨, Í∞ÑÌò∏ÏÇ¨, Î¨ºÎ¶¨ÏπòÎ£å, ÏûÑÏÉÅÎ≥ëÎ¶¨ / ÏàòÏùòÏÇ¨, ÎèôÎ¨º ÏÇ¨Ïú°, Í≥§Ï∂© ÏÇ¨Ïú° / Í±¥Ï∂ï ÏÑ§Í≥Ñ, Í∞êÎ¶¨, Ï∏°ÎüâÏÇ¨, Ï°∞Í≤Ω ÎîîÏûêÏù∏ / Ï≤úÎ¨∏ÌïôÏûê, ÏßÄÏßàÌïôÏûê / ÏÉùÎ™ÖÍ≥µÌïô, Í∏∞Í≥Ñ Í≥µÌïô, ÏÉùÎ¨ºÍ≥µÌïô, Ï†ÑÏûêÍ≥µÌïô / ÏùòÏÇ¨, Í∞ÑÌò∏ÏÇ¨, ÏïΩÏ†úÏÇ¨, ÏûÑÏÉÅÎ≥ëÎ¶¨ / ÌäπÏàòÏûëÎ¨º Ïû¨Î∞∞, ÎÜçÏóÖ, ÏûÑÏóÖ, Ï∂ïÏÇ∞ÏóÖ, ÏõêÏòà, ÌîåÎ°úÎ¶¨Ïä§Ìä∏"
    }
}

mbti_full_description = """
### üìù MBTI Ïú†ÌòïÎ≥Ñ Ìïú Ï§Ñ ÏÑ§Î™Ö
#### üî• Ïô∏Ìñ•Ìòï (E) vs ‚ùÑÔ∏è ÎÇ¥Ìñ•Ìòï (I)  
- **E (Ïô∏Ìñ•Ìòï)** üéâüó£Ô∏èüöÄüåû: ÏÇ¨ÎûåÎì§Í≥º Ïñ¥Ïö∏Î¶¨Î©∞ ÏóêÎÑàÏßÄÎ•º ÏñªÎäî ÏÇ¨ÍµêÏ†ÅÏù∏ ÏÑ±Í≤©!  
- **I (ÎÇ¥Ìñ•Ìòï)** üìöüõãÔ∏èüåôü§´: ÌòºÏûêÎßåÏùò ÏãúÍ∞ÑÏùÑ Ï¶êÍ∏∞Î©∞ ÎÇ¥Î©¥Ïóê ÏßëÏ§ëÌïòÎäî ÏÑ±Í≤©!  
#### üìä ÏßÅÍ¥ÄÌòï (N) vs üßê Í∞êÍ∞ÅÌòï (S)  
- **N (ÏßÅÍ¥ÄÌòï)** üí°‚ú®üé®üîÆ: Ï∞ΩÏùòÏ†ÅÏù¥Í≥† ÌÅ∞ Í∑∏Î¶ºÏùÑ Î≥¥Î©∞ ÏïÑÏù¥ÎîîÏñ¥Î•º Ï§ëÏãú!  
- **S (Í∞êÍ∞ÅÌòï)** üîéüìèüõ†Ô∏èüçΩÔ∏è: ÌòÑÏã§Ï†ÅÏù¥Í≥† Íµ¨Ï≤¥Ï†ÅÏù∏ Ï†ïÎ≥¥Î•º Î∞îÌÉïÏúºÎ°ú ÌñâÎèô!  
#### ü§ù Í∞êÏ†ïÌòï (F) vs ‚öñÔ∏è ÏÇ¨Í≥†Ìòï (T)  
- **F (Í∞êÏ†ïÌòï)** ‚ù§Ô∏èü•∞üå∏ü´Ç: Í≥µÍ∞êÍ≥º ÏÇ¨Îûå Ï§ëÏã¨ÏúºÎ°ú Îî∞ÎúªÌïú Í≤∞Ï†ïÏùÑ ÎÇ¥Î¶º!  
- **T (ÏÇ¨Í≥†Ìòï)** üß†‚öôÔ∏èüìäüìè: ÎÖºÎ¶¨ÏôÄ Í∞ùÍ¥ÄÏ†Å ÌåêÎã®ÏúºÎ°ú Î¨∏Ï†úÎ•º Ìï¥Í≤∞!  
#### ‚è≥ ÌåêÎã®Ìòï (J) vs üåä Ïù∏ÏãùÌòï (P)  
- **J (Í≥ÑÌöçÌòï)** üìÖüìåüìù‚úÖ: Ï≤¥Í≥ÑÏ†ÅÏù¥Í≥† Í≥ÑÌöçÏ†ÅÏúºÎ°ú ÏùºÏùÑ Ï≤òÎ¶¨ÌïòÎäî Ïä§ÌÉÄÏùº!  
- **P (Ï¶âÌù•Ìòï)** üé≠üé¢üå™Ô∏èüåç: Ïú†Ïó∞ÌïòÍ≥† Î≥ÄÌôîÏóê Ïûò Ï†ÅÏùëÌïòÎäî ÏûêÏú†Î°úÏö¥ Ïä§ÌÉÄÏùº!  
#### üé≠ MBTI Ïú†ÌòïÎ≥Ñ Ìïú Ï§Ñ ÏÑ§Î™Ö  
- ‚úÖ **ISTJ** (ÌòÑÏã§Ï£ºÏùòÏûê) üèõÔ∏èüìöüßë‚Äç‚öñÔ∏è: ÏõêÏπôÏùÑ Ï§ëÏãúÌïòÎ©∞ ÍººÍººÌïú Í≥ÑÌöçÏúºÎ°ú Î™©ÌëúÎ•º Îã¨ÏÑ±!  
- ‚úÖ **ISFJ** (Îî∞ÎúªÌïú ÏàòÌò∏Ïûê) üõ°Ô∏èüß∏üíñ: ÌÉÄÏù∏ÏùÑ Î∞∞Î†§ÌïòÎ©∞ ÌóåÏã†Ï†ÅÏù∏ ÎèÑÏõÄÏùÑ Ï£ºÎäî ÏÑ±Í≤©!  
- ‚úÖ **INFJ** (Ïã†ÎπÑÎ°úÏö¥ Ï°∞Ïñ∏Ïûê) üåøüîÆüìñ: ÍπäÏùÄ ÌÜµÏ∞∞Î†•ÏúºÎ°ú ÏÇ¨ÎûåÎì§ÏóêÍ≤å ÏòÅÍ∞êÏùÑ Ï£ºÎäî Ïù¥ÏÉÅÏ£ºÏùòÏûê!  
- ‚úÖ **INTJ** (Ï†ÑÎûµÍ∞Ä) üß†‚ôüÔ∏èüìà: ÎØ∏ÎûòÎ•º ÏÑ§Í≥ÑÌïòÎ©∞ Î™©ÌëúÎ•º Ìñ•Ìï¥ ÎÇòÏïÑÍ∞ÄÎäî ÎßàÏä§ÌÑ∞ÎßàÏù∏Îìú!  
- ‚úÖ **ISTP** (ÎßåÎä• Ïû¨Ï£ºÍæº) üîßüï∂Ô∏èüèçÔ∏è: Î¨∏Ï†úÎ•º Ïã§ÏßàÏ†ÅÏúºÎ°ú Ìï¥Í≤∞ÌïòÎäî Ïã§Ïö©Ï†ÅÏù∏ Î™®ÌóòÍ∞Ä!  
- ‚úÖ **ISFP** (ÏòàÏà†Í∞Ä) üé®üéµü¶ã: Í∞êÏÑ±ÏùÑ ÌëúÌòÑÌïòÎ©∞ ÏûêÏú†Î°úÏö¥ ÏÇ∂ÏùÑ Ï∂îÍµ¨ÌïòÎäî ÏòàÏà†Í∞Ä!  
- ‚úÖ **INFP** (Ïù¥ÏÉÅÏ£ºÏùòÏûê) üååüìúüïäÔ∏è: ÎÇ¥Î©¥Ïùò Í∞ÄÏπòÎ•º Ï§ëÏãúÌïòÎ©∞ ÏÑ∏ÏÉÅÏùÑ Îçî ÎÇòÏùÄ Í≥≥ÏúºÎ°ú ÎßåÎìúÎäî Î™ΩÏÉÅÍ∞Ä!  
- ‚úÖ **INTP** (ÎÖºÎ¶¨Ï†ÅÏù∏ Ï≤†ÌïôÏûê) ü§îüìñ‚öôÔ∏è: Ìò∏Í∏∞Ïã¨ ÎßéÍ≥† ÎÖºÎ¶¨Ï†ÅÏúºÎ°ú ÏÑ∏ÏÉÅÏùÑ ÌÉêÍµ¨ÌïòÎäî ÏÇ¨ÏÉâÍ∞Ä!  
- ‚úÖ **ESTP** (Î™®ÌóòÍ∞Ä) üèéÔ∏èüî•üé§: ÏàúÍ∞ÑÏùÑ Ï¶êÍ∏∞Î©∞ ÎèÑÏ†ÑÍ≥º Î™®ÌóòÏùÑ ÏÇ¨ÎûëÌïòÎäî ÌôúÎèôÍ∞Ä!  
- ‚úÖ **ESFP** (ÏÇ¨ÍµêÏ†ÅÏù∏ Ïó∞ÏòàÏù∏) üé≠üé§üéä: ÏÇ¨ÎûåÎì§Í≥º Ìï®ÍªòÌïòÎ©∞ Î∂ÑÏúÑÍ∏∞Î•º ÎùÑÏö∞Îäî ÌååÌã∞Ïùò Ï§ëÏã¨!  
- ‚úÖ **ENFP** (ÏûêÏú†Î°úÏö¥ ÏòÅÌòº) üåàüöÄüí°: Ï∞ΩÏùòÏ†ÅÏù∏ ÏïÑÏù¥ÎîîÏñ¥Î°ú ÏÑ∏ÏÉÅÏùÑ Î∞ùÌûàÎäî Ïó¥Ï†ïÏ†ÅÏù∏ ÏòÅÌòº!  
- ‚úÖ **ENTP** (ÌÜ†Î°†Í∞Ä) üó£Ô∏è‚ö°‚ôüÔ∏è: ÏÉàÎ°úÏö¥ ÏïÑÏù¥ÎîîÏñ¥Î•º ÌÉêÍµ¨ÌïòÎ©∞ ÎÖºÏüÅÏùÑ Ï¶êÍ∏∞Îäî ÌòÅÏã†Í∞Ä!  
- ‚úÖ **ESTJ** (ÏóÑÍ≤©Ìïú Í¥ÄÎ¶¨Ïûê) üèóÔ∏èüìäüõ†Ô∏è: Ï≤¥Í≥ÑÏ†ÅÏúºÎ°ú Î™©ÌëúÎ•º Îã¨ÏÑ±ÌïòÎäî Î¶¨ÎçîÏã≠Ïùò ÎåÄÍ∞Ä!  
- ‚úÖ **ESFJ** (ÏπúÏ†àÌïú Ïô∏ÍµêÍ¥Ä) üíêü§óüè°: ÏÇ¨ÎûåÎì§ÏùÑ Ïó∞Í≤∞ÌïòÎ©∞ Îî∞ÎúªÌïú Í≥µÎèôÏ≤¥Î•º ÎßåÎìúÎäî Ïô∏ÍµêÍ¥Ä!  
- ‚úÖ **ENFJ** (Ïó¥Ï†ïÏ†ÅÏù∏ Î¶¨Îçî) üåüüé§ü´∂: ÌÉÄÏù∏ÏùÑ Ïù¥ÎÅåÎ©∞ Í∏çÏ†ïÏ†ÅÏù∏ Î≥ÄÌôîÎ•º ÎßåÎìúÎäî Ïπ¥Î¶¨Ïä§Îßà Î¶¨Îçî!  
- ‚úÖ **ENTJ** (ÏïºÎßùÍ∞Ä) üëëüìàüî•: Î™©ÌëúÎ•º Ìñ•Ìï¥ ÎèåÏßÑÌïòÎ©∞ ÌÅ∞ Í∑∏Î¶ºÏùÑ Í∑∏Î¶¨Îäî ÏßÄÌúòÍ¥Ä!
"""

multi_iq_full_description = """
### üé® Îã§Ï§ëÏßÄÎä• Ïú†ÌòïÎ≥Ñ Ìïú Ï§Ñ ÏÑ§Î™Ö Î∞è Ï∂îÏ≤ú ÏßÅÏóÖ  
- üìñ **Ïñ∏Ïñ¥ ÏßÄÎä•** üìùüìöüì¢: ÎßêÍ≥º Í∏ÄÏùÑ ÌÜµÌï¥ ÏÉùÍ∞ÅÏùÑ ÌëúÌòÑÌïòÎäî Îç∞ ÌÉÅÏõî!  
    - **Ï∂îÏ≤ú ÏßÅÏóÖ**: ÏÜåÏÑ§Í∞Ä, ÏãúÏù∏, ÏûëÍ∞Ä, ÎÖºÏÑ§ / ÎèôÌôî ÏûëÍ∞Ä, Î∞©ÏÜ°ÏûëÍ∞Ä, ÏòÅÌôîÎåÄÎ≥∏ÏûëÍ∞Ä, ÏõπÌà∞ ÏûëÍ∞Ä / ÏïÑÎÇòÏö¥ÏÑú, Î¶¨Ìè¨ÌÑ∞, ÏÑ±Ïö∞ / ÍµêÏÇ¨, ÍµêÏàò, Í∞ïÏÇ¨, ÎèÖÏÑú ÏßÄÎèÑÏÇ¨ / Ïñ∏Ïñ¥ÏπòÎ£åÏÇ¨, Ïã¨Î¶¨ÏπòÎ£åÏÇ¨, Íµ¨Ïó∞ÎèôÌôîÍ∞Ä  
- üî¢ **ÎÖºÎ¶¨-ÏàòÌïô ÏßÄÎä•** üßÆüìäüß†: Î∂ÑÏÑùÏ†Å ÏÇ¨Í≥†ÏôÄ Î¨∏Ï†ú Ìï¥Í≤∞ Îä•Î†•Ïù¥ Îõ∞Ïñ¥ÎÇ®!  
    - **Ï∂îÏ≤ú ÏßÅÏóÖ**: Í≥ºÌïôÏûê, Î¨ºÎ¶¨ÌïôÏûê, ÏàòÌïôÏûê / ÏùòÎ£åÍ≥µÌïô, Ï†ÑÏûêÍ≥µÌïô, Ïª¥Ìì®ÌÑ∞ Í≥µÌïô, Ìï≠Í≥µÏö∞Ï£ºÍ≥µÌïô / Ïï†ÎÑêÎ¶¨Ïä§Ìä∏, Í≤ΩÏòÅ Ïª®ÏÑ§ÌåÖ, ÌöåÍ≥ÑÏÇ¨, ÏÑ∏Î¨¥ÏÇ¨ / Ìà¨ÏûêÎ∂ÑÏÑùÍ∞Ä, M&A Ï†ÑÎ¨∏Í∞Ä / IT Ïª®ÏÑ§ÌåÖ, Ïª¥Ìì®ÌÑ∞ ÌîÑÎ°úÍ∑∏ÎûòÎ®∏, web Í∞úÎ∞ú / ÌÜµÏã† Ïã†Ìò∏Ï≤òÎ¶¨, ÌÜµÍ≥ÑÌïô, AI Í∞úÎ∞ú, Ï†ïÎ≥¥Ï≤òÎ¶¨, ÎπÖÎç∞Ïù¥ÌÑ∞ ÏóÖÎ¨¥ / ÏùÄÌñâÏõê, Í∏àÏúµÍ∏∞Í¥Ä, Í∞ïÏÇ¨, ÎπÑÌèâÍ∞Ä, ÎÖºÏÑ§ / Î≥ÄÌò∏ÏÇ¨, Î≥ÄÎ¶¨ÏÇ¨, Í≤ÄÏÇ¨, ÌåêÏÇ¨ / ÏùòÏÇ¨, Í±¥Ï∂ïÍ∞Ä, ÏÑ§Í≥ÑÏÇ¨  
- üé® **Í≥µÍ∞Ñ ÏßÄÎä•** üé®üì∏üèõÔ∏è: Í∑∏Î¶ºÍ≥º ÎîîÏûêÏù∏ÏúºÎ°ú Í≥µÍ∞ÑÏùÑ ÏïÑÎ¶ÑÎãµÍ≤å ÌëúÌòÑ!  
    - **Ï∂îÏ≤ú ÏßÅÏóÖ**: ÏÇ¨ÏßÑÏÇ¨, Ï¥¨ÏòÅÍ∏∞ÏÇ¨, ÎßåÌôîÍ∞Ä, Ïï†ÎãàÎ©îÏù¥ÏÖò, ÌôîÍ∞Ä, ÏïÑÌã∞Ïä§Ìä∏ / Í±¥Ï∂ï ÏÑ§Í≥Ñ, Ïù∏ÌÖåÎ¶¨Ïñ¥, ÎîîÏûêÏù¥ÎÑà / ÏßÄÎèÑ Ï†úÏûë, ÏóîÏßÄÎãàÏñ¥, Î∞úÎ™ÖÍ∞Ä / Ï†ÑÏûêÍ≥µÌïô, Í∏∞Í≥ÑÍ≥µÌïô, ÌÜµÏã†Í≥µÌïô, ÏÇ∞ÏóÖÍ≥µÌïô, Î°úÎ¥á Í∞úÎ∞ú / ÏòÅÌôîÍ∞êÎèÖ, Î∞©ÏÜ° ÌîºÎîî, Ìë∏ÎìúÏä§ÌÉÄÏùºÎ¶¨Ïä§Ìä∏ / Í¥ëÍ≥† Ï†úÏûë, Ïù∏ÏáÑ ÏóÖÎ¨¥  
- üéµ **ÏùåÏïÖ ÏßÄÎä•** üé∂üéßüé∏: ÏÜåÎ¶¨ÏôÄ Î¶¨Îì¨ÏùÑ ÎäêÎÅºÍ≥† Ï∞ΩÏ°∞ÌïòÎäî ÏùåÏïÖÏ†Å Ïû¨Îä•!  
    - **Ï∂îÏ≤ú ÏßÅÏóÖ**: ÏùåÏïÖÍµêÏÇ¨, ÏùåÌñ•ÏÇ¨, ÏûëÍ≥°Í∞Ä, ÏûëÏÇ¨Í∞Ä, Ìé∏Í≥°Í∞Ä, Í∞ÄÏàò, ÏÑ±ÏïÖÍ∞Ä / ÏïÖÍ∏∞ Ïó∞Ï£º / ÎèôÏãúÌÜµÏó≠ÏÇ¨, ÏÑ±Ïö∞ / ÎÆ§ÏßÄÏª¨ Î∞∞Ïö∞ / Î∞úÎ†à, Î¨¥Ïö© / ÏùåÌñ• Î∂ÄÎ¨∏, Ïó∞Ïòà Í∏∞ÌöçÏÇ¨ / DJ, Í∞úÏù∏ ÏùåÏïÖ Î∞©ÏÜ°, Í∞ÄÏàò Îß§ÎãàÏßÄÎ®ºÌä∏  
- üèÉ **Ïã†Ï≤¥-Ïö¥Îèô ÏßÄÎä•** üèÄü§∏‚Äç‚ôÇÔ∏èüèÜ: Î™∏ÏùÑ ÌôúÏö©Ìï¥ Ïä§Ìè¨Ï∏†ÏôÄ ÏõÄÏßÅÏûÑÏóêÏÑú ÎëêÍ∞Å!  
    - **Ï∂îÏ≤ú ÏßÅÏóÖ**: Ïô∏Í≥ºÏùòÏÇ¨, ÏπòÍ∏∞Í≥µÏÇ¨, ÌïúÏùòÏÇ¨, ÏàòÏùòÏÇ¨, Í∞ÑÌò∏ÏÇ¨, ÎåÄÏ≤¥ÏùòÌïô / Î¨ºÎ¶¨ÏπòÎ£åÏÇ¨, ÏûëÏóÖÏπòÎ£åÏÇ¨ / ÏïÖÍ∏∞ Ïó∞Ï£º, ÏÑ±ÏïÖÍ∞Ä, Í∞ÄÏàò, Î¨¥Ïö©, Ïó∞Í∑π / Ïä§Ìè¨Ï∏†, Ï≤¥Ïú°ÍµêÏÇ¨, Î™®Îç∏ / Í≤ΩÏ∞∞, Í≤ΩÌò∏Ïõê, Íµ∞Ïù∏, ÏÜåÎ∞©Í¥Ä / ÎÜçÏóÖ, ÏûÑÏóÖ, ÏàòÏÇ∞ÏóÖ, Ï∂ïÏÇ∞ÏóÖ / Í≥µÏòà, Ïï°ÏÑ∏ÏÑúÎ¶¨ Ï†úÏûë, Í∞ÄÍµ¨ Ï†úÏûë  
- ü§ù **ÎåÄÏù∏Í¥ÄÍ≥Ñ ÏßÄÎä•** ü§ùüó£Ô∏èüí¨: ÏÇ¨ÎûåÎì§Í≥º ÏÜåÌÜµÌïòÎ©∞ Í¥ÄÍ≥ÑÎ•º Ïûò Îß∫Îäî Îä•Î†•!  
    - **Ï∂îÏ≤ú ÏßÅÏóÖ**: Î≥ÄÌò∏ÏÇ¨, Í≤ÄÏÇ¨, ÌåêÏÇ¨, Î≤ïÎ¨¥ÏÇ¨ / ÍµêÏÇ¨, ÍµêÏàò, Í∞ïÏÇ¨ / ÌôçÎ≥¥ ÏóÖÎ¨¥, ÎßàÏºÄÌåÖ / ÏßÄÎ∞∞Ïù∏, ÎπÑÏÑú, ÏäπÎ¨¥Ïõê, ÌåêÎß§ÏóÖÎ¨¥ / Í∏∞Ïûê, Î¶¨Ìè¨ÌÑ∞, Î≥¥ÌóòÏÑúÎπÑÏä§ / Ïô∏ÍµêÍ¥Ä, Íµ≠Ï†úÍ≥µÎ¨¥Ïõê, Í≤ΩÏ∞∞ / Î≥ëÏõêÏΩîÎîîÎÑ§Ïù¥ÌÑ∞, Í∞ÑÌò∏ÏÇ¨ / Ìò∏ÌÖîÎ¶¨Ïñ¥, ÌïôÏäµÏßÄ ÍµêÏÇ¨, Ïõ®Îî©ÌîåÎûòÎÑà, ÏõÉÏùåÏπòÎ£åÏÇ¨, ÏÑ±ÏßÅÏûê  
- üßò **ÏûêÍ∏∞ Ïù¥Ìï¥ ÏßÄÎä•** üßò‚Äç‚ôÇÔ∏èüí≠üìñ: ÏûêÏã†ÏùÑ ÍπäÏù¥ Ïù¥Ìï¥ÌïòÍ≥† ÏÑ±Ï∞∞ÌïòÎäî ÎÇ¥Î©¥Ïùò Ìûò!  
    - **Ï∂îÏ≤ú ÏßÅÏóÖ**: Î≥ÄÌò∏ÏÇ¨, Í≤ÄÏÇ¨, ÌåêÏÇ¨, Î≥ÄÎ¶¨ÏÇ¨, ÌèâÎ°†Í∞Ä, ÎÖºÏÑ§ / ÍµêÏÇ¨, ÍµêÏàò, Ïã¨Î¶¨ÏÉÅÎã¥ÏÇ¨ / Ïä§Ìè¨Ï∏† Í∞êÎèÖ, ÏΩîÏπò, Ïã¨Ìåê, Ïä§Ìè¨Ï∏† Ìï¥ÏÑ§Í∞Ä / ÌòëÏÉÅÍ∞Ä, CEO, CTO, Ïª®ÏÑ§ÌåÖ, ÎßàÏºÄÌåÖ, ÌöåÏÇ¨ Í≤ΩÏòÅ / Í∏∞Ïûê, ÏïÑÎÇòÏö¥ÏÑú, ÏöîÎ¶¨ÏÇ¨, Ïã¨ÏÇ¨ÏúÑÏõê / ÏùòÏÇ¨, Ï†úÏïΩ Î∂ÑÏïº Ïó∞Íµ¨Ïõê / ÏÑ±ÏßÅÏûê, Ï≤†ÌïôÏûê, Ìà¨ÏûêÎ∂ÑÏÑùÍ∞Ä, ÏûêÏÇ∞Í¥ÄÎ¶¨ / ÏòÅÌôîÍ∞êÎèÖ, ÏûëÍ∞Ä, Í±¥Ï∂ïÍ∞Ä  
- üå± **ÏûêÏó∞ ÏπúÌôî ÏßÄÎä•** üåøüê¶üåç: ÏûêÏó∞Í≥º ÎèôÎ¨ºÏùÑ ÏÇ¨ÎûëÌïòÎ©∞ ÌôòÍ≤ΩÏóê ÎØºÍ∞êÌïú Ïû¨Îä•!  
    - **Ï∂îÏ≤ú ÏßÅÏóÖ**: ÏùòÏÇ¨, Í∞ÑÌò∏ÏÇ¨, Î¨ºÎ¶¨ÏπòÎ£å, ÏûÑÏÉÅÎ≥ëÎ¶¨ / ÏàòÏùòÏÇ¨, ÎèôÎ¨º ÏÇ¨Ïú°, Í≥§Ï∂© ÏÇ¨Ïú° / Í±¥Ï∂ï ÏÑ§Í≥Ñ, Í∞êÎ¶¨, Ï∏°ÎüâÏÇ¨, Ï°∞Í≤Ω ÎîîÏûêÏù∏ / Ï≤úÎ¨∏ÌïôÏûê, ÏßÄÏßàÌïôÏûê / ÏÉùÎ™ÖÍ≥µÌïô, Í∏∞Í≥Ñ Í≥µÌïô, ÏÉùÎ¨ºÍ≥µÌïô, Ï†ÑÏûêÍ≥µÌïô / ÏùòÏÇ¨, Í∞ÑÌò∏ÏÇ¨, ÏïΩÏ†úÏÇ¨, ÏûÑÏÉÅÎ≥ëÎ¶¨ / ÌäπÏàòÏûëÎ¨º Ïû¨Î∞∞, ÎÜçÏóÖ, ÏûÑÏóÖ, Ï∂ïÏÇ∞ÏóÖ, ÏõêÏòà, ÌîåÎ°úÎ¶¨Ïä§Ìä∏  
"""

# WeatherAPI ÌÅ¥ÎûòÏä§
class WeatherAPI:
    def __init__(self, cache_ttl=600):
        self.cache = cache_handler
        self.cache_ttl = cache_ttl

    def fetch_weather(self, url, params):
        session = requests.Session()
        retry_strategy = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("https://", adapter)
        try:
            response = session.get(url, params=params, timeout=3)
            response.raise_for_status()
            return response.json()
        except:
            return self.cache.get(f"weather:{params.get('q', '')}") or "ÎÇ†Ïî® Ï†ïÎ≥¥Î•º Î∂àÎü¨Ïò¨ Ïàò ÏóÜÏäµÎãàÎã§."

    @lru_cache(maxsize=100)
    def get_city_info(self, city_name):
        cache_key = f"city_info:{city_name}"
        cached = self.cache.get(cache_key)
        if cached:
            return cached
        url = "http://api.openweathermap.org/geo/1.0/direct"
        params = {'q': city_name, 'limit': 1, 'appid': WEATHER_API_KEY}
        data = self.fetch_weather(url, params)
        if data and isinstance(data, list) and len(data) > 0:
            city_info = {"name": data[0]["name"], "lat": data[0]["lat"], "lon": data[0]["lon"]}
            self.cache.setex(cache_key, 86400, city_info)
            return city_info
        return None

    def get_city_weather(self, city_name):
        cache_key = f"weather:{city_name}"
        cached_data = self.cache.get(cache_key)
        if cached_data:
            return cached_data
        
        city_info = self.get_city_info(city_name)
        if not city_info:
            return f"'{city_name}'Ïùò ÎÇ†Ïî® Ï†ïÎ≥¥Î•º Í∞ÄÏ†∏Ïò¨ Ïàò ÏóÜÏäµÎãàÎã§."
        
        url = "https://api.openweathermap.org/data/2.5/weather"
        params = {'lat': city_info["lat"], 'lon': city_info["lon"], 'appid': WEATHER_API_KEY, 'units': 'metric', 'lang': 'kr'}
        data = self.fetch_weather(url, params)
        if isinstance(data, str):
            return data
        weather_emojis = {'Clear': '‚òÄÔ∏è', 'Clouds': '‚òÅÔ∏è', 'Rain': 'üåßÔ∏è', 'Snow': '‚ùÑÔ∏è', 'Thunderstorm': '‚õàÔ∏è', 'Drizzle': 'üå¶Ô∏è', 'Mist': 'üå´Ô∏è'}
        weather_emoji = weather_emojis.get(data['weather'][0]['main'], 'üå§Ô∏è')
        result = (
            f"ÌòÑÏû¨ {data['name']}, {data['sys']['country']} ÎÇ†Ïî® {weather_emoji}\n"
            f"ÎÇ†Ïî®: {data['weather'][0]['description']}\n"
            f"Ïò®ÎèÑ: {data['main']['temp']}¬∞C\n"
            f"Ï≤¥Í∞ê: {data['main']['feels_like']}¬∞C\n"
            f"ÏäµÎèÑ: {data['main']['humidity']}%\n"
            f"ÌíçÏÜç: {data['wind']['speed']}m/s\n"
            f"Îçî Í∂ÅÍ∏àÌïú Ï†ê ÏûàÎÇòÏöî? üòä"
        )
        self.cache.setex(cache_key, self.cache_ttl, result)
        return result

    def get_forecast_by_day(self, city_name, days_from_today=1):
        cache_key = f"forecast:{city_name}:{days_from_today}"
        cached_data = self.cache.get(cache_key)
        if cached_data:
            return cached_data
        
        city_info = self.get_city_info(city_name)
        if not city_info:
            return f"'{city_name}'Ïùò ÎÇ†Ïî® ÏòàÎ≥¥Î•º Í∞ÄÏ†∏Ïò¨ Ïàò ÏóÜÏäµÎãàÎã§."
        
        url = "https://api.openweathermap.org/data/2.5/forecast"
        params = {'lat': city_info["lat"], 'lon': city_info["lon"], 'appid': WEATHER_API_KEY, 'units': 'metric', 'lang': 'kr'}
        data = self.fetch_weather(url, params)
        if isinstance(data, str):
            return data
        target_date = (datetime.now() + timedelta(days=days_from_today)).strftime('%Y-%m-%d')
        forecast_text = f"{city_info['name']}Ïùò {target_date} ÎÇ†Ïî® ÏòàÎ≥¥ üå§Ô∏è\n\n"
        weather_emojis = {'Clear': '‚òÄÔ∏è', 'Clouds': '‚òÅÔ∏è', 'Rain': 'üåßÔ∏è', 'Snow': '‚ùÑÔ∏è', 'Thunderstorm': '‚õàÔ∏è', 'Drizzle': 'üå¶Ô∏è', 'Mist': 'üå´Ô∏è'}
        
        found = False
        for forecast in data['list']:
            dt = datetime.fromtimestamp(forecast['dt']).strftime('%Y-%m-%d')
            if dt == target_date:
                found = True
                time_only = datetime.fromtimestamp(forecast['dt']).strftime('%H:%M')
                weather_emoji = weather_emojis.get(forecast['weather'][0]['main'], 'üå§Ô∏è')
                forecast_text += (
                    f"‚è∞ {time_only} {forecast['weather'][0]['description']} {weather_emoji} "
                    f"{forecast['main']['temp']}¬∞C üíß{forecast['main']['humidity']}% üå¨Ô∏è{forecast['wind']['speed']}m/s\n\n"
                )
        
        result = forecast_text + "Îçî Í∂ÅÍ∏àÌïú Ï†ê ÏûàÎÇòÏöî? üòä" if found else f"'{city_name}'Ïùò {target_date} ÎÇ†Ïî® ÏòàÎ≥¥Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§."
        self.cache.setex(cache_key, self.cache_ttl, result)
        return result

    def get_weekly_forecast(self, city_name):
        cache_key = f"weekly_forecast:{city_name}"
        cached_data = self.cache.get(cache_key)
        if cached_data:
            return cached_data
        
        city_info = self.get_city_info(city_name)
        if not city_info:
            return f"'{city_name}'Ïùò Ï£ºÍ∞Ñ ÏòàÎ≥¥Î•º Í∞ÄÏ†∏Ïò¨ Ïàò ÏóÜÏäµÎãàÎã§."
        
        url = "https://api.openweathermap.org/data/2.5/forecast"
        params = {'lat': city_info["lat"], 'lon': city_info["lon"], 'appid': WEATHER_API_KEY, 'units': 'metric', 'lang': 'kr'}
        data = self.fetch_weather(url, params)
        if isinstance(data, str):
            return data
        today = datetime.now().date()
        week_end = today + timedelta(days=6)
        daily_forecast = {}
        weekdays_kr = ["ÏõîÏöîÏùº", "ÌôîÏöîÏùº", "ÏàòÏöîÏùº", "Î™©ÏöîÏùº", "Í∏àÏöîÏùº", "ÌÜ†ÏöîÏùº", "ÏùºÏöîÏùº"]
        today_weekday = today.weekday()
        
        for forecast in data['list']:
            dt = datetime.fromtimestamp(forecast['dt']).date()
            if today <= dt <= week_end:
                dt_str = dt.strftime('%Y-%m-%d')
                if dt_str not in daily_forecast:
                    weekday_idx = (today_weekday + (dt - today).days) % 7
                    daily_forecast[dt_str] = {
                        'weekday': weekdays_kr[weekday_idx],
                        'temp_min': forecast['main']['temp_min'],
                        'temp_max': forecast['main']['temp_max'],
                        'weather': forecast['weather'][0]['description']
                    }
                else:
                    daily_forecast[dt_str]['temp_min'] = min(daily_forecast[dt_str]['temp_min'], forecast['main']['temp_min'])
                    daily_forecast[dt_str]['temp_max'] = max(daily_forecast[dt_str]['temp_max'], forecast['main']['temp_max'])
        
        today_str = today.strftime('%Y-%m-%d')
        today_weekday_str = weekdays_kr[today_weekday]
        forecast_text = f"{today_str}({today_weekday_str}) Í∏∞Ï§Ä {city_info['name']}Ïùò Ï£ºÍ∞Ñ ÎÇ†Ïî® ÏòàÎ≥¥ üå§Ô∏è\n"
        weather_emojis = {'Clear': '‚òÄÔ∏è', 'Clouds': '‚òÅÔ∏è', 'Rain': 'üåßÔ∏è', 'Snow': '‚ùÑÔ∏è', 'Thunderstorm': '‚õàÔ∏è', 'Drizzle': 'üå¶Ô∏è', 'Mist': 'üå´Ô∏è'}
        
        for date, info in daily_forecast.items():
            weather_emoji = weather_emojis.get(info['weather'].split()[0], 'üå§Ô∏è')
            forecast_text += (
                f"\n{info['weekday']}: {info['weather']} {weather_emoji} "
                f"ÏµúÏ†Ä {info['temp_min']}¬∞C ÏµúÍ≥† {info['temp_max']}¬∞C\n\n"
            )
        
        result = forecast_text + "\nÎçî Í∂ÅÍ∏àÌïú Ï†ê ÏûàÎÇòÏöî? üòä"
        self.cache.setex(cache_key, self.cache_ttl, result)
        return result

# FootballAPI ÌÅ¥ÎûòÏä§
class FootballAPI:
    def __init__(self, api_key, cache_ttl=600):
        self.api_key = api_key
        self.base_url = "https://api.football-data.org/v4/competitions"
        self.cache = cache_handler
        self.cache_ttl = cache_ttl

    def fetch_league_standings(self, league_code, league_name):
        cache_key = f"league_standings:{league_code}"
        cached_data = self.cache.get(cache_key)
        if cached_data is not None:
            return cached_data

        url = f"{self.base_url}/{league_code}/standings"
        headers = {'X-Auth-Token': self.api_key}
        
        try:
            time.sleep(1)
            response = requests.get(url, headers=headers, timeout=3)
            response.raise_for_status()
            data = response.json()
            
            standings = data['standings'][0]['table'] if league_code not in ["CL"] else data['standings']
            if league_code in ["CL"]:
                standings_data = []
                for group in standings:
                    for team in group['table']:
                        standings_data.append({
                            'ÏàúÏúÑ': team['position'],
                            'Í∑∏Î£π': group['group'],
                            'ÌåÄ': team['team']['name'],
                            'Í≤ΩÍ∏∞': team['playedGames'],
                            'Ïäπ': team['won'],
                            'Î¨¥': team['draw'],
                            'Ìå®': team['lost'],
                            'ÎìùÏ†ê': team['goalsFor'],
                            'Ïã§Ï†ê': team['goalsAgainst'],
                            'ÎìùÏã§Ï∞®': team['goalsFor'] - team['goalsAgainst'],
                            'Ìè¨Ïù∏Ìä∏': team['points']
                        })
                df = pd.DataFrame(standings_data)
            else:
                df = pd.DataFrame([
                    {
                        'ÏàúÏúÑ': team['position'],
                        'ÌåÄ': team['team']['name'],
                        'Í≤ΩÍ∏∞': team['playedGames'],
                        'Ïäπ': team['won'],
                        'Î¨¥': team['draw'],
                        'Ìå®': team['lost'],
                        'ÎìùÏ†ê': team['goalsFor'],
                        'Ïã§Ï†ê': team['goalsAgainst'],
                        'ÎìùÏã§Ï∞®': team['goalsFor'] - team['goalsAgainst'],
                        'Ìè¨Ïù∏Ìä∏': team['points']
                    } for team in standings
                ])
            
            result = {"league_name": league_name, "data": df}
            self.cache.setex(cache_key, self.cache_ttl, result)
            return result
        
        except requests.exceptions.RequestException as e:
            return {"league_name": league_name, "error": f"{league_name} Î¶¨Í∑∏ ÏàúÏúÑÎ•º Í∞ÄÏ†∏Ïò§Îäî Ï§ë Î¨∏Ï†úÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)} üòì"}

    def fetch_league_scorers(self, league_code, league_name):
        cache_key = f"league_scorers:{league_code}"
        cached_data = self.cache.get(cache_key)
        if cached_data is not None:
            return cached_data

        url = f"{self.base_url}/{league_code}/scorers"
        headers = {'X-Auth-Token': self.api_key}
        
        try:
            time.sleep(1)
            response = requests.get(url, headers=headers, timeout=3)
            response.raise_for_status()
            data = response.json()
            
            scorers = [{"ÏàúÏúÑ": i+1, "ÏÑ†Ïàò": s['player']['name'], "ÌåÄ": s['team']['name'], "ÎìùÏ†ê": s['goals']} 
                       for i, s in enumerate(data['scorers'][:10])]
            df = pd.DataFrame(scorers)
            result = {"league_name": league_name, "data": df}
            self.cache.setex(cache_key, self.cache_ttl, result)
            return result
        
        except requests.exceptions.RequestException as e:
            return {"league_name": league_name, "error": f"{league_name} Î¶¨Í∑∏ ÎìùÏ†êÏàúÏúÑ Ï†ïÎ≥¥Î•º Í∞ÄÏ†∏Ïò§Îäî Ï§ë Î¨∏Ï†úÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)} üòì"}

# Ï¥àÍ∏∞Ìôî
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
client = Client(exclude_providers=["OpenaiChat", "Copilot", "Liaobots", "Jmuz", "PollinationsAI", "ChatGptEs"])
weather_api = WeatherAPI()
football_api = FootballAPI(api_key=SPORTS_API_KEY)
naver_request_count = 0
NAVER_DAILY_LIMIT = 25000
st.set_page_config(page_title="AI Ï±óÎ¥á", page_icon="ü§ñ")

# ÏÑ∏ÏÖò ÏÉÅÌÉú Ï¥àÍ∏∞Ìôî
def init_session_state():
    if "is_logged_in" not in st.session_state:
        st.session_state.is_logged_in = False
    if "user_id" not in st.session_state:
        st.session_state.user_id = None
    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "ÏïàÎÖïÌïòÏÑ∏Ïöî! Î¨¥ÏóáÏùÑ ÎèÑÏôÄÎìúÎ¶¥ÍπåÏöî?üòä"}]
    if "session_id" not in st.session_state:
        st.session_state.session_id = str(uuid.uuid4())

# ÎèÑÏãú Î∞è ÏãúÍ∞Ñ Ï∂îÏ∂ú
CITY_PATTERNS = [
    re.compile(r'(?:Ïò§Îäò|ÎÇ¥Ïùº|Î™®Î†à|Ïù¥Î≤à Ï£º|Ï£ºÍ∞Ñ)?\s*([Í∞Ä-Ìû£a-zA-Z\s]{2,20}(?:Ïãú|Íµ∞|city)?)Ïùò?\s*ÎÇ†Ïî®', re.IGNORECASE),
    re.compile(r'(?:Ïò§Îäò|ÎÇ¥Ïùº|Î™®Î†à|Ïù¥Î≤à Ï£º|Ï£ºÍ∞Ñ)?\s*([Í∞Ä-Ìû£a-zA-Z\s]{2,20}(?:Ïãú|Íµ∞|city)?)\s*ÎÇ†Ïî®', re.IGNORECASE),
]
def extract_city_from_query(query):
    for pattern in CITY_PATTERNS:
        match = pattern.search(query)
        if match:
            city = match.group(1).strip()
            if city not in ["Ïò§Îäò", "ÎÇ¥Ïùº", "Î™®Î†à", "Ïù¥Î≤à Ï£º", "Ï£ºÍ∞Ñ", "ÌòÑÏû¨"]:
                return city
    return "ÏÑúÏö∏"

TIME_CITY_PATTERNS = [
    re.compile(r'([Í∞Ä-Ìû£a-zA-Z]{2,20}(?:Ïãú|Íµ∞)?)Ïùò?\s*ÏãúÍ∞Ñ'),
    re.compile(r'([Í∞Ä-Ìû£a-zA-Z]{2,20}(?:Ïãú|Íµ∞)?)\s*ÏãúÍ∞Ñ'),
]
def extract_city_from_time_query(query):
    for pattern in TIME_CITY_PATTERNS:
        match = pattern.search(query)
        if match:
            city = match.group(1).strip()
            if city != "ÌòÑÏû¨":
                return city
    return "ÏÑúÏö∏"

LEAGUE_MAPPING = {
    "epl": {"name": "ÌîÑÎ¶¨ÎØ∏Ïñ¥Î¶¨Í∑∏ (ÏòÅÍµ≠)", "code": "PL"},
    "laliga": {"name": "ÎùºÎ¶¨Í∞Ä (Ïä§ÌéòÏù∏)", "code": "PD"},
    "bundesliga": {"name": "Î∂ÑÎç∞Ïä§Î¶¨Í∞Ä (ÎèÖÏùº)", "code": "BL1"},
    "seriea": {"name": "ÏÑ∏Î¶¨Ïóê A (Ïù¥ÌÉàÎ¶¨ÏïÑ)", "code": "SA"},
    "ligue1": {"name": "Î¶¨Í∑∏ 1 (ÌîÑÎûëÏä§)", "code": "FL1"},
    "championsleague": {"name": "Ï±îÌîºÏñ∏Ïä§ Î¶¨Í∑∏", "code": "CL"}
}

def extract_league_from_query(query):
    query_lower = query.lower().replace(" ", "")
    league_keywords = {
        "epl": ["epl", "ÌîÑÎ¶¨ÎØ∏Ïñ¥Î¶¨Í∑∏"],
        "laliga": ["laliga", "ÎùºÎ¶¨Í∞Ä"],
        "bundesliga": ["bundesliga", "Î∂ÑÎç∞Ïä§Î¶¨Í∞Ä"],
        "seriea": ["seriea", "ÏÑ∏Î¶¨Ïóêa"],
        "ligue1": ["ligue1", "Î¶¨Í∑∏1"],
        "championsleague": ["championsleague", "Ï±îÌîºÏñ∏Ïä§Î¶¨Í∑∏", "ucl"]
    }
    for league_key, keywords in league_keywords.items():
        if any(keyword in query_lower for keyword in keywords):
            return league_key
    return None

def get_kst_time():
    kst_timezone = pytz.timezone("Asia/Seoul")
    kst_time = datetime.now(kst_timezone)
    return f"ÎåÄÌïúÎØºÍµ≠ Í∏∞Ï§Ä : {kst_time.strftime('%YÎÖÑ %mÏõî %dÏùº %p %I:%M')}ÏûÖÎãàÎã§. ‚è∞\n\n Îçî Í∂ÅÍ∏àÌïú Ï†ê ÏûàÎÇòÏöî? üòä"

def get_time_by_city(city_name="ÏÑúÏö∏"):
    city_info = weather_api.get_city_info(city_name)
    if not city_info:
        return f"'{city_name}'Ïùò ÏãúÍ∞Ñ Ï†ïÎ≥¥Î•º Í∞ÄÏ†∏Ïò¨ Ïàò ÏóÜÏäµÎãàÎã§."
    tf = TimezoneFinder()
    timezone_str = tf.timezone_at(lng=city_info["lon"], lat=city_info["lat"]) or "Asia/Seoul"
    timezone = pytz.timezone(timezone_str)
    city_time = datetime.now(timezone)
    return f"ÌòÑÏû¨ {city_name} ÏãúÍ∞Ñ: {city_time.strftime('%YÎÖÑ %mÏõî %dÏùº %p %I:%M')}ÏûÖÎãàÎã§. ‚è∞\n\n Îçî Í∂ÅÍ∏àÌïú Ï†ê ÏûàÎÇòÏöî? üòä"

# ÏÇ¨Ïö©Ïûê Î∞è Ï±ÑÌåÖ Í∏∞Î°ù Í¥ÄÎ¶¨
def create_or_get_user(nickname):
    user = supabase.table("users").select("*").eq("nickname", nickname).execute()
    if user.data:
        return user.data[0]["id"], True
    new_user = supabase.table("users").insert({"nickname": nickname, "created_at": datetime.now().isoformat()}).execute()
    return new_user.data[0]["id"], False


def save_chat_history(user_id, session_id, question, answer, time_taken):
    # generatorÏù¥Î©¥ contentÎ•º Î≥ëÌï©Ìï¥ÏÑú Î¨∏ÏûêÏó¥Î°ú Î≥ÄÌôò
    if hasattr(answer, '__iter__') and not isinstance(answer, (str, dict, list)):
        try:
            answer = ''.join([chunk.choices[0].delta.content for chunk in answer if hasattr(chunk.choices[0].delta, 'content') and chunk.choices[0].delta.content])
        except Exception as e:
            answer = f"[streaming ÏùëÎãµ Ïò§Î•ò: {str(e)}]"

    supabase.table("chat_history").insert({
        "user_id": user_id,
        "session_id": session_id,
        "question": question,
        "answer": answer,
        "time_taken": time_taken,
        "created_at": datetime.now().isoformat()
    }).execute()

# def save_chat_history(user_id, session_id, question, answer, time_taken):
#     if isinstance(answer, dict) and "table" in answer and isinstance(answer["table"], pd.DataFrame):
#         answer_to_save = {
#             "header": answer["header"],
#             "table": answer["table"].to_dict(orient="records"),
#             "footer": answer["footer"]
#         }
#     else:
#         answer_to_save = answer
    
#     supabase.table("chat_history").insert({
#         "user_id": user_id,
#         "session_id": session_id,
#         "question": question,
#         "answer": answer_to_save,
#         "time_taken": time_taken,
#         "created_at": datetime.now().isoformat()
#     }).execute()

def async_save_chat_history(user_id, session_id, question, answer, time_taken):
    threading.Thread(target=save_chat_history, args=(user_id, session_id, question, answer, time_taken)).start()

# ÏùòÏïΩÌíà Í≤ÄÏÉâ
def get_drug_info(drug_query):
    drug_name = drug_query.replace("ÏïΩÌíàÍ≤ÄÏÉâ", "").strip()
    cache_key = f"drug:{drug_name}"
    cached = cache_handler.get(cache_key)
    if cached:
        return cached
    
    url = 'http://apis.data.go.kr/1471000/DrbEasyDrugInfoService/getDrbEasyDrugList'
    params = {'serviceKey': DRUG_API_KEY, 'pageNo': '1', 'numOfRows': '1', 'itemName': urllib.parse.quote(drug_name), 'type': 'json'}
    try:
        response = requests.get(url, params=params, timeout=3)
        response.raise_for_status()
        data = response.json()
        if 'body' in data and 'items' in data['body'] and data['body']['items']:
            item = data['body']['items'][0]
            efcy = item.get('efcyQesitm', 'Ï†ïÎ≥¥ ÏóÜÏùå')[:150] + ("..." if len(item.get('efcyQesitm', '')) > 150 else "")
            use_method = item.get('useMethodQesitm', 'Ï†ïÎ≥¥ ÏóÜÏùå')[:150] + ("..." if len(item.get('useMethodQesitm', '')) > 150 else "")
            atpn = item.get('atpnQesitm', 'Ï†ïÎ≥¥ ÏóÜÏùå')[:150] + ("..." if len(item.get('atpnQesitm', '')) > 150 else "")
            
            result = (
                f"üíä **ÏùòÏïΩÌíà Ï†ïÎ≥¥** üíä\n\n"
                f"‚úÖ **ÏïΩÌíàÎ™Ö**: {item.get('itemName', 'Ï†ïÎ≥¥ ÏóÜÏùå')}\n\n"
                f"‚úÖ **Ï†úÏ°∞ÏÇ¨**: {item.get('entpName', 'Ï†ïÎ≥¥ ÏóÜÏùå')}\n\n"
                f"‚úÖ **Ìö®Îä•**: {efcy}\n\n"
                f"‚úÖ **Ïö©Î≤ïÏö©Îüâ**: {use_method}\n\n"
                f"‚úÖ **Ï£ºÏùòÏÇ¨Ìï≠**: {atpn}\n\n"
                f"Îçî Í∂ÅÍ∏àÌïú Ï†ê ÏûàÎÇòÏöî? üòä"
            )
            cache_handler.setex(cache_key, 86400, result)
            return result
        return f"'{drug_name}'Ïùò Í≥µÏãù Ï†ïÎ≥¥Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§."
    except Exception as e:
        logger.error(f"ÏïΩÌíà API Ïò§Î•ò: {str(e)}")
        return f"'{drug_name}'Ïùò Ï†ïÎ≥¥Î•º Í∞ÄÏ†∏Ïò§Îäî Ï§ë Î¨∏Ï†úÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§. üòì"

# Naver API Í≤ÄÏÉâ
def get_naver_api_results(query):
    global naver_request_count
    cache_key = f"naver:{query}"
    cached = cache_handler.get(cache_key)
    if cached:
        return cached
    
    if naver_request_count >= NAVER_DAILY_LIMIT:
        return "Í≤ÄÏÉâ ÌïúÎèÑ Ï¥àÍ≥ºÎ°ú Í≤∞Í≥ºÎ•º Í∞ÄÏ†∏Ïò¨ Ïàò ÏóÜÏäµÎãàÎã§. üòì"
    enc_text = urllib.parse.quote(query)
    url = f"https://openapi.naver.com/v1/search/webkr?query={enc_text}&display=5&sort=date"
    request = urllib.request.Request(url)
    request.add_header("X-Naver-Client-Id", NAVER_CLIENT_ID)
    request.add_header("X-Naver-Client-Secret", NAVER_CLIENT_SECRET)
    try:
        response = urllib.request.urlopen(request, timeout=3)
        naver_request_count += 1
        if response.getcode() == 200:
            data = json.loads(response.read().decode('utf-8'))
            results = data.get('items', [])
            if not results:
                return "Í≤ÄÏÉâ Í≤∞Í≥ºÍ∞Ä ÏóÜÏäµÎãàÎã§. üòì"
            
            response_text = "üåê **Ïõπ Í≤ÄÏÉâ Í≤∞Í≥º** \n\n"
            response_text += "\n\n".join(
                [f"**Í≤∞Í≥º {i}**\n\nüìÑ **Ï†úÎ™©**: {re.sub(r'<b>|</b>', '', item['title'])}\n\nüìù **ÎÇ¥Ïö©**: {re.sub(r'<b>|</b>', '', item.get('description', 'ÎÇ¥Ïö© ÏóÜÏùå'))[:100]}...\n\nüîó **ÎßÅÌÅ¨**: {item.get('link', '')}"
                 for i, item in enumerate(results, 1)]
            ) + "\n\nÎçî Í∂ÅÍ∏àÌïú Ï†ê ÏûàÎÇòÏöî? üòä"
            cache_handler.setex(cache_key, 3600, response_text)
            return response_text
    except Exception as e:
        logger.error(f"Naver API Ïò§Î•ò: {str(e)}")
        return "Í≤ÄÏÉâ Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§. üòì"

# ArXiv ÎÖºÎ¨∏ Í≤ÄÏÉâ
def fetch_arxiv_paper(paper):
    return {
        "title": paper.title,
        "authors": ", ".join(str(a) for a in paper.authors),
        "summary": paper.summary[:200],
        "entry_id": paper.entry_id,
        "pdf_url": paper.pdf_url,
        "published": paper.published.strftime('%Y-%m-%d')
    }

def get_arxiv_papers(query, max_results=3):
    cache_key = f"arxiv:{query}:{max_results}"
    cached = cache_handler.get(cache_key)
    if cached:
        return cached
    search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.SubmittedDate)
    with ThreadPoolExecutor() as executor:
        results = list(executor.map(fetch_arxiv_paper, search.results()))
    if not results:
        return "Ìï¥Îãπ ÌÇ§ÏõåÎìúÎ°ú ÎÖºÎ¨∏ÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§."
    
    response = "üìö **Arxiv ÎÖºÎ¨∏ Í≤ÄÏÉâ Í≤∞Í≥º** üìö\n\n"
    response += "\n\n".join(
        [f"**ÎÖºÎ¨∏ {i}**\n\nüìÑ **Ï†úÎ™©**: {r['title']}\n\nüë• **Ï†ÄÏûê**: {r['authors']}\n\nüìù **Ï¥àÎ°ù**: {r['summary']}...\n\nüîó **ÎÖºÎ¨∏ ÌéòÏù¥ÏßÄ**: {r['entry_id']}\n\nüìÖ **Ï∂úÌåêÏùº**: {r['published']}"
         for i, r in enumerate(results, 1)]
    ) + "\n\nÎçî Í∂ÅÍ∏àÌïú Ï†ê ÏûàÎÇòÏöî? üòä"
    cache_handler.setex(cache_key, 3600, response)
    return response

# PubMed ÎÖºÎ¨∏ Í≤ÄÏÉâ
base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/"

def search_pubmed(query, max_results=5):
    search_url = f"{base_url}esearch.fcgi"
    params = {"db": "pubmed", "term": query, "retmode": "json", "retmax": max_results, "api_key": NCBI_KEY}
    response = requests.get(search_url, params=params, timeout=3)
    return response.json()

def get_pubmed_summaries(id_list):
    summary_url = f"{base_url}esummary.fcgi"
    params = {"db": "pubmed", "id": ",".join(id_list), "retmode": "json", "api_key": NCBI_KEY}
    response = requests.get(summary_url, params=params, timeout=3)
    return response.json()

def get_pubmed_abstract(id_list):
    fetch_url = f"{base_url}efetch.fcgi"
    params = {"db": "pubmed", "id": ",".join(id_list), "retmode": "xml", "rettype": "abstract", "api_key": NCBI_KEY}
    response = requests.get(fetch_url, params=params, timeout=3)
    return response.text

def extract_first_two_sentences(abstract_text):
    if not abstract_text or abstract_text.isspace():
        return "No abstract available"
    sentences = [s.strip() for s in abstract_text.split('.') if s.strip()]
    return " ".join(sentences[:2]) + "." if sentences else "No abstract available"

def parse_abstracts(xml_text):
    abstract_dict = {}
    try:
        root = ET.fromstring(xml_text)
        for article in root.findall(".//PubmedArticle"):
            pmid = article.find(".//MedlineCitation/PMID").text
            abstract_elem = article.find(".//Abstract/AbstractText")
            abstract = abstract_elem.text if abstract_elem is not None else "No abstract available"
            abstract_dict[pmid] = extract_first_two_sentences(abstract)
    except ET.ParseError:
        return {}
    return abstract_dict

def get_pubmed_papers(query, max_results=5):
    cache_key = f"pubmed:{query}:{max_results}"
    cached = cache_handler.get(cache_key)
    if cached:
        return cached
    
    search_results = search_pubmed(query, max_results)
    pubmed_ids = search_results["esearchresult"]["idlist"]
    if not pubmed_ids:
        return "Ìï¥Îãπ ÌÇ§ÏõåÎìúÎ°ú ÏùòÌïô ÎÖºÎ¨∏ÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§."
    
    summaries = get_pubmed_summaries(pubmed_ids)
    abstracts_xml = get_pubmed_abstract(pubmed_ids)
    abstract_dict = parse_abstracts(abstracts_xml)
    
    response = "üìö **PubMed ÎÖºÎ¨∏ Í≤ÄÏÉâ Í≤∞Í≥º** üìö\n\n"
    response += "\n\n".join(
        [f"**ÎÖºÎ¨∏ {i}**\n\nüÜî **PMID**: {pmid}\n\nüìñ **Ï†úÎ™©**: {summaries['result'][pmid].get('title', 'No title')}\n\nüìÖ **Ï∂úÌåêÏùº**: {summaries['result'][pmid].get('pubdate', 'No date')}\n\n‚úçÔ∏è **Ï†ÄÏûê**: {', '.join([author.get('name', '') for author in summaries['result'][pmid].get('authors', [])])}\n\nüìù **Ï¥àÎ°ù**: {abstract_dict.get(pmid, 'No abstract')}"
         for i, pmid in enumerate(pubmed_ids, 1)]
    ) + "\n\nÎçî Í∂ÅÍ∏àÌïú Ï†ê ÏûàÎÇòÏöî? üòä"
    cache_handler.setex(cache_key, 3600, response)
    return response

# ÎåÄÌôîÌòï ÏùëÎãµ (Ïä§Ìä∏Î¶¨Î∞ç Ï†ÅÏö©)
conversation_cache = MemoryCache()
async def get_conversational_response(query, messages):
    cache_key = f"conv:{needs_search(query)}:{query}"
    cached = conversation_cache.get(cache_key)
    if cached:
        return cached, False
    
    system_message = {"role": "system", "content": "ÏπúÏ†àÌïú AI Ï±óÎ¥áÏûÖÎãàÎã§. Ï†ÅÏ†àÌïú Ïù¥Î™®ÏßÄ ÏÇ¨Ïö©: ‚úÖ(ÏôÑÎ£å), ‚ùì(ÏßàÎ¨∏), üòä(ÏπúÏ†à)"}
    conversation_history = [system_message] + messages[-2:] + [{"role": "user", "content": query}]
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=conversation_history,
            web_search=False,
            stream=True
        )
        return response, True
    except Exception as e:
        logger.error(f"ÎåÄÌôî ÏùëÎãµ ÏÉùÏÑ± Ï§ë Ïò§Î•ò: {str(e)}")
        return f"ÏùëÎãµÏùÑ ÏÉùÏÑ±ÌïòÎäî Ï§ë Î¨∏Ï†úÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)} üòì", False

GREETINGS = ["ÏïàÎÖï", "ÌïòÏù¥", "Ìó¨Î°ú", "„Öé„Öá", "ÏôìÏóÖ", "Ìï†Î°±", "Ìó§Ïù¥"]
GREETING_RESPONSE = "ÏïàÎÖïÌïòÏÑ∏Ïöî! Î∞òÍ∞ëÏäµÎãàÎã§. Î¨¥ÏóáÏùÑ ÎèÑÏôÄÎìúÎ¶¥ÍπåÏöî? üòä"

@lru_cache(maxsize=100)
def needs_search(query):
    query_lower = query.strip().lower().replace(" ", "")
    if "ÎÇ†Ïî®" in query_lower:
        return "weather" if "ÎÇ¥Ïùº" not in query_lower else "tomorrow_weather"
    if "ÏãúÍ∞Ñ" in query_lower or "ÎÇ†Ïßú" in query_lower:
        return "time"
    if "Î¶¨Í∑∏ÏàúÏúÑ" in query_lower:
        return "league_standings"
    if "Î¶¨Í∑∏ÎìùÏ†êÏàúÏúÑ" in query_lower or "ÎìùÏ†êÏàúÏúÑ" in query_lower:
        return "league_scorers"
    if "ÏïΩÌíàÍ≤ÄÏÉâ" in query_lower:
        return "drug"
    if "Í≥µÌïôÎÖºÎ¨∏" in query_lower or "arxiv" in query_lower:
        return "arxiv_search"
    if "ÏùòÌïôÎÖºÎ¨∏" in query_lower:
        return "pubmed_search"
    if "Í≤ÄÏÉâ" in query_lower:
        return "naver_search"
    if "mbti" in query_lower:
        if "Ïú†Ìòï" in query_lower or "ÏÑ§Î™Ö" in query_lower:
            return "mbti_types"
        return "mbti"
    if "Îã§Ï§ëÏßÄÎä•" in query_lower or "multi_iq" in query_lower:
        if "Ïú†Ìòï" in query_lower or "ÏÑ§Î™Ö" in query_lower:
            return "multi_iq_types"
        if "ÏßÅÏóÖ" in query_lower or "Ï∂îÏ≤ú" in query_lower:
            return "multi_iq_jobs"
        return "multi_iq"
    if any(greeting in query_lower for greeting in GREETINGS):
        return "conversation"
    return "conversation"

def process_query(query, messages):
    cache_key = f"query:{hash(query)}"
    cached = cache_handler.get(cache_key)
    if cached is not None:
        return cached, False
    
    query_type = needs_search(query)
    query_lower = query.strip().lower().replace(" ", "")
    
    with ThreadPoolExecutor() as executor:
        if query_type == "weather":
            future = executor.submit(weather_api.get_city_weather, extract_city_from_query(query))
            result = future.result()
            cache_handler.setex(cache_key, 600, result)
            return result, False
        elif query_type == "tomorrow_weather":
            future = executor.submit(weather_api.get_forecast_by_day, extract_city_from_query(query), 1)
            result = future.result()
            cache_handler.setex(cache_key, 600, result)
            return result, False
        elif query_type == "time":
            if "Ïò§ÎäòÎÇ†Ïßú" in query_lower or "ÌòÑÏû¨ÎÇ†Ïßú" in query_lower or "Í∏àÏùºÎÇ†Ïßú" in query_lower:
                result = get_kst_time()
            else:
                city = extract_city_from_time_query(query)
                future = executor.submit(get_time_by_city, city)
                result = future.result()
            cache_handler.setex(cache_key, 600, result)
            return result, False
        elif query_type == "league_standings":
            league_key = extract_league_from_query(query)
            if league_key:
                league_info = LEAGUE_MAPPING[league_key]
                future = executor.submit(football_api.fetch_league_standings, league_info["code"], league_info["name"])
                result = future.result()
                result = result["error"] if "error" in result else {
                    "header": f"{result['league_name']} Î¶¨Í∑∏ ÏàúÏúÑ",
                    "table": result["data"],
                    "footer": "Îçî Í∂ÅÍ∏àÌïú Ï†ê ÏûàÎÇòÏöî? üòä"
                }
                cache_handler.setex(cache_key, 600, result)
            else:
                result = "ÏßÄÏõêÌïòÏßÄ ÏïäÎäî Î¶¨Í∑∏ÏûÖÎãàÎã§. üòì ÏßÄÏõê Î¶¨Í∑∏: EPL, LaLiga, Bundesliga, Serie A, Ligue 1"
            return result, False
        elif query_type == "league_scorers":
            league_key = extract_league_from_query(query)
            if league_key:
                league_info = LEAGUE_MAPPING[league_key]
                future = executor.submit(football_api.fetch_league_scorers, league_info["code"], league_info["name"])
                try:
                    result = future.result()
                    result = result["error"] if "error" in result else {
                        "header": f"{result['league_name']} Î¶¨Í∑∏ ÎìùÏ†êÏàúÏúÑ (ÏÉÅÏúÑ 10Î™Ö)",
                        "table": result["data"],
                        "footer": "Îçî Í∂ÅÍ∏àÌïú Ï†ê ÏûàÎÇòÏöî? üòä"
                    }
                    cache_handler.setex(cache_key, 600, result)
                except Exception as e:
                    result = f"Î¶¨Í∑∏ ÎìùÏ†êÏàúÏúÑ Ï°∞Ìöå Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)} üòì"
            else:
                result = "ÏßÄÏõêÌïòÏßÄ ÏïäÎäî Î¶¨Í∑∏ÏûÖÎãàÎã§. üòì ÏßÄÏõê Î¶¨Í∑∏: EPL, LaLiga, Bundesliga, Serie A, Ligue 1"
            return result, False
        elif query_type == "drug":
            future = executor.submit(get_drug_info, query)
            result = future.result()
            cache_handler.setex(cache_key, 600, result)
            return result, False
        elif query_type == "arxiv_search":
            keywords = query.replace("Í≥µÌïôÎÖºÎ¨∏", "").replace("arxiv", "").strip()
            future = executor.submit(get_arxiv_papers, keywords)
            result = future.result()
            cache_handler.setex(cache_key, 600, result)
            return result, False
        elif query_type == "pubmed_search":
            keywords = query.replace("ÏùòÌïôÎÖºÎ¨∏", "").strip()
            future = executor.submit(get_pubmed_papers, keywords)
            result = future.result()
            cache_handler.setex(cache_key, 600, result)
            return result, False
        elif query_type == "naver_search":
            search_query = query.lower().replace("Í≤ÄÏÉâ", "").strip()
            future = executor.submit(get_naver_api_results, search_query)
            result = future.result()
            cache_handler.setex(cache_key, 600, result)
            return result, False
        elif query_type == "mbti":
            result = (
                "MBTI Í≤ÄÏÇ¨Î•º ÏõêÌïòÏãúÎÇòÏöî? ‚ú® ÏïÑÎûò ÏÇ¨Ïù¥Ìä∏ÏóêÏÑú Î¨¥Î£åÎ°ú ÏÑ±Í≤© Ïú†Ìòï Í≤ÄÏÇ¨Î•º Ìï† Ïàò ÏûàÏñ¥Ïöî! üòä\n"
                "[16Personalities MBTI Í≤ÄÏÇ¨](https://www.16personalities.com/ko/%EB%AC%B4%EB%A3%8C-%EC%84%B1%EA%B2%A9-%EC%9C%A0%ED%98%95-%EA%B2%80%EC%82%AC) üåü\n"
                "Ïù¥ ÏÇ¨Ïù¥Ìä∏Îäî 16Í∞ÄÏßÄ ÏÑ±Í≤© Ïú†ÌòïÏùÑ Í∏∞Î∞òÏúºÎ°ú Ìïú ÌÖåÏä§Ìä∏Î•º Ï†úÍ≥µÌïòÎ©∞, Í≤∞Í≥ºÏóê Îî∞Îùº ÏÑ±Í≤© ÏÑ§Î™ÖÍ≥º Ïù∏Í∞ÑÍ¥ÄÍ≥Ñ Ï°∞Ïñ∏ Îì±ÏùÑ ÌôïÏù∏Ìï† Ïàò ÏûàÏñ¥Ïöî! üí°"
            )
            cache_handler.setex(cache_key, 600, result)
            return result, False
        elif query_type == "mbti_types":
            specific_type = query_lower.replace("mbti", "").replace("Ïú†Ìòï", "").replace("ÏÑ§Î™Ö", "").strip().upper()
            if specific_type in mbti_descriptions:
                result = f"### üé≠ {specific_type} Ìïú Ï§Ñ ÏÑ§Î™Ö\n- ‚úÖ **{specific_type}** {mbti_descriptions[specific_type]}"
            else:
                result = mbti_full_description
            cache_handler.setex(cache_key, 600, result)
            return result, False
        elif query_type == "multi_iq":
            result = (
                "Îã§Ï§ëÏßÄÎä• Í≤ÄÏÇ¨Î•º ÏõêÌïòÏãúÎÇòÏöî? üéâ ÏïÑÎûò ÏÇ¨Ïù¥Ìä∏ÏóêÏÑú Î¨¥Î£åÎ°ú Îã§Ï§ëÏßÄÎä• ÌÖåÏä§Ìä∏Î•º Ìï¥Î≥º Ïàò ÏûàÏñ¥Ïöî! üòÑ\n"
                "[Multi IQ Test](https://multiiqtest.com/) üöÄ\n"
                "Ïù¥ ÏÇ¨Ïù¥Ìä∏Îäî ÌïòÏõåÎìú Í∞ÄÎìúÎÑàÏùò Îã§Ï§ëÏßÄÎä• Ïù¥Î°†ÏùÑ Í∏∞Î∞òÏúºÎ°ú Ìïú ÌÖåÏä§Ìä∏Î•º Ï†úÍ≥µÌïòÎ©∞, Îã§ÏñëÌïú ÏßÄÎä• ÏòÅÏó≠ÏùÑ ÌèâÍ∞ÄÌï¥Ï§çÎãàÎã§! üìö‚ú®"
            )
            cache_handler.setex(cache_key, 600, result)
            return result, False
        elif query_type == "multi_iq_types":
            specific_type = query_lower.replace("Îã§Ï§ëÏßÄÎä•", "").replace("multi_iq", "").replace("Ïú†Ìòï", "").replace("ÏÑ§Î™Ö", "").strip().replace(" ", "")
            if specific_type in multi_iq_descriptions:
                result = f"### üé® {specific_type.replace('ÏßÄÎä•', ' ÏßÄÎä•')} Ìïú Ï§Ñ ÏÑ§Î™Ö\n- üìñ **{specific_type.replace('ÏßÄÎä•', ' ÏßÄÎä•')}** {multi_iq_descriptions[specific_type]['description']}"
            else:
                result = multi_iq_full_description
            cache_handler.setex(cache_key, 600, result)
            return result, False
        elif query_type == "multi_iq_jobs":
            specific_type = query_lower.replace("Îã§Ï§ëÏßÄÎä•", "").replace("multi_iq", "").replace("ÏßÅÏóÖ", "").replace("Ï∂îÏ≤ú", "").strip().replace(" ", "")
            if specific_type in multi_iq_descriptions:
                result = f"### üé® {specific_type.replace('ÏßÄÎä•', ' ÏßÄÎä•')} Ï∂îÏ≤ú ÏßÅÏóÖ\n- üìñ **{specific_type.replace('ÏßÄÎä•', ' ÏßÄÎä•')}**: {multi_iq_descriptions[specific_type]['description']}- **Ï∂îÏ≤ú ÏßÅÏóÖ**: {multi_iq_descriptions[specific_type]['jobs']}"
            else:
                result = multi_iq_full_description
            cache_handler.setex(cache_key, 600, result)
            return result, False
        elif query_type == "conversation":
            if query_lower in GREETINGS:
                result = GREETING_RESPONSE
                cache_handler.setex(cache_key, 600, result)
                return result, False
            elif "Ïò§ÎäòÎÇ†Ïßú" in query_lower or "ÌòÑÏû¨ÎÇ†Ïßú" in query_lower or "Í∏àÏùºÎÇ†Ïßú" in query_lower:
                result = get_kst_time()
                cache_handler.setex(cache_key, 600, result)
                return result, False
            else:
                response, is_stream = asyncio.run(get_conversational_response(query, messages))
                return response, is_stream
        else:
            result = "ÏïÑÏßÅ ÏßÄÏõêÌïòÏßÄ ÏïäÎäî Í∏∞Îä•Ïù¥ÏóêÏöî. üòÖ"
            cache_handler.setex(cache_key, 600, result)
            return result, False

def show_chat_dashboard():
    st.title("Chat with AI ü§ñ")
    
    # ÎèÑÏõÄÎßê Î≤ÑÌäº
    if st.button("ÎèÑÏõÄÎßê ‚ÑπÔ∏è"):
        st.info(
            "Ï±óÎ¥áÍ≥º Îçî ÏâΩÍ≤å ÎåÄÌôîÌïòÎäî Î∞©Î≤ïÏù¥ÏóêÏöî! üëá:\n"
            "1. **ÎÇ†Ïî®** ‚òÄÔ∏è: '[ÎèÑÏãúÎ™Ö] ÎÇ†Ïî®' (Ïòà: ÏÑúÏö∏ ÎÇ†Ïî®)\n"
            "2. **ÏãúÍ∞Ñ/ÎÇ†Ïßú** ‚è±Ô∏è: '[ÎèÑÏãúÎ™Ö] ÏãúÍ∞Ñ' ÎòêÎäî 'Ïò§Îäò ÎÇ†Ïßú' (Ïòà: Î∂ÄÏÇ∞ ÏãúÍ∞Ñ, Í∏àÏùº ÎÇ†Ïßú)\n"
            "3. **Î¶¨Í∑∏ÏàúÏúÑ** ‚öΩ: '[Î¶¨Í∑∏ Ïù¥Î¶Ñ] Î¶¨Í∑∏ ÏàúÏúÑ ÎòêÎäî Î¶¨Í∑∏ÎìùÏ†êÏàúÏúÑ' (Ïòà: EPL Î¶¨Í∑∏ÏàúÏúÑ, EPL Î¶¨Í∑∏ÎìùÏ†êÏàúÏúÑ)\n"
            "   - ÏßÄÏõê Î¶¨Í∑∏: EPL, LaLiga, Bundesliga, Serie A, Ligue 1, ChampionsLeague\n"
            "4. **ÏïΩÌíàÍ≤ÄÏÉâ** üíä: 'ÏïΩÌíàÍ≤ÄÏÉâ [ÏïΩ Ïù¥Î¶Ñ]' (Ïòà: ÏïΩÌíàÍ≤ÄÏÉâ Í≤åÎ≥¥Î¶∞)\n"
            "5. **Í≥µÌïôÎÖºÎ¨∏** üìö: 'Í≥µÌïôÎÖºÎ¨∏ [ÌÇ§ÏõåÎìú]' (Ïòà: Í≥µÌïôÎÖºÎ¨∏ Multimodal AI)\n"
            "6. **ÏùòÌïôÎÖºÎ¨∏** ü©∫: 'ÏùòÌïôÎÖºÎ¨∏ [ÌÇ§ÏõåÎìú]' (Ïòà: ÏùòÌïôÎÖºÎ¨∏ cancer therapy)\n"
            "7. **Í≤ÄÏÉâ** üåê: 'Í≤ÄÏÉâ ÌÇ§ÏõåÎìú' (Ïòà: Í≤ÄÏÉâ ÏµúÍ∑º Ï†ÑÏãúÌöå Ï∂îÏ≤ú)\n"
            "8. **MBTI** ‚ú®: 'MBTI' ÎòêÎäî 'MBTI Ïú†Ìòï' (Ïòà: MBTI Í≤ÄÏÇ¨, INTJ ÏÑ§Î™Ö)\n"
            "9. **Îã§Ï§ëÏßÄÎä•** üéâ: 'Îã§Ï§ëÏßÄÎä•' ÎòêÎäî 'Îã§Ï§ëÏßÄÎä• Ïú†Ìòï' (Ïòà: Îã§Ï§ëÏßÄÎä• Í≤ÄÏÇ¨, Ïñ∏Ïñ¥ÏßÄÎä• ÏßÅÏóÖ)\n\n"
            "Í∂ÅÍ∏àÌïú Ï†ê ÏûàÏúºÎ©¥ ÏßàÎ¨∏Ìï¥Ï£ºÏÑ∏Ïöî! üòä"
        )
    
    # ÏµúÍ∑º Î©îÏãúÏßÄ ÌëúÏãú
    for msg in st.session_state.messages[-10:]:
        with st.chat_message(msg['role']):
            if isinstance(msg['content'], dict) and "table" in msg['content']:
                st.markdown(f"### {msg['content']['header']}")
                st.dataframe(pd.DataFrame(msg['content']['table']), use_container_width=True, hide_index=True)
                st.markdown(msg['content']['footer'])
            else:
                st.markdown(msg['content'], unsafe_allow_html=True)
    
    # ÏÇ¨Ïö©Ïûê ÏûÖÎ†• Ï≤òÎ¶¨
    if user_prompt := st.chat_input("ÏßàÎ¨∏Ìï¥ Ï£ºÏÑ∏Ïöî!"):
        st.session_state.messages.append({"role": "user", "content": user_prompt})
        
        with st.chat_message("user"):
            st.markdown(user_prompt)
        
        with st.chat_message("assistant"):
            # Spinner Ï†ÅÏö©
            with st.spinner("ÏùëÎãµÏùÑ Ï§ÄÎπÑ Ï§ëÏûÖÎãàÎã§... ‚è≥"):
                try:
                    start_time = time.time()
                    response, is_stream = process_query(user_prompt, st.session_state.messages)
                    time_taken = round(time.time() - start_time, 2)
                    
                    # Ïä§Ìä∏Î¶¨Î∞ç ÏùëÎãµ Ï≤òÎ¶¨
                    if is_stream:
                        chatbot_response = ""
                        message_placeholder = st.empty()
                        for chunk in response:
                            if hasattr(chunk, 'choices') and len(chunk.choices) > 0 and hasattr(chunk.choices[0], 'delta') and hasattr(chunk.choices[0].delta, 'content'):
                                content = chunk.choices[0].delta.content
                                if content is not None:
                                    chatbot_response += content
                                    message_placeholder.markdown(chatbot_response + "‚ñå")
                            else:
                                logger.warning(f"ÏòàÏÉÅÏπò Î™ªÌïú Ï≤≠ÌÅ¨ Íµ¨Ï°∞: {chunk}")
                        message_placeholder.markdown(chatbot_response)
                        st.session_state.messages.append({"role": "assistant", "content": chatbot_response})
                    else:
                        # Ï†ïÏ†Å ÏùëÎãµ Ï≤òÎ¶¨
                        if isinstance(response, dict) and "table" in response:
                            st.markdown(f"### {response['header']}")
                            st.dataframe(response['table'], use_container_width=True, hide_index=True)
                            st.markdown(response['footer'])
                        else:
                            st.markdown(response, unsafe_allow_html=True)
                        st.session_state.messages.append({"role": "assistant", "content": response})
                    
                    # ÎåÄÌôî Í∏∞Î°ù ÎπÑÎèôÍ∏∞ Ï†ÄÏû•
                    async_save_chat_history(st.session_state.user_id, st.session_state.session_id, user_prompt, response, time_taken)
                
                except Exception as e:
                    error_msg = f"ÏùëÎãµÏùÑ Ï§ÄÎπÑÌïòÎã§ Î¨∏Ï†úÍ∞Ä ÏÉùÍ≤ºÏñ¥Ïöî: {str(e)} üòì"
                    logger.error(f"ÎåÄÌôî Ï≤òÎ¶¨ Ï§ë Ïò§Î•ò: {str(e)}", exc_info=True)
                    st.markdown(error_msg, unsafe_allow_html=True)
                    st.session_state.messages.append({"role": "assistant", "content": error_msg})



def show_login_page():
    st.title("Î°úÍ∑∏Ïù∏ ü§ó")
    with st.form("login_form"):
        nickname = st.text_input("ÎãâÎÑ§ÏûÑ", placeholder="Ïòà: ÌõÑÏïà")
        submit_button = st.form_submit_button("ÏãúÏûëÌïòÍ∏∞ üöÄ")
        
        if submit_button and nickname:
            try:
                user_id, is_existing = create_or_get_user(nickname)
                st.session_state.user_id = user_id
                st.session_state.is_logged_in = True
                st.session_state.messages = [{"role": "assistant", "content": "ÏïàÎÖïÌïòÏÑ∏Ïöî! Î¨¥ÏóáÏùÑ ÎèÑÏôÄÎìúÎ¶¥ÍπåÏöî? ÎèÑÏõÄÎßêÎèÑ ÌôúÏö©Ìï¥ Î≥¥ÏÑ∏Ïöî üòä"}]
                st.session_state.session_id = str(uuid.uuid4())
                st.toast(f"ÌôòÏòÅÌï©ÎãàÎã§, {nickname}Îãò! üéâ")
                time.sleep(1)
                st.rerun()
            except Exception:
                st.toast("Î°úÍ∑∏Ïù∏ Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§. Îã§Ïãú ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî.", icon="‚ùå")

def main():
    init_session_state()
    if not st.session_state.is_logged_in:
        show_login_page()
    else:
        show_chat_dashboard()

if __name__ == "__main__":
    main()
