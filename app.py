import streamlit as st
import time
import uuid
from supabase import create_client
import os
from datetime import datetime, timedelta
import pytz
import logging
import requests
from bs4 import BeautifulSoup
import pandas as pd
from googlesearch import search
from g4f.client import Client
from timezonefinder import TimezoneFinder
import re
import json
import urllib.request
import urllib.parse
from langdetect import detect
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry
from concurrent.futures import ThreadPoolExecutor
import arxiv
from diskcache import Cache
from functools import lru_cache

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
WEATHER_API_KEY = os.getenv("WEATHER_API_KEY")
DRUG_API_KEY = os.getenv("DRUG_API_KEY")
NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.WARNING if os.getenv("ENV") == "production" else logging.INFO)
logger = logging.getLogger("HybridChat")

# ìºì‹œ ì„¤ì •
cache = Cache("cache_directory")

class MemoryCache:
    def __init__(self):
        self.cache = {}
        self.expiry = {}
    
    def get(self, key):
        if key in self.cache and time.time() < self.expiry[key]:
            return self.cache[key]
        return cache.get(key)
    
    def setex(self, key, ttl, value):
        self.cache[key] = value
        self.expiry[key] = time.time() + ttl
        cache.set(key, value, expire=ttl)

cache_handler = MemoryCache()

# WeatherAPI í´ë˜ìŠ¤
class WeatherAPI:
    def __init__(self, cache_ttl=600):
        self.cache = cache_handler
        self.cache_ttl = cache_ttl

    def fetch_weather(self, url, params):
        session = requests.Session()
        retry_strategy = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("https://", adapter)
        response = session.get(url, params=params, timeout=5)
        response.raise_for_status()
        return response.json()

    @lru_cache(maxsize=100)
    def get_city_info(self, city_name):
        cache_key = f"city_info:{city_name}"
        cached = self.cache.get(cache_key)
        if cached:
            return cached
        url = "http://api.openweathermap.org/geo/1.0/direct"
        params = {'q': city_name, 'limit': 1, 'appid': WEATHER_API_KEY}
        data = self.fetch_weather(url, params)
        if data and len(data) > 0:
            city_info = {"name": data[0]["name"], "lat": data[0]["lat"], "lon": data[0]["lon"]}
            self.cache.setex(cache_key, 86400, city_info)
            return city_info
        return None

    def get_city_weather(self, city_name):
        cache_key = f"weather:{city_name}"
        cached_data = self.cache.get(cache_key)
        if cached_data:
            return cached_data
        
        city_info = self.get_city_info(city_name)
        if not city_info:
            return f"'{city_name}'ì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        url = "https://api.openweathermap.org/data/2.5/weather"
        params = {'lat': city_info["lat"], 'lon': city_info["lon"], 'appid': WEATHER_API_KEY, 'units': 'metric', 'lang': 'kr'}
        data = self.fetch_weather(url, params)
        weather_emojis = {'Clear': 'â˜€ï¸', 'Clouds': 'â˜ï¸', 'Rain': 'ğŸŒ§ï¸', 'Snow': 'â„ï¸', 'Thunderstorm': 'â›ˆï¸', 'Drizzle': 'ğŸŒ¦ï¸', 'Mist': 'ğŸŒ«ï¸'}
        weather_emoji = weather_emojis.get(data['weather'][0]['main'], 'ğŸŒ¤ï¸')
        result = (
            f"í˜„ì¬ {data['name']}, {data['sys']['country']} ë‚ ì”¨ {weather_emoji}\n"
            f"ë‚ ì”¨: {data['weather'][0]['description']}\n"
            f"ì˜¨ë„: {data['main']['temp']}Â°C\n"
            f"ì²´ê°: {data['main']['feels_like']}Â°C\n"
            f"ìŠµë„: {data['main']['humidity']}%\n"
            f"í’ì†: {data['wind']['speed']}m/s\n"
            f"ë” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"
        )
        self.cache.setex(cache_key, self.cache_ttl, result)
        return result

    def get_forecast_by_day(self, city_name, days_from_today=1):
        cache_key = f"forecast:{city_name}:{days_from_today}"
        cached_data = self.cache.get(cache_key)
        if cached_data:
            return cached_data
        
        city_info = self.get_city_info(city_name)
        if not city_info:
            return f"'{city_name}'ì˜ ë‚ ì”¨ ì˜ˆë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        url = "https://api.openweathermap.org/data/2.5/forecast"
        params = {'lat': city_info["lat"], 'lon': city_info["lon"], 'appid': WEATHER_API_KEY, 'units': 'metric', 'lang': 'kr'}
        data = self.fetch_weather(url, params)
        target_date = (datetime.now() + timedelta(days=days_from_today)).strftime('%Y-%m-%d')
        forecast_text = f"{city_info['name']}ì˜ {target_date} ë‚ ì”¨ ì˜ˆë³´ ğŸŒ¤ï¸\n"
        weather_emojis = {'Clear': 'â˜€ï¸', 'Clouds': 'â˜ï¸', 'Rain': 'ğŸŒ§ï¸', 'Snow': 'â„ï¸', 'Thunderstorm': 'â›ˆï¸', 'Drizzle': 'ğŸŒ¦ï¸', 'Mist': 'ğŸŒ«ï¸'}
        
        found = False
        for forecast in data['list']:
            dt = datetime.fromtimestamp(forecast['dt']).strftime('%Y-%m-%d')
            if dt == target_date:
                found = True
                time_only = datetime.fromtimestamp(forecast['dt']).strftime('%H:%M')
                weather_emoji = weather_emojis.get(forecast['weather'][0]['main'], 'ğŸŒ¤ï¸')
                forecast_text += (
                    f"â° {time_only} {forecast['weather'][0]['description']} {weather_emoji} "
                    f"{forecast['main']['temp']}Â°C ğŸ’§{forecast['main']['humidity']}% ğŸŒ¬ï¸{forecast['wind']['speed']}m/s\n"
                )
        
        result = forecast_text + "\në” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š" if found else f"'{city_name}'ì˜ {target_date} ë‚ ì”¨ ì˜ˆë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        self.cache.setex(cache_key, self.cache_ttl, result)
        return result

    def get_weekly_forecast(self, city_name):
        cache_key = f"weekly_forecast:{city_name}"
        cached_data = self.cache.get(cache_key)
        if cached_data:
            return cached_data
        
        city_info = self.get_city_info(city_name)
        if not city_info:
            return f"'{city_name}'ì˜ ì£¼ê°„ ì˜ˆë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        url = "https://api.openweathermap.org/data/2.5/forecast"
        params = {'lat': city_info["lat"], 'lon': city_info["lon"], 'appid': WEATHER_API_KEY, 'units': 'metric', 'lang': 'kr'}
        data = self.fetch_weather(url, params)
        today = datetime.now().date()
        week_end = today + timedelta(days=6)
        daily_forecast = {}
        weekdays_kr = ["ì›”ìš”ì¼", "í™”ìš”ì¼", "ìˆ˜ìš”ì¼", "ëª©ìš”ì¼", "ê¸ˆìš”ì¼", "í† ìš”ì¼", "ì¼ìš”ì¼"]
        today_weekday = today.weekday()
        
        for forecast in data['list']:
            dt = datetime.fromtimestamp(forecast['dt']).date()
            if today <= dt <= week_end:
                dt_str = dt.strftime('%Y-%m-%d')
                if dt_str not in daily_forecast:
                    weekday_idx = (today_weekday + (dt - today).days) % 7
                    daily_forecast[dt_str] = {
                        'weekday': weekdays_kr[weekday_idx],
                        'temp_min': forecast['main']['temp_min'],
                        'temp_max': forecast['main']['temp_max'],
                        'weather': forecast['weather'][0]['description']
                    }
                else:
                    daily_forecast[dt_str]['temp_min'] = min(daily_forecast[dt_str]['temp_min'], forecast['main']['temp_min'])
                    daily_forecast[dt_str]['temp_max'] = max(daily_forecast[dt_str]['temp_max'], forecast['main']['temp_max'])
        
        today_str = today.strftime('%Y-%m-%d')
        today_weekday_str = weekdays_kr[today_weekday]
        forecast_text = f"{today_str}({today_weekday_str}) ê¸°ì¤€ {city_info['name']}ì˜ ì£¼ê°„ ë‚ ì”¨ ì˜ˆë³´ ğŸŒ¤ï¸\n"
        weather_emojis = {'Clear': 'â˜€ï¸', 'Clouds': 'â˜ï¸', 'Rain': 'ğŸŒ§ï¸', 'Snow': 'â„ï¸', 'Thunderstorm': 'â›ˆï¸', 'Drizzle': 'ğŸŒ¦ï¸', 'Mist': 'ğŸŒ«ï¸'}
        
        for date, info in daily_forecast.items():
            weather_emoji = weather_emojis.get(info['weather'].split()[0], 'ğŸŒ¤ï¸')
            forecast_text += (
                f"{info['weekday']}: {info['weather']} {weather_emoji} "
                f"ìµœì € {info['temp_min']}Â°C ìµœê³  {info['temp_max']}Â°C\n"
            )
        
        result = forecast_text + "\në” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"
        self.cache.setex(cache_key, self.cache_ttl, result)
        return result

# ì´ˆê¸°í™”
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
client = Client()
weather_api = WeatherAPI()
naver_request_count = 0
NAVER_DAILY_LIMIT = 25000
st.set_page_config(page_title="AI ì±—ë´‡", page_icon="ğŸ¤–")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "is_logged_in" not in st.session_state:
    st.session_state.is_logged_in = False
    st.session_state.user_id = None
    st.session_state.chat_history = []
    st.session_state.session_id = str(uuid.uuid4())

# ë„ì‹œ ì¶”ì¶œ
CITY_PATTERNS = [
    re.compile(r'(?:ì˜¤ëŠ˜|ë‚´ì¼|ëª¨ë ˆ|ì´ë²ˆ ì£¼|ì£¼ê°„)?\s*([ê°€-í£a-zA-Z\s]{2,20}(?:ì‹œ|êµ°|city)?)ì˜?\s*ë‚ ì”¨', re.IGNORECASE),
    re.compile(r'(?:ì˜¤ëŠ˜|ë‚´ì¼|ëª¨ë ˆ|ì´ë²ˆ ì£¼|ì£¼ê°„)?\s*([ê°€-í£a-zA-Z\s]{2,20}(?:ì‹œ|êµ°|city)?)\s*ë‚ ì”¨', re.IGNORECASE),
    re.compile(r'ë‚ ì”¨\s*(?:ì˜¤ëŠ˜|ë‚´ì¼|ëª¨ë ˆ|ì´ë²ˆ ì£¼|ì£¼ê°„)?\s*([ê°€-í£a-zA-Z\s]{2,20}(?:ì‹œ|êµ°|city)?)', re.IGNORECASE),
]
def extract_city_from_query(query):
    for pattern in CITY_PATTERNS:
        match = pattern.search(query)
        if match:
            city = match.group(1).strip()
            if city not in ["ì˜¤ëŠ˜", "ë‚´ì¼", "ëª¨ë ˆ", "ì´ë²ˆ ì£¼", "ì£¼ê°„", "í˜„ì¬"]:
                return city
    return "ì„œìš¸"

TIME_CITY_PATTERNS = [
    re.compile(r'([ê°€-í£a-zA-Z]{2,20}(?:ì‹œ|êµ°)?)ì˜?\s*ì‹œê°„'),
    re.compile(r'([ê°€-í£a-zA-Z]{2,20}(?:ì‹œ|êµ°)?)\s*ì‹œê°„'),
    re.compile(r'ì‹œê°„\s*([ê°€-í£a-zA-Z\s]{2,20}(?:ì‹œ|êµ°)?)'),
]
def extract_city_from_time_query(query):
    for pattern in TIME_CITY_PATTERNS:
        match = pattern.search(query)
        if match:
            city = match.group(1).strip()
            if city != "í˜„ì¬":
                return city
    return "ì„œìš¸"

# ì‹œê°„ ì •ë³´
def get_time_by_city(city_name="ì„œìš¸"):
    city_info = weather_api.get_city_info(city_name)
    if not city_info:
        return f"'{city_name}'ì˜ ì‹œê°„ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    tf = TimezoneFinder()
    timezone_str = tf.timezone_at(lng=city_info["lon"], lat=city_info["lat"]) or "Asia/Seoul"
    timezone = pytz.timezone(timezone_str)
    city_time = datetime.now(timezone)
    am_pm = "ì˜¤ì „" if city_time.strftime("%p") == "AM" else "ì˜¤í›„"
    return f"í˜„ì¬ {city_name} ì‹œê°„: {city_time.strftime('%Yë…„ %mì›” %dì¼ %p %I:%M')} â°\në” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"

# ì‚¬ìš©ì ë° ì±„íŒ… ê¸°ë¡ ê´€ë¦¬
def create_or_get_user(nickname):
    user = supabase.table("users").select("*").eq("nickname", nickname).execute()
    if user.data:
        return user.data[0]["id"], True
    new_user = supabase.table("users").insert({"nickname": nickname, "created_at": datetime.now().isoformat()}).execute()
    return new_user.data[0]["id"], False

def save_chat_history(user_id, session_id, question, answer, time_taken):
    supabase.table("chat_history").insert({
        "user_id": user_id, "session_id": session_id, "question": question,
        "answer": answer, "time_taken": time_taken, "created_at": datetime.now().isoformat()
    }).execute()

def async_save_chat_history(user_id, session_id, question, answer, time_taken):
    threading.Thread(target=save_chat_history, args=(user_id, session_id, question, answer, time_taken)).start()

# ì˜ì•½í’ˆ ê²€ìƒ‰
def get_drug_info(drug_query):
    drug_name = drug_query.replace("ì•½í’ˆê²€ìƒ‰", "").strip()
    cache_key = f"drug:{drug_name}"
    cached = cache_handler.get(cache_key)
    if cached:
        return cached
    
    url = 'http://apis.data.go.kr/1471000/DrbEasyDrugInfoService/getDrbEasyDrugList'
    params = {'serviceKey': DRUG_API_KEY, 'pageNo': '1', 'numOfRows': '1', 'itemName': urllib.parse.quote(drug_name), 'type': 'json'}
    try:
        response = requests.get(url, params=params, timeout=5)
        response.raise_for_status()
        data = response.json()
        if 'body' in data and 'items' in data['body'] and data['body']['items']:
            item = data['body']['items'][0]
            efcy = item.get('efcyQesitm', 'ì •ë³´ ì—†ìŒ')[:150] + ("..." if len(item.get('efcyQesitm', '')) > 150 else "")
            use_method = item.get('useMethodQesitm', 'ì •ë³´ ì—†ìŒ')[:150] + ("..." if len(item.get('useMethodQesitm', '')) > 150 else "")
            atpn = item.get('atpnQesitm', 'ì •ë³´ ì—†ìŒ')[:150] + ("..." if len(item.get('atpnQesitm', '')) > 150 else "")
            result = (
                f"ğŸ’Š **ì˜ì•½í’ˆ ì •ë³´** ğŸ’Š\n"
                f"âœ… **ì•½í’ˆëª…**: {item.get('itemName', 'ì •ë³´ ì—†ìŒ')}\n"
                f"âœ… **ì œì¡°ì‚¬**: {item.get('entpName', 'ì •ë³´ ì—†ìŒ')}\n"
                f"âœ… **íš¨ëŠ¥**: {efcy}\n"
                f"âœ… **ìš©ë²•ìš©ëŸ‰**: {use_method}\n"
                f"âœ… **ì£¼ì˜ì‚¬í•­**: {atpn}\n"
                f"â„¹ï¸ ìì„¸í•œ ì •ë³´ëŠ” [ì•½í•™ì •ë³´ì›](https://www.health.kr/searchDrug/search_detail.asp)ì—ì„œ í™•ì¸í•˜ì„¸ìš”! ğŸ©º\n"
                f"ë” ê¶ê¸ˆí•œ ì  ìˆìœ¼ì‹ ê°€ìš”? ğŸ˜Š"
            )
            cache_handler.setex(cache_key, 86400, result)
            return result
        else:
            return search_and_summarize_drug(drug_name)
    except Exception as e:
        logger.error(f"ì•½í’ˆ API ì˜¤ë¥˜: {str(e)}")
        return search_and_summarize_drug(drug_name)

def search_and_summarize_drug(drug_name):
    search_results = search_and_summarize(f"{drug_name} ì˜ì•½í’ˆ ì •ë³´", num_results=5)
    if not search_results.empty:
        return f"'{drug_name}' ê³µì‹ ì •ë³´ ì—†ìŒ. ì›¹ ê²€ìƒ‰ ìš”ì•½:\n{get_ai_summary(search_results)}"
    return f"'{drug_name}' ì˜ì•½í’ˆ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

# Naver API ë° ì›¹ ê²€ìƒ‰
def get_naver_api_results(query):
    global naver_request_count
    if naver_request_count >= NAVER_DAILY_LIMIT:
        return search_and_summarize(query, num_results=5)
    enc_text = urllib.parse.quote(query)
    url = f"https://openapi.naver.com/v1/search/webkr?query={enc_text}&display=10&sort=date"
    request = urllib.request.Request(url)
    request.add_header("X-Naver-Client-Id", NAVER_CLIENT_ID)
    request.add_header("X-Naver-Client-Secret", NAVER_CLIENT_SECRET)
    results = []
    try:
        response = urllib.request.urlopen(request)
        naver_request_count += 1
        if response.getcode() == 200:
            data = json.loads(response.read().decode('utf-8'))
            for item in data.get('items', [])[:5]:
                title = re.sub(r'<b>|</b>', '', item['title'])
                contents = re.sub(r'<b>|</b>', '', item.get('description', 'ë‚´ìš© ì—†ìŒ'))[:100] + "..."
                results.append({"title": title, "contents": contents, "url": item.get('link', ''), "date": item.get('pubDate', '')})
    except Exception:
        return search_and_summarize(query, num_results=5)
    return pd.DataFrame(results)

def search_and_summarize(query, num_results=5):
    data = []
    with ThreadPoolExecutor() as executor:
        futures = [executor.submit(requests.get, link, timeout=5) for link in search(query, num_results=num_results)]
        for future in futures:
            try:
                response = future.result()
                soup = BeautifulSoup(response.text, 'html.parser')
                title = soup.title.get_text() if soup.title else "No title"
                description = ' '.join([p.get_text() for p in soup.find_all('p')[:3]])
                data.append({"title": title, "contents": description[:500], "link": response.url})
            except Exception:
                continue
    return pd.DataFrame(data)

def get_ai_summary(search_results):
    if search_results.empty:
        return "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    context = "\n".join([f"ì¶œì²˜: {row['title']}\në‚´ìš©: {row['contents']}" for _, row in search_results.iterrows()])
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": f"ê²€ìƒ‰ ê²°ê³¼ë¥¼ 2~3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½:\n{context}"}]
    )
    summary = response.choices[0].message.content
    sources = "\nğŸ“œ **ì¶œì²˜**\n" + "\n".join([f"ğŸŒ [{row['title']}]({row['link']})" for _, row in search_results.iterrows()])
    return f"{summary}{sources}\në” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"

# ë…¼ë¬¸ ê²€ìƒ‰
def fetch_arxiv_paper(paper):
    return {
        "title": paper.title,
        "authors": ", ".join(str(a) for a in paper.authors),
        "summary": paper.summary[:200],
        "entry_id": paper.entry_id,
        "pdf_url": paper.get_pdf_url(),
        "published": paper.published.strftime('%Y-%m-%d')
    }

def get_arxiv_papers(query, max_results=3):
    cache_key = f"arxiv:{query}:{max_results}"
    cached = cache_handler.get(cache_key)
    if cached:
        return cached
    search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.SubmittedDate)
    with ThreadPoolExecutor() as executor:
        results = list(executor.map(fetch_arxiv_paper, search.results()))
    if not results:
        return "í•´ë‹¹ í‚¤ì›Œë“œë¡œ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    response = "ğŸ“š **Arxiv ë…¼ë¬¸ ê²€ìƒ‰ ê²°ê³¼** ğŸ“š\n" + "\n".join(
        [f"**ë…¼ë¬¸ {i}**\n"
         f"ğŸ“„ **ì œëª©**: {r['title']}\n"
         f"ğŸ‘¥ **ì €ì**: {r['authors']}\n"
         f"ğŸ“ **ì´ˆë¡**: {r['summary']}...\n"
         f"ğŸ”— **ë…¼ë¬¸ í˜ì´ì§€**: {r['entry_id']}\n"
         f"ğŸ“¥ **PDF ë‹¤ìš´ë¡œë“œ**: [{r['pdf_url'].split('/')[-1]}]({r['pdf_url']})\n"
         f"ğŸ“… **ì¶œíŒì¼**: {r['published']}\n"
         f"{'-' * 50}"
         for i, r in enumerate(results, 1)]
    ) + "\në” ë§ì€ ë…¼ë¬¸ì„ ë³´ê³  ì‹¶ë‹¤ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”! ğŸ˜Š"
    cache_handler.setex(cache_key, 3600, response)
    return response

# ëŒ€í™”í˜• ì‘ë‹µ
conversation_cache = MemoryCache()
def get_conversational_response(query, chat_history):
    cache_key = f"conv:{needs_search(query)}:{query}:{hash(str(chat_history[-5:]))}"
    cached = conversation_cache.get(cache_key)
    if cached:
        return cached
    messages = [{"role": "system", "content": "ì¹œì ˆí•˜ê³  ìƒí˜¸ì‘ìš©ì ì¸ AI ì±—ë´‡ì…ë‹ˆë‹¤."}] + [
        {"role": msg["role"], "content": msg["content"]} for msg in chat_history[-5:]
    ] + [{"role": "user", "content": query}]
    response = client.chat.completions.create(model="gpt-4", messages=messages)
    result = response.choices[0].message.content
    conversation_cache.setex(cache_key, 600, result)
    return result

GREETING_RESPONSES = {
    "ì•ˆë…•": "ì•ˆë…•í•˜ì„¸ìš”! ë°˜ê°‘ìŠµë‹ˆë‹¤! ğŸ˜Š",
    "ì•ˆë…• ë°˜ê°€ì›Œ": "ì•ˆë…•í•˜ì„¸ìš”! ì €ë„ ë°˜ê°‘ìŠµë‹ˆë‹¤! ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì–´ë– ì‹ ê°€ìš”? ğŸ˜„",
    "í•˜ì´": "í•˜ì´! ì¢‹ì€ í•˜ë£¨ ë³´ë‚´ì„¸ìš”! ğŸ˜Š",
    "í—¬ë¡œ": "ì•ˆë…•í•˜ì„¸ìš”! ë°˜ê°‘ìŠµë‹ˆë‹¤! ğŸ˜Š",
    "í—¤ì´": "í—¤ì´! ì˜ ì§€ë‚´ì„¸ìš”? ğŸ˜„",
    "ì™“ì—…": "ì™“ì—…! ë­í•˜ê³  ê³„ì‹ ê°€ìš”? ğŸ˜Š",
    "ì™“ì¹": "ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì–´ë– ì‹ ê°€ìš”? ğŸ˜„",
}

# ì¿¼ë¦¬ ë¶„ë¥˜
def needs_search(query):
    query_lower = query.strip().lower()
    greeting_keywords = ["ì•ˆë…•", "í•˜ì´", "ë°˜ê°€ì›Œ", "ì•ˆë‡½", "ë­í•´", "í—¬ë¡œ", "í—¬ë¡±", "í•˜ì‡", "í—¤ì´", "í—¤ì´ìš”", "ì™“ì—…", "ì™“ì¹", "ì—ì´ìš”"]
    emotion_keywords = ["ë°°ê³ í”„ë‹¤", "ë°°ê³ í”„", "ì¡¸ë¦¬ë‹¤", "í”¼ê³¤í•˜ë‹¤", "í™”ë‚¨", "ì—´ë°›ìŒ", "ì§œì¦ë‚¨", "í”¼ê³¤í•¨"]
    if any(greeting in query_lower for greeting in greeting_keywords) or \
       any(emo in query_lower for emo in emotion_keywords) or \
       len(query_lower) <= 3 and "?" not in query_lower:
        return "conversation"
    intent_keywords = ["ì¶”ì²œí•´ì¤˜", "ë­ ë¨¹ì„ê¹Œ", "ë©”ë‰´", "ë­í• ê¹Œ"]
    if any(kw in query_lower for kw in intent_keywords):
        return "conversation"
    time_keywords = ["í˜„ì¬ ì‹œê°„", "ì‹œê°„", "ëª‡ ì‹œ", "ì§€ê¸ˆ", "ëª‡ì‹œ", "ëª‡ ì‹œì•¼", "ì§€ê¸ˆ ì‹œê°„", "í˜„ì¬", "ì‹œê³„"]
    if any(keyword in query_lower for keyword in time_keywords) and \
       any(timeword in query_lower for timeword in ["ì‹œê°„", "ëª‡ì‹œ", "ëª‡ ì‹œ", "ì‹œê³„"]):
        return "time"
    weather_keywords = ["ë‚ ì”¨", "ì˜¨ë„", "ê¸°ì˜¨"]
    if any(keyword in query_lower for keyword in weather_keywords) and "ë‚´ì¼" in query_lower:
        return "tomorrow_weather"
    elif any(keyword in query_lower for keyword in weather_keywords) and "ëª¨ë ˆ" in query_lower:
        return "day_after_tomorrow_weather"
    elif any(keyword in query_lower for keyword in weather_keywords) and ("ì£¼ê°„" in query_lower or any(kw in query_lower for kw in ["ì´ë²ˆ ì£¼", "ì£¼ê°„ ì˜ˆë³´", "ì£¼ê°„ ë‚ ì”¨"])):
        return "weekly_forecast"
    elif any(keyword in query_lower for keyword in weather_keywords):
        return "weather"
    drug_keywords = ["ì•½í’ˆê²€ìƒ‰"]
    drug_pattern = r'^ì•½í’ˆê²€ìƒ‰\s+[ê°€-í£a-zA-Z]{2,10}(?:ì•½|ì •|ì‹œëŸ½|ìº¡ìŠ)?$'
    if any(keyword in query_lower for keyword in drug_keywords) and re.match(drug_pattern, query_lower):
        return "drug"
    if query_lower == "mbti ê²€ì‚¬":
        return "mbti"
    if query_lower == "ë‹¤ì¤‘ì§€ëŠ¥ ê²€ì‚¬":
        return "multi_iq"
    arxiv_keywords = ["ë…¼ë¬¸ê²€ìƒ‰", "arxiv", "paper", "research"]
    if any(kw in query_lower for kw in arxiv_keywords) and len(query_lower) > 5:
        return "arxiv_search"
    search_keywords = ["ê²€ìƒ‰", "ì•Œë ¤ì¤˜", "ì •ë³´", "ë­ì•¼", "ë¬´ì—‡ì´ì•¼", "ë¬´ì—‡ì¸ì§€", "ì°¾ì•„ì„œ", "ì •ë¦¬í•´ì¤˜", "ì„¤ëª…í•´ì¤˜", "ì•Œê³ ì‹¶ì–´", "ì•Œë ¤ì¤„ë˜"]
    if any(kw in query_lower for kw in search_keywords) and len(query_lower) > 5:
        return "web_search"
    return "general_query"

# UI í•¨ìˆ˜
def show_login_page():
    st.title("ë¡œê·¸ì¸ ğŸ¤—")
    with st.form("login_form"):
        nickname = st.text_input("ë‹‰ë„¤ì„", placeholder="ì˜ˆ: í›„ì•ˆ")
        if st.form_submit_button("ì‹œì‘í•˜ê¸° ğŸš€") and nickname:
            user_id, is_existing = create_or_get_user(nickname)
            st.session_state.user_id = user_id
            st.session_state.is_logged_in = True
            st.toast(f"í™˜ì˜í•©ë‹ˆë‹¤, {nickname}ë‹˜! ğŸ‰")
            time.sleep(1)
            st.rerun()

@st.cache_data(ttl=600)
def get_cached_response(query):
    return process_query(query)

def process_query(query):
    query_type = needs_search(query)
    if query_type == "mbti":
        return (
            "MBTI ê²€ì‚¬ë¥¼ ì›í•˜ì‹œë‚˜ìš”? âœ¨ ì•„ë˜ ì‚¬ì´íŠ¸ì—ì„œ ë¬´ë£Œë¡œ ì„±ê²© ìœ í˜• ê²€ì‚¬ë¥¼ í•  ìˆ˜ ìˆì–´ìš”! ğŸ˜Š\n"
            "[16Personalities MBTI ê²€ì‚¬](https://www.16personalities.com/ko/%EB%AC%B4%EB%A3%8C-%EC%84%B1%EA%B2%A9-%EC%9C%A0%ED%98%95-%EA%B2%80%EC%82%AC) ğŸŒŸ\n"
            "ì´ ì‚¬ì´íŠ¸ëŠ” 16ê°€ì§€ ì„±ê²© ìœ í˜•ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ í…ŒìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ë©°, ê²°ê³¼ì— ë”°ë¼ ì„±ê²© ì„¤ëª…ê³¼ ì¸ê°„ê´€ê³„ ì¡°ì–¸ ë“±ì„ í™•ì¸í•  ìˆ˜ ìˆì–´ìš”! ğŸ§ ğŸ’¡"
        )
    elif query_type == "multi_iq":
        return (
            "ë‹¤ì¤‘ì§€ëŠ¥ ê²€ì‚¬ë¥¼ ì›í•˜ì‹œë‚˜ìš”? ğŸ‰ ì•„ë˜ ì‚¬ì´íŠ¸ì—ì„œ ë¬´ë£Œë¡œ ë‹¤ì¤‘ì§€ëŠ¥ í…ŒìŠ¤íŠ¸ë¥¼ í•´ë³¼ ìˆ˜ ìˆì–´ìš”! ğŸ˜„\n"
            "[Multi IQ Test](https://multiiqtest.com/) ğŸš€\n"
            "ì´ ì‚¬ì´íŠ¸ëŠ” í•˜ì›Œë“œ ê°€ë“œë„ˆì˜ ë‹¤ì¤‘ì§€ëŠ¥ ì´ë¡ ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ í…ŒìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ë©°, ë‹¤ì–‘í•œ ì§€ëŠ¥ ì˜ì—­ì„ í‰ê°€í•´ì¤ë‹ˆë‹¤! ğŸ“šâœ¨"
        )
    elif query_type == "time":
        city = extract_city_from_time_query(query)
        return get_time_by_city(city)
    elif query_type == "weather":
        city = extract_city_from_query(query)
        return weather_api.get_city_weather(city)
    elif query_type == "tomorrow_weather":
        city = extract_city_from_query(query)
        return weather_api.get_forecast_by_day(city, days_from_today=1)
    elif query_type == "day_after_tomorrow_weather":
        city = extract_city_from_query(query)
        return weather_api.get_forecast_by_day(city, days_from_today=2)
    elif query_type == "weekly_forecast":
        city = extract_city_from_query(query)
        return weather_api.get_weekly_forecast(city)
    elif query_type == "drug":
        return get_drug_info(query)
    elif query_type == "conversation":
        if query.strip() in GREETING_RESPONSES:
            return GREETING_RESPONSES[query.strip()]
        return get_conversational_response(query, st.session_state.chat_history)
    elif query_type == "web_search":
        language = detect(query)
        if language == 'ko' and naver_request_count < NAVER_DAILY_LIMIT:
            return get_ai_summary(get_naver_api_results(query))
        return get_ai_summary(search_and_summarize(query))
    elif query_type == "arxiv_search":
        keywords = query.replace("ë…¼ë¬¸ê²€ìƒ‰", "").replace("arxiv", "").replace("paper", "").replace("research", "").strip()
        return get_arxiv_papers(keywords)
    elif query_type == "general_query":
        return get_conversational_response(query, st.session_state.chat_history)
    return "ì•„ì§ ì§€ì›í•˜ì§€ ì•ŠëŠ” ê¸°ëŠ¥ì´ì—ìš”. ğŸ˜…"

def show_chat_dashboard():
    st.title("AI ì±—ë´‡ ğŸ¤–")
    
    # ë„ì›€ë§ ë²„íŠ¼ ì¶”ê°€
    if st.button("ë„ì›€ë§ â„¹ï¸"):
        st.info(
            "ì±—ë´‡ì„ ë” ì˜ í™œìš©í•˜ë ¤ë©´ ì•„ë˜ í˜•ì‹ì„ ì°¸ê³ í•˜ì„¸ìš”:\n\n"
            "1. **ì•½í’ˆê²€ìƒ‰** ğŸ’Š: 'ì•½í’ˆê²€ìƒ‰ [ì•½ ì´ë¦„]' (ì˜ˆ: ì•½í’ˆê²€ìƒ‰ íƒ€ì´ë ˆë†€)\n"
            "2. **ë…¼ë¬¸ê²€ìƒ‰** ğŸ“š: 'ë…¼ë¬¸ê²€ìƒ‰ [í‚¤ì›Œë“œ]' (ì˜ˆ: ë…¼ë¬¸ê²€ìƒ‰ machine learning)\n"
            "3. **ë‚ ì”¨ê²€ìƒ‰** â˜€ï¸: '[ë„ì‹œëª…] ë‚ ì”¨' ë˜ëŠ” 'ë‚´ì¼ [ë„ì‹œëª…] ë‚ ì”¨' (ì˜ˆ: ì„œìš¸ ë‚ ì”¨, ë‚´ì¼ ë¶€ì‚° ë‚ ì”¨)\n\n"
            "ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ ì–¸ì œë“  ì§ˆë¬¸í•´ì£¼ì„¸ìš”! ğŸ˜Š"
        )
    
    # ì±„íŒ… ê¸°ë¡ í‘œì‹œ
    for msg in st.session_state.chat_history:
        with st.chat_message(msg['role']):
            st.markdown(msg['content'], unsafe_allow_html=True)
    
    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    if user_prompt := st.chat_input("ì§ˆë¬¸í•´ ì£¼ì„¸ìš”!"):
        st.chat_message("user").markdown(user_prompt)
        st.session_state.chat_history.append({"role": "user", "content": user_prompt})
        with st.chat_message("assistant"):
            placeholder = st.empty()
            placeholder.markdown("â³ ì‘ë‹µ ìƒì„± ì¤‘...")
            start_time = time.time()
            with ThreadPoolExecutor() as executor:
                future = executor.submit(get_cached_response, user_prompt)
                response = future.result()
            time_taken = round(time.time() - start_time, 2)
            placeholder.markdown(response, unsafe_allow_html=True)
            st.session_state.chat_history.append({"role": "assistant", "content": response})
            async_save_chat_history(st.session_state.user_id, st.session_state.session_id, user_prompt, response, time_taken)

# ë©”ì¸ ì‹¤í–‰
def main():
    if not st.session_state.is_logged_in:
        show_login_page()
    else:
        show_chat_dashboard()

if __name__ == "__main__":
    main()
    
# import streamlit as st
# import time
# import uuid
# from supabase import create_client
# import os
# from datetime import datetime, timedelta
# import pytz
# import logging
# import requests
# from bs4 import BeautifulSoup
# import pandas as pd
# from googlesearch import search
# from g4f.client import Client
# from timezonefinder import TimezoneFinder
# import re
# import json
# import urllib.request
# import urllib.parse
# from langdetect import detect
# from requests.adapters import HTTPAdapter
# from requests.packages.urllib3.util.retry import Retry
# from concurrent.futures import ThreadPoolExecutor
# import arxiv
# from diskcache import Cache
# from functools import lru_cache

# # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# SUPABASE_URL = os.getenv("SUPABASE_URL")
# SUPABASE_KEY = os.getenv("SUPABASE_KEY")
# WEATHER_API_KEY = os.getenv("WEATHER_API_KEY")
# DRUG_API_KEY = os.getenv("DRUG_API_KEY")
# NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
# NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")

# # ë¡œê¹… ì„¤ì •
# logging.basicConfig(level=logging.WARNING if os.getenv("ENV") == "production" else logging.INFO)
# logger = logging.getLogger("HybridChat")

# # ìºì‹œ ì„¤ì • (ë””ìŠ¤í¬ ìºì‹œ)
# cache = Cache("cache_directory")

# class MemoryCache:
#     def __init__(self):
#         self.cache = {}
#         self.expiry = {}
    
#     def get(self, key):
#         if key in self.cache and time.time() < self.expiry[key]:
#             return self.cache[key]
#         return cache.get(key)
    
#     def setex(self, key, ttl, value):
#         self.cache[key] = value
#         self.expiry[key] = time.time() + ttl
#         cache.set(key, value, expire=ttl)

# cache_handler = MemoryCache()

# # WeatherAPI í´ë˜ìŠ¤ (ë³‘ë ¬ ì²˜ë¦¬ ë° ìºì‹±)
# class WeatherAPI:
#     def __init__(self, cache_ttl=600):
#         self.cache = cache_handler
#         self.cache_ttl = cache_ttl

#     def fetch_weather(self, url, params):
#         session = requests.Session()
#         retry_strategy = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
#         adapter = HTTPAdapter(max_retries=retry_strategy)
#         session.mount("https://", adapter)
#         response = session.get(url, params=params, timeout=5)
#         response.raise_for_status()
#         return response.json()

#     @lru_cache(maxsize=100)
#     def get_city_info(self, city_name):
#         cache_key = f"city_info:{city_name}"
#         cached = self.cache.get(cache_key)
#         if cached:
#             return cached
#         url = "http://api.openweathermap.org/geo/1.0/direct"
#         params = {'q': city_name, 'limit': 1, 'appid': WEATHER_API_KEY}
#         data = self.fetch_weather(url, params)
#         if data and len(data) > 0:
#             city_info = {"name": data[0]["name"], "lat": data[0]["lat"], "lon": data[0]["lon"]}
#             self.cache.setex(cache_key, 86400, city_info)  # 24ì‹œê°„ ìºì‹±
#             return city_info
#         return None

#     def get_city_weather(self, city_name):
#         cache_key = f"weather:{city_name}"
#         cached_data = self.cache.get(cache_key)
#         if cached_data:
#             return cached_data
        
#         city_info = self.get_city_info(city_name)
#         if not city_info:
#             return f"'{city_name}'ì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
#         url = "https://api.openweathermap.org/data/2.5/weather"
#         params = {'lat': city_info["lat"], 'lon': city_info["lon"], 'appid': WEATHER_API_KEY, 'units': 'metric', 'lang': 'kr'}
#         data = self.fetch_weather(url, params)
#         weather_emojis = {'Clear': 'â˜€ï¸', 'Clouds': 'â˜ï¸', 'Rain': 'ğŸŒ§ï¸', 'Snow': 'â„ï¸', 'Thunderstorm': 'â›ˆï¸', 'Drizzle': 'ğŸŒ¦ï¸', 'Mist': 'ğŸŒ«ï¸'}
#         weather_emoji = weather_emojis.get(data['weather'][0]['main'], 'ğŸŒ¤ï¸')
#         result = (
#             f"í˜„ì¬ {data['name']}, {data['sys']['country']} ë‚ ì”¨ {weather_emoji}\n"
#             f"ë‚ ì”¨: {data['weather'][0]['description']}\n"
#             f"ì˜¨ë„: {data['main']['temp']}Â°C\n"
#             f"ì²´ê°: {data['main']['feels_like']}Â°C\n"
#             f"ìŠµë„: {data['main']['humidity']}%\n"
#             f"í’ì†: {data['wind']['speed']}m/s\n"
#             f"ë” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"
#         )
#         self.cache.setex(cache_key, self.cache_ttl, result)
#         return result

#     def get_forecast_by_day(self, city_name, days_from_today=1):
#         cache_key = f"forecast:{city_name}:{days_from_today}"
#         cached_data = self.cache.get(cache_key)
#         if cached_data:
#             return cached_data
        
#         city_info = self.get_city_info(city_name)
#         if not city_info:
#             return f"'{city_name}'ì˜ ë‚ ì”¨ ì˜ˆë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
#         url = "https://api.openweathermap.org/data/2.5/forecast"
#         params = {'lat': city_info["lat"], 'lon': city_info["lon"], 'appid': WEATHER_API_KEY, 'units': 'metric', 'lang': 'kr'}
#         data = self.fetch_weather(url, params)
#         target_date = (datetime.now() + timedelta(days=days_from_today)).strftime('%Y-%m-%d')
#         forecast_text = f"{city_info['name']}ì˜ {target_date} ë‚ ì”¨ ì˜ˆë³´ ğŸŒ¤ï¸\n"
#         weather_emojis = {'Clear': 'â˜€ï¸', 'Clouds': 'â˜ï¸', 'Rain': 'ğŸŒ§ï¸', 'Snow': 'â„ï¸', 'Thunderstorm': 'â›ˆï¸', 'Drizzle': 'ğŸŒ¦ï¸', 'Mist': 'ğŸŒ«ï¸'}
        
#         found = False
#         for forecast in data['list']:
#             dt = datetime.fromtimestamp(forecast['dt']).strftime('%Y-%m-%d')
#             if dt == target_date:
#                 found = True
#                 time_only = datetime.fromtimestamp(forecast['dt']).strftime('%H:%M')
#                 weather_emoji = weather_emojis.get(forecast['weather'][0]['main'], 'ğŸŒ¤ï¸')
#                 forecast_text += (
#                     f"â° {time_only} {forecast['weather'][0]['description']} {weather_emoji} "
#                     f"{forecast['main']['temp']}Â°C ğŸ’§{forecast['main']['humidity']}% ğŸŒ¬ï¸{forecast['wind']['speed']}m/s\n"
#                 )
        
#         result = forecast_text + "\në” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š" if found else f"'{city_name}'ì˜ {target_date} ë‚ ì”¨ ì˜ˆë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
#         self.cache.setex(cache_key, self.cache_ttl, result)
#         return result

#     def get_weekly_forecast(self, city_name):
#         cache_key = f"weekly_forecast:{city_name}"
#         cached_data = self.cache.get(cache_key)
#         if cached_data:
#             return cached_data
        
#         city_info = self.get_city_info(city_name)
#         if not city_info:
#             return f"'{city_name}'ì˜ ì£¼ê°„ ì˜ˆë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
#         url = "https://api.openweathermap.org/data/2.5/forecast"
#         params = {'lat': city_info["lat"], 'lon': city_info["lon"], 'appid': WEATHER_API_KEY, 'units': 'metric', 'lang': 'kr'}
#         data = self.fetch_weather(url, params)
#         today = datetime.now().date()
#         week_end = today + timedelta(days=6)
#         daily_forecast = {}
#         weekdays_kr = ["ì›”ìš”ì¼", "í™”ìš”ì¼", "ìˆ˜ìš”ì¼", "ëª©ìš”ì¼", "ê¸ˆìš”ì¼", "í† ìš”ì¼", "ì¼ìš”ì¼"]
#         today_weekday = today.weekday()
        
#         for forecast in data['list']:
#             dt = datetime.fromtimestamp(forecast['dt']).date()
#             if today <= dt <= week_end:
#                 dt_str = dt.strftime('%Y-%m-%d')
#                 if dt_str not in daily_forecast:
#                     weekday_idx = (today_weekday + (dt - today).days) % 7
#                     daily_forecast[dt_str] = {
#                         'weekday': weekdays_kr[weekday_idx],
#                         'temp_min': forecast['main']['temp_min'],
#                         'temp_max': forecast['main']['temp_max'],
#                         'weather': forecast['weather'][0]['description']
#                     }
#                 else:
#                     daily_forecast[dt_str]['temp_min'] = min(daily_forecast[dt_str]['temp_min'], forecast['main']['temp_min'])
#                     daily_forecast[dt_str]['temp_max'] = max(daily_forecast[dt_str]['temp_max'], forecast['main']['temp_max'])
        
#         today_str = today.strftime('%Y-%m-%d')
#         today_weekday_str = weekdays_kr[today_weekday]
#         forecast_text = f"{today_str}({today_weekday_str}) ê¸°ì¤€ {city_info['name']}ì˜ ì£¼ê°„ ë‚ ì”¨ ì˜ˆë³´ ğŸŒ¤ï¸\n"
#         weather_emojis = {'Clear': 'â˜€ï¸', 'Clouds': 'â˜ï¸', 'Rain': 'ğŸŒ§ï¸', 'Snow': 'â„ï¸', 'Thunderstorm': 'â›ˆï¸', 'Drizzle': 'ğŸŒ¦ï¸', 'Mist': 'ğŸŒ«ï¸'}
        
#         for date, info in daily_forecast.items():
#             weather_emoji = weather_emojis.get(info['weather'].split()[0], 'ğŸŒ¤ï¸')
#             forecast_text += (
#                 f"{info['weekday']}: {info['weather']} {weather_emoji} "
#                 f"ìµœì € {info['temp_min']}Â°C ìµœê³  {info['temp_max']}Â°C\n"
#             )
        
#         result = forecast_text + "\në” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"
#         self.cache.setex(cache_key, self.cache_ttl, result)
#         return result

# # ì´ˆê¸°í™”
# supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
# client = Client()
# weather_api = WeatherAPI()
# naver_request_count = 0
# NAVER_DAILY_LIMIT = 25000
# st.set_page_config(page_title="AI ì±—ë´‡", page_icon="ğŸ¤–")

# # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# if "is_logged_in" not in st.session_state:
#     st.session_state.is_logged_in = False
#     st.session_state.user_id = None
#     st.session_state.chat_history = []
#     st.session_state.session_id = str(uuid.uuid4())

# # ë„ì‹œ ì¶”ì¶œ
# CITY_PATTERNS = [
#     re.compile(r'(?:ì˜¤ëŠ˜|ë‚´ì¼|ëª¨ë ˆ|ì´ë²ˆ ì£¼|ì£¼ê°„)?\s*([ê°€-í£a-zA-Z\s]{2,20}(?:ì‹œ|êµ°|city)?)ì˜?\s*ë‚ ì”¨', re.IGNORECASE),
#     re.compile(r'(?:ì˜¤ëŠ˜|ë‚´ì¼|ëª¨ë ˆ|ì´ë²ˆ ì£¼|ì£¼ê°„)?\s*([ê°€-í£a-zA-Z\s]{2,20}(?:ì‹œ|êµ°|city)?)\s*ë‚ ì”¨', re.IGNORECASE),
#     re.compile(r'ë‚ ì”¨\s*(?:ì˜¤ëŠ˜|ë‚´ì¼|ëª¨ë ˆ|ì´ë²ˆ ì£¼|ì£¼ê°„)?\s*([ê°€-í£a-zA-Z\s]{2,20}(?:ì‹œ|êµ°|city)?)', re.IGNORECASE),
# ]
# def extract_city_from_query(query):
#     for pattern in CITY_PATTERNS:
#         match = pattern.search(query)
#         if match:
#             city = match.group(1).strip()
#             if city not in ["ì˜¤ëŠ˜", "ë‚´ì¼", "ëª¨ë ˆ", "ì´ë²ˆ ì£¼", "ì£¼ê°„", "í˜„ì¬"]:
#                 return city
#     return "ì„œìš¸"

# TIME_CITY_PATTERNS = [
#     re.compile(r'([ê°€-í£a-zA-Z]{2,20}(?:ì‹œ|êµ°)?)ì˜?\s*ì‹œê°„'),
#     re.compile(r'([ê°€-í£a-zA-Z]{2,20}(?:ì‹œ|êµ°)?)\s*ì‹œê°„'),
#     re.compile(r'ì‹œê°„\s*([ê°€-í£a-zA-Z\s]{2,20}(?:ì‹œ|êµ°)?)'),
# ]
# def extract_city_from_time_query(query):
#     for pattern in TIME_CITY_PATTERNS:
#         match = pattern.search(query)
#         if match:
#             city = match.group(1).strip()
#             if city != "í˜„ì¬":
#                 return city
#     return "ì„œìš¸"

# # ì‹œê°„ ì •ë³´
# def get_time_by_city(city_name="ì„œìš¸"):
#     city_info = weather_api.get_city_info(city_name)
#     if not city_info:
#         return f"'{city_name}'ì˜ ì‹œê°„ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
#     tf = TimezoneFinder()
#     timezone_str = tf.timezone_at(lng=city_info["lon"], lat=city_info["lat"]) or "Asia/Seoul"
#     timezone = pytz.timezone(timezone_str)
#     city_time = datetime.now(timezone)
#     am_pm = "ì˜¤ì „" if city_time.strftime("%p") == "AM" else "ì˜¤í›„"
#     return f"í˜„ì¬ {city_name} ì‹œê°„: {city_time.strftime('%Yë…„ %mì›” %dì¼ %p %I:%M')} â°\në” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"

# # ì‚¬ìš©ì ë° ì±„íŒ… ê¸°ë¡ ê´€ë¦¬
# def create_or_get_user(nickname):
#     user = supabase.table("users").select("*").eq("nickname", nickname).execute()
#     if user.data:
#         return user.data[0]["id"], True
#     new_user = supabase.table("users").insert({"nickname": nickname, "created_at": datetime.now().isoformat()}).execute()
#     return new_user.data[0]["id"], False

# def save_chat_history(user_id, session_id, question, answer, time_taken):
#     supabase.table("chat_history").insert({
#         "user_id": user_id, "session_id": session_id, "question": question,
#         "answer": answer, "time_taken": time_taken, "created_at": datetime.now().isoformat()
#     }).execute()

# def async_save_chat_history(user_id, session_id, question, answer, time_taken):
#     threading.Thread(target=save_chat_history, args=(user_id, session_id, question, answer, time_taken)).start()

# # ì˜ì•½í’ˆ ê²€ìƒ‰
# def get_drug_info(drug_name):
#     cache_key = f"drug:{drug_name}"
#     cached = cache_handler.get(cache_key)
#     if cached:
#         return cached
#     url = 'http://apis.data.go.kr/1471000/DrbEasyDrugInfoService/getDrbEasyDrugList'
#     params = {'serviceKey': DRUG_API_KEY, 'pageNo': '1', 'numOfRows': '1', 'itemName': urllib.parse.quote(drug_name), 'type': 'json'}
#     try:
#         response = requests.get(url, params=params, timeout=5)
#         response.raise_for_status()
#         data = response.json()
#         if 'body' in data and 'items' in data['body'] and data['body']['items']:
#             item = data['body']['items'][0]
#             def cut_to_sentence(text, max_len=150):
#                 if not text or len(text) <= max_len:
#                     return text
#                 truncated = text[:max_len]
#                 last_punctuation = max(truncated.rfind('.'), truncated.rfind('!'), truncated.rfind('?'), truncated.rfind(','))
#                 if last_punctuation > 0:
#                     result = truncated[:last_punctuation + 1]
#                     if len(text) > max_len:
#                         result += " ë“±"
#                     return result
#                 return truncated + "..."
#             efcy = cut_to_sentence(item.get('efcyQesitm', 'ì •ë³´ ì—†ìŒ'))
#             use_method = cut_to_sentence(item.get('useMethodQesitm', 'ì •ë³´ ì—†ìŒ'))
#             atpn = cut_to_sentence(item.get('atpnQesitm', 'ì •ë³´ ì—†ìŒ'))
#             result = (
#                 f"ğŸ’Š **ì˜ì•½í’ˆ ì •ë³´** ğŸ’Š\n"
#                 f"âœ… **ì•½í’ˆëª…**: {item.get('itemName', 'ì •ë³´ ì—†ìŒ')}\n"
#                 f"âœ… **ì œì¡°ì‚¬**: {item.get('entpName', 'ì •ë³´ ì—†ìŒ')}\n"
#                 f"âœ… **íš¨ëŠ¥**: {efcy}\n"
#                 f"âœ… **ìš©ë²•ìš©ëŸ‰**: {use_method}\n"
#                 f"âœ… **ì£¼ì˜ì‚¬í•­**: {atpn}\n"
#                 f"â„¹ï¸ ìì„¸í•œ ì •ë³´ëŠ” <a href='https://www.health.kr/searchDrug/search_detail.asp'>ì•½í•™ì •ë³´ì›</a>ì—ì„œ í™•ì¸í•˜ì„¸ìš”! ğŸ©º\n"
#                 f"ë” ê¶ê¸ˆí•œ ì  ìˆìœ¼ì‹ ê°€ìš”? ğŸ˜Š"
#             )
#             cache_handler.setex(cache_key, 86400, result)  # 24ì‹œê°„ ìºì‹±
#             return result
#         else:
#             return search_and_summarize_drug(drug_name)
#     except Exception as e:
#         return search_and_summarize_drug(drug_name)

# def search_and_summarize_drug(drug_name):
#     search_results = search_and_summarize(f"{drug_name} ì˜ì•½í’ˆ ì •ë³´", num_results=5)
#     if not search_results.empty:
#         return f"'{drug_name}' ê³µì‹ ì •ë³´ ì—†ìŒ. ì›¹ ê²€ìƒ‰ ìš”ì•½:\n{get_ai_summary(search_results)}"
#     return f"'{drug_name}' ì˜ì•½í’ˆ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

# # Naver API ë° ì›¹ ê²€ìƒ‰
# def get_naver_api_results(query):
#     global naver_request_count
#     if naver_request_count >= NAVER_DAILY_LIMIT:
#         return search_and_summarize(query, num_results=5)
#     enc_text = urllib.parse.quote(query)
#     url = f"https://openapi.naver.com/v1/search/webkr?query={enc_text}&display=10&sort=date"
#     request = urllib.request.Request(url)
#     request.add_header("X-Naver-Client-Id", NAVER_CLIENT_ID)
#     request.add_header("X-Naver-Client-Secret", NAVER_CLIENT_SECRET)
#     results = []
#     try:
#         response = urllib.request.urlopen(request)
#         naver_request_count += 1
#         if response.getcode() == 200:
#             data = json.loads(response.read().decode('utf-8'))
#             for item in data.get('items', [])[:5]:
#                 title = re.sub(r'<b>|</b>', '', item['title'])
#                 contents = re.sub(r'<b>|</b>', '', item.get('description', 'ë‚´ìš© ì—†ìŒ'))[:100] + "..."
#                 results.append({"title": title, "contents": contents, "url": item.get('link', ''), "date": item.get('pubDate', '')})
#     except Exception:
#         return search_and_summarize(query, num_results=5)
#     return pd.DataFrame(results)

# def search_and_summarize(query, num_results=5):
#     data = []
#     with ThreadPoolExecutor() as executor:  # ë³‘ë ¬ ì²˜ë¦¬
#         futures = [executor.submit(requests.get, link, timeout=5) for link in search(query, num_results=num_results)]
#         for future in futures:
#             try:
#                 response = future.result()
#                 soup = BeautifulSoup(response.text, 'html.parser')
#                 title = soup.title.get_text() if soup.title else "No title"
#                 description = ' '.join([p.get_text() for p in soup.find_all('p')[:3]])
#                 data.append({"title": title, "contents": description[:500], "link": response.url})
#             except Exception:
#                 continue
#     return pd.DataFrame(data)

# def get_ai_summary(search_results):
#     if search_results.empty:
#         return "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
#     context = "\n".join([f"ì¶œì²˜: {row['title']}\në‚´ìš©: {row['contents']}" for _, row in search_results.iterrows()])
#     response = client.chat.completions.create(
#         model="gpt-4o",
#         messages=[{"role": "user", "content": f"ê²€ìƒ‰ ê²°ê³¼ë¥¼ 2~3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½:\n{context}"}]
#     )
#     summary = response.choices[0].message.content
#     sources = "\nğŸ“œ **ì¶œì²˜**\n" + "\n".join([f"ğŸŒ [{row['title']}]({row['link']})" for _, row in search_results.iterrows()])
#     return f"{summary}{sources}\në” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"

# # ë…¼ë¬¸ ê²€ìƒ‰
# # ë…¼ë¬¸ ê²€ìƒ‰
# def fetch_arxiv_paper(paper):
#     return {
#         "title": paper.title,
#         "authors": ", ".join(str(a) for a in paper.authors),
#         "summary": paper.summary[:200],
#         "entry_id": paper.entry_id,
#         "pdf_url": paper.get_pdf_url(),  # PDF ë§í¬ ì¶”ê°€
#         "published": paper.published.strftime('%Y-%m-%d')
#     }

# def get_arxiv_papers(query, max_results=3):
#     cache_key = f"arxiv:{query}:{max_results}"
#     cached = cache_handler.get(cache_key)
#     if cached:
#         return cached
#     search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.SubmittedDate)
#     with ThreadPoolExecutor() as executor:
#         results = list(executor.map(fetch_arxiv_paper, search.results()))
#     if not results:
#         return "í•´ë‹¹ í‚¤ì›Œë“œë¡œ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
#     response = "ğŸ“š **Arxiv ë…¼ë¬¸ ê²€ìƒ‰ ê²°ê³¼** ğŸ“š\n" + "\n".join(
#         [f"**ë…¼ë¬¸ {i}**\n"
#          f"ğŸ“„ **ì œëª©**: {r['title']}\n"
#          f"ğŸ‘¥ **ì €ì**: {r['authors']}\n"
#          f"ğŸ“ **ì´ˆë¡**: {r['summary']}...\n"
#          f"ğŸ”— **ë…¼ë¬¸ í˜ì´ì§€**: {r['entry_id']}\n"
#          f"ğŸ“¥ **PDF ë‹¤ìš´ë¡œë“œ**: [{r['pdf_url'].split('/')[-1]}]({r['pdf_url']})\n"  # PDF ë§í¬ ì¶”ê°€
#          f"ğŸ“… **ì¶œíŒì¼**: {r['published']}\n"
#          f"{'-' * 50}"
#          for i, r in enumerate(results, 1)]
#     ) + "\në” ë§ì€ ë…¼ë¬¸ì„ ë³´ê³  ì‹¶ë‹¤ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”! ğŸ˜Š"
#     cache_handler.setex(cache_key, 3600, response)
#     return response

# # ëŒ€í™”í˜• ì‘ë‹µ
# conversation_cache = MemoryCache()
# def get_conversational_response(query, chat_history):
#     cache_key = f"conv:{needs_search(query)}:{query}:{hash(str(chat_history[-5:]))}"
#     cached = conversation_cache.get(cache_key)
#     if cached:
#         return cached
#     messages = [{"role": "system", "content": "ì¹œì ˆí•˜ê³  ìƒí˜¸ì‘ìš©ì ì¸ AI ì±—ë´‡ì…ë‹ˆë‹¤."}] + [
#         {"role": msg["role"], "content": msg["content"]} for msg in chat_history[-5:]
#     ] + [{"role": "user", "content": query}]
#     response = client.chat.completions.create(model="gpt-4", messages=messages)
#     result = response.choices[0].message.content
#     conversation_cache.setex(cache_key, 600, result)
#     return result

# GREETING_RESPONSES = {
#     "ì•ˆë…•": "ì•ˆë…•í•˜ì„¸ìš”! ë°˜ê°‘ìŠµë‹ˆë‹¤! ğŸ˜Š",
#     "ì•ˆë…• ë°˜ê°€ì›Œ": "ì•ˆë…•í•˜ì„¸ìš”! ì €ë„ ë°˜ê°‘ìŠµë‹ˆë‹¤! ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì–´ë– ì‹ ê°€ìš”? ğŸ˜„",
#     "í•˜ì´": "í•˜ì´! ì¢‹ì€ í•˜ë£¨ ë³´ë‚´ì„¸ìš”! ğŸ˜Š",
#     "í—¬ë¡œ": "ì•ˆë…•í•˜ì„¸ìš”! ë°˜ê°‘ìŠµë‹ˆë‹¤! ğŸ˜Š",
#     "í—¤ì´": "í—¤ì´! ì˜ ì§€ë‚´ì„¸ìš”? ğŸ˜„",
#     "ì™“ì—…": "ì™“ì—…! ë­í•˜ê³  ê³„ì‹ ê°€ìš”? ğŸ˜Š",
#     "ì™“ì¹": "ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì–´ë– ì‹ ê°€ìš”? ğŸ˜„",
# }

# # ì¿¼ë¦¬ ë¶„ë¥˜
# def needs_search(query):
#     query_lower = query.strip().lower()
#     greeting_keywords = ["ì•ˆë…•", "í•˜ì´", "ë°˜ê°€ì›Œ", "ì•ˆë‡½", "ë­í•´", "í—¬ë¡œ", "í—¬ë¡±", "í•˜ì‡", "í—¤ì´", "í—¤ì´ìš”", "ì™“ì—…", "ì™“ì¹", "ì—ì´ìš”"]
#     emotion_keywords = ["ë°°ê³ í”„ë‹¤", "ë°°ê³ í”„", "ì¡¸ë¦¬ë‹¤", "í”¼ê³¤í•˜ë‹¤", "í™”ë‚¨", "ì—´ë°›ìŒ", "ì§œì¦ë‚¨", "í”¼ê³¤í•¨"]
#     if any(greeting in query_lower for greeting in greeting_keywords) or \
#        any(emo in query_lower for emo in emotion_keywords) or \
#        len(query_lower) <= 3 and "?" not in query_lower:
#         return "conversation"
#     intent_keywords = ["ì¶”ì²œí•´ì¤˜", "ë­ ë¨¹ì„ê¹Œ", "ë©”ë‰´", "ë­í• ê¹Œ"]
#     if any(kw in query_lower for kw in intent_keywords):
#         return "conversation"
#     time_keywords = ["í˜„ì¬ ì‹œê°„", "ì‹œê°„", "ëª‡ ì‹œ", "ì§€ê¸ˆ", "ëª‡ì‹œ", "ëª‡ ì‹œì•¼", "ì§€ê¸ˆ ì‹œê°„", "í˜„ì¬", "ì‹œê³„"]
#     if any(keyword in query_lower for keyword in time_keywords) and \
#        any(timeword in query_lower for timeword in ["ì‹œê°„", "ëª‡ì‹œ", "ëª‡ ì‹œ", "ì‹œê³„"]):
#         return "time"
#     weather_keywords = ["ë‚ ì”¨", "ì˜¨ë„", "ê¸°ì˜¨"]
#     if any(keyword in query_lower for keyword in weather_keywords) and "ë‚´ì¼" in query_lower:
#         return "tomorrow_weather"
#     elif any(keyword in query_lower for keyword in weather_keywords) and "ëª¨ë ˆ" in query_lower:
#         return "day_after_tomorrow_weather"
#     elif any(keyword in query_lower for keyword in weather_keywords) and ("ì£¼ê°„" in query_lower or any(kw in query_lower for kw in ["ì´ë²ˆ ì£¼", "ì£¼ê°„ ì˜ˆë³´", "ì£¼ê°„ ë‚ ì”¨"])):
#         return "weekly_forecast"
#     elif any(keyword in query_lower for keyword in weather_keywords):
#         return "weather"
#     drug_keywords = ["ì•½", "ì˜ì•½í’ˆ", "ì•½í’ˆ"]
#     drug_pattern = r'^[ê°€-í£a-zA-Z]{2,10}(?:ì•½|ì •|ì‹œëŸ½|ìº¡ìŠ)$'
#     if any(keyword in query_lower for keyword in drug_keywords) and re.match(drug_pattern, query_lower):
#         return "drug"
#     if query_lower == "mbti ê²€ì‚¬":
#         return "mbti"
#     if query_lower == "ë‹¤ì¤‘ì§€ëŠ¥ ê²€ì‚¬":
#         return "multi_iq"
#     arxiv_keywords = ["ë…¼ë¬¸ê²€ìƒ‰", "arxiv", "paper", "research"]
#     if any(kw in query_lower for kw in arxiv_keywords) and len(query_lower) > 5:
#         return "arxiv_search"
#     search_keywords = ["ê²€ìƒ‰", "ì•Œë ¤ì¤˜", "ì •ë³´", "ë­ì•¼", "ë¬´ì—‡ì´ì•¼", "ë¬´ì—‡ì¸ì§€", "ì°¾ì•„ì„œ", "ì •ë¦¬í•´ì¤˜", "ì„¤ëª…í•´ì¤˜", "ì•Œê³ ì‹¶ì–´", "ì•Œë ¤ì¤„ë˜"]
#     if any(kw in query_lower for kw in search_keywords) and len(query_lower) > 5:
#         return "web_search"
#     return "general_query"

# # UI í•¨ìˆ˜
# def show_login_page():
#     st.title("ë¡œê·¸ì¸ ğŸ¤—")
#     with st.form("login_form"):
#         nickname = st.text_input("ë‹‰ë„¤ì„", placeholder="ì˜ˆ: í›„ì•ˆ")
#         if st.form_submit_button("ì‹œì‘í•˜ê¸° ğŸš€") and nickname:
#             user_id, is_existing = create_or_get_user(nickname)
#             st.session_state.user_id = user_id
#             st.session_state.is_logged_in = True
#             st.toast(f"í™˜ì˜í•©ë‹ˆë‹¤, {nickname}ë‹˜! ğŸ‰")
#             time.sleep(1)
#             st.rerun()

# @st.cache_data(ttl=600)
# def get_cached_response(query):
#     return process_query(query)

# def process_query(query):
#     query_type = needs_search(query)
#     if query_type == "mbti":
#         return (
#             "MBTI ê²€ì‚¬ë¥¼ ì›í•˜ì‹œë‚˜ìš”? âœ¨ ì•„ë˜ ì‚¬ì´íŠ¸ì—ì„œ ë¬´ë£Œë¡œ ì„±ê²© ìœ í˜• ê²€ì‚¬ë¥¼ í•  ìˆ˜ ìˆì–´ìš”! ğŸ˜Š\n"
#             "[16Personalities MBTI ê²€ì‚¬](https://www.16personalities.com/ko/%EB%AC%B4%EB%A3%8C-%EC%84%B1%EA%B2%A9-%EC%9C%A0%ED%98%95-%EA%B2%80%EC%82%AC) ğŸŒŸ\n"
#             "ì´ ì‚¬ì´íŠ¸ëŠ” 16ê°€ì§€ ì„±ê²© ìœ í˜•ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ í…ŒìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ë©°, ê²°ê³¼ì— ë”°ë¼ ì„±ê²© ì„¤ëª…ê³¼ ì¸ê°„ê´€ê³„ ì¡°ì–¸ ë“±ì„ í™•ì¸í•  ìˆ˜ ìˆì–´ìš”! ğŸ§ ğŸ’¡"
#         )
#     elif query_type == "multi_iq":
#         return (
#             "ë‹¤ì¤‘ì§€ëŠ¥ ê²€ì‚¬ë¥¼ ì›í•˜ì‹œë‚˜ìš”? ğŸ‰ ì•„ë˜ ì‚¬ì´íŠ¸ì—ì„œ ë¬´ë£Œë¡œ ë‹¤ì¤‘ì§€ëŠ¥ í…ŒìŠ¤íŠ¸ë¥¼ í•´ë³¼ ìˆ˜ ìˆì–´ìš”! ğŸ˜„\n"
#             "[Multi IQ Test](https://multiiqtest.com/) ğŸš€\n"
#             "ì´ ì‚¬ì´íŠ¸ëŠ” í•˜ì›Œë“œ ê°€ë“œë„ˆì˜ ë‹¤ì¤‘ì§€ëŠ¥ ì´ë¡ ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ í…ŒìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ë©°, ë‹¤ì–‘í•œ ì§€ëŠ¥ ì˜ì—­ì„ í‰ê°€í•´ì¤ë‹ˆë‹¤! ğŸ“šâœ¨"
#         )
#     elif query_type == "time":
#         city = extract_city_from_time_query(query)
#         return get_time_by_city(city)
#     elif query_type == "weather":
#         city = extract_city_from_query(query)
#         return weather_api.get_city_weather(city)
#     elif query_type == "tomorrow_weather":
#         city = extract_city_from_query(query)
#         return weather_api.get_forecast_by_day(city, days_from_today=1)
#     elif query_type == "day_after_tomorrow_weather":
#         city = extract_city_from_query(query)
#         return weather_api.get_forecast_by_day(city, days_from_today=2)
#     elif query_type == "weekly_forecast":
#         city = extract_city_from_query(query)
#         return weather_api.get_weekly_forecast(city)
#     elif query_type == "drug":
#         return get_drug_info(query.strip())
#     elif query_type == "conversation":
#         if query.strip() in GREETING_RESPONSES:
#             return GREETING_RESPONSES[query.strip()]
#         return get_conversational_response(query, st.session_state.chat_history)
#     elif query_type == "web_search":
#         language = detect(query)
#         if language == 'ko' and naver_request_count < NAVER_DAILY_LIMIT:
#             return get_ai_summary(get_naver_api_results(query))
#         return get_ai_summary(search_and_summarize(query))
#     elif query_type == "arxiv_search":
#         keywords = query.replace("ë…¼ë¬¸ê²€ìƒ‰", "").replace("arxiv", "").replace("paper", "").replace("research", "").strip()
#         return get_arxiv_papers(keywords)
#     elif query_type == "general_query":
#         return get_conversational_response(query, st.session_state.chat_history)
#     return "ì•„ì§ ì§€ì›í•˜ì§€ ì•ŠëŠ” ê¸°ëŠ¥ì´ì—ìš”. ğŸ˜…"

# def show_chat_dashboard():
#     st.title("AI ì±—ë´‡ ğŸ¤–")
#     for msg in st.session_state.chat_history:
#         with st.chat_message(msg['role']):
#             st.markdown(msg['content'], unsafe_allow_html=True)
    
#     if user_prompt := st.chat_input("ì§ˆë¬¸í•´ ì£¼ì„¸ìš”!"):
#         st.chat_message("user").markdown(user_prompt)
#         st.session_state.chat_history.append({"role": "user", "content": user_prompt})
#         with st.chat_message("assistant"):
#             placeholder = st.empty()
#             placeholder.markdown("â³ ì‘ë‹µ ìƒì„± ì¤‘...")
#             start_time = time.time()
#             with ThreadPoolExecutor() as executor:
#                 future = executor.submit(get_cached_response, user_prompt)
#                 response = future.result()
#             time_taken = round(time.time() - start_time, 2)
#             placeholder.markdown(response, unsafe_allow_html=True)
#             st.session_state.chat_history.append({"role": "assistant", "content": response})
#             async_save_chat_history(st.session_state.user_id, st.session_state.session_id, user_prompt, response, time_taken)

# # ë©”ì¸ ì‹¤í–‰
# def main():
#     if not st.session_state.is_logged_in:
#         show_login_page()
#     else:
#         show_chat_dashboard()

# if __name__ == "__main__":
#     main()
