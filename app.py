# set lib
from config.imports import *
from config.env import *

# Import utils modules
from utils.webpage_analyzer import (
    fetch_webpage_content, 
    summarize_webpage_content, 
    extract_urls_from_text,
    is_url_summarization_request,
    is_numbered_link_request,
    is_followup_question
)
from utils.providers import (
    select_best_provider_with_priority,
    select_random_available_provider,
    get_client
)
from utils.query_analyzer import (
    needs_search,
    extract_city_from_query,
    extract_city_from_time_query,
    extract_league_from_query,
    is_drug_inquiry,
    extract_drug_name,
    is_paper_search,
    extract_keywords_for_paper_search,
    is_time_query,
    LEAGUE_MAPPING
)
# Import weather, football, and drug modules
from utils.weather import WeatherAPI
from utils.football import FootballAPI
from utils.drug_info import DrugAPI

# set logger
logging.basicConfig(level=logging.WARNING if os.getenv("ENV") == "production" else logging.INFO)
logger = logging.getLogger("HybridChat")
logging.getLogger("streamlit").setLevel(logging.WARNING)
logging.getLogger("httpx").setLevel(logging.WARNING)

# set cach
cache = Cache("cache_directory")

class MemoryCache:
    def __init__(self):
        self.cache = {}
        self.expiry = {}
    
    def get(self, key):
        if key in self.cache and time.time() < self.expiry[key]:
            return self.cache[key]
        return cache.get(key)
    
    def setex(self, key, ttl, value):
        self.cache[key] = value
        self.expiry[key] = time.time() + ttl
        cache.set(key, value, expire=ttl)

cache_handler = MemoryCache()

# ë‚ ì§œ ì¼ê´„ì  ìˆ˜ì • 
def format_date(fordate):
    if fordate == 'No date':
        return 'ë‚ ì§œ ì—†ìŒ'
    try:
        date_obj = datetime.strptime(fordate, '%Y %b %d')
        return date_obj.strftime('%Y.%m.%d')
    except ValueError:
        return fordate

# JSON íŒŒì¼ì—ì„œ MBTI ë° ë‹¤ì¤‘ì§€ëŠ¥ ë°ì´í„° ë¡œë“œ (ìºì‹± ì ìš©)
def load_personality_data():
    cache_key = "personality_data"
    cached_data = cache_handler.get(cache_key)
    if cached_data:
        return cached_data
    
    try:
        with open("config/personality_multi_data.json", "r", encoding="utf-8") as f:
            data = json.load(f)
        cache_handler.setex(cache_key, 86400, data)  # 24ì‹œê°„ ìºì‹±
        return data
    except FileNotFoundError:
        logger.error("personality_multi_data.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        raise
    except json.JSONDecodeError:
        logger.error("personality_multi_data.json íŒŒì¼ì˜ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤.")
        raise

# ë°ì´í„° ë¡œë“œ
personality_data = load_personality_data()
mbti_descriptions = personality_data["mbti_descriptions"]
multi_iq_descriptions = personality_data["multi_iq_descriptions"]
mbti_full_description = personality_data["mbti_full_description"]
multi_iq_full_description = personality_data["multi_iq_full_description"]

# ì´ˆê¸°í™” - API í´ë˜ìŠ¤ë“¤ì„ utilsì—ì„œ importí•˜ì—¬ ì‚¬ìš©
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
weather_api = WeatherAPI(cache_handler=cache_handler, WEATHER_API_KEY=WEATHER_API_KEY)
football_api = FootballAPI(api_key=SPORTS_API_KEY, cache_handler=cache_handler)
drug_api = DrugAPI(api_key=DRUG_API_KEY, cache_handler=cache_handler)  # ìƒˆë¡œ ì¶”ê°€
naver_request_count = 0
NAVER_DAILY_LIMIT = 25000
st.set_page_config(page_title="AI ì±—ë´‡", page_icon="ğŸ¤–")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ë¶€ë¶„ì— ê²€ìƒ‰ ê²°ê³¼ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
def init_session_state():
    if "is_logged_in" not in st.session_state:
        st.session_state.is_logged_in = False
    if "user_id" not in st.session_state:
        st.session_state.user_id = None
    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?ğŸ˜Š"}]
    if "session_id" not in st.session_state:
        st.session_state.session_id = str(uuid.uuid4())
    if "client" not in st.session_state or "provider_name" not in st.session_state:
        client, provider_name = select_random_available_provider()
        st.session_state.client = client
        st.session_state.provider_name = provider_name
    # ê²€ìƒ‰ ê²°ê³¼ ì»¨í…ìŠ¤íŠ¸ ì €ì¥ì„ ìœ„í•œ ë³€ìˆ˜ ì¶”ê°€
    if "search_contexts" not in st.session_state:
        st.session_state.search_contexts = {}
    if "current_context" not in st.session_state:
        st.session_state.current_context = None

# ë¬¸í™” í–‰ì‚¬ ê´€ë ¨ í•¨ìˆ˜ 
def fetch_xml(api_key: str) -> ET.Element:
    """API í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ XML ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    url = f"http://openapi.seoul.go.kr:8088/{api_key}/xml/culturalEventInfo/1/100/"
    try:
        response = requests.get(url)
        response.raise_for_status()
        root = ET.fromstring(response.content)
        return root
    except requests.exceptions.RequestException as e:
        logger.error(f"ë¬¸í™”í–‰ì‚¬ API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return None

def select_target_district(root: ET.Element, target_district: str = ""):
    """
    target_districtê°€ ë¹ˆ ë¬¸ìì—´ì´ë©´ XMLì—ì„œ ëª¨ë“  êµ¬ë¥¼ ì¶”ì¶œí•˜ì—¬
    ëœë¤ìœ¼ë¡œ 5ê°œ ì„ íƒí•œ í›„ ì„ íƒëœ êµ¬ ëª©ë¡(ë¦¬ìŠ¤íŠ¸)ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    target_districtì— ê°’ì´ ìˆë‹¤ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if target_district:
        return target_district
    districts = {row.findtext('GUNAME', default='ì •ë³´ ì—†ìŒ') for row in root.findall('.//row')}
    districts = [d for d in districts if d != "ì •ë³´ ì—†ìŒ"]
    if districts:
        selected_districts = random.sample(districts, min(5, len(districts)))
        return selected_districts
    else:
        return None

def extract_event_date(date_str: str):
    """
    ë‚ ì§œ ë¬¸ìì—´ì—ì„œ ì‹œì‘ì¼ë§Œ ì¶”ì¶œí•˜ì—¬ datetime.date ê°ì²´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    íŒŒì‹±ì— ì‹¤íŒ¨í•˜ë©´ Noneì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    date_match = re.match(r"(\d{4}[-./]\d{2}[-./]\d{2})", date_str)
    if not date_match:
        return None
    date_part = date_match.group(1).replace('.', '-').replace('/', '-')
    try:
        event_date = datetime.strptime(date_part, "%Y-%m-%d").date()
        return event_date
    except Exception:
        return None

def get_future_events(api_key: str, target_district: str = ""):
    """API í‚¤ì™€ target_district(ë¹ˆ ë¬¸ìì—´ì´ë©´ ëœë¤ ì„ íƒ)ë¥¼ ë°›ì•„ ë¯¸ë˜ í–‰ì‚¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    root = fetch_xml(api_key)
    today = datetime.today().date()
    selected_district = select_target_district(root, target_district)
    if not selected_district:
        return "êµ¬ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    events = []
    for row in root.findall('.//row'):
        district = row.findtext('GUNAME', default='ì •ë³´ ì—†ìŒ')
        date_str = row.findtext('DATE', default='ì •ë³´ ì—†ìŒ')
        event_date = extract_event_date(date_str)
        if not event_date or event_date <= today:
            continue
        # selected_districtê°€ ë¦¬ìŠ¤íŠ¸ì´ë©´ í¬í•¨ ì—¬ë¶€, ë¬¸ìì—´ì´ë©´ ë™ì¼ ì—¬ë¶€ë¥¼ ë¹„êµ
        if isinstance(selected_district, list):
            if district not in selected_district:
                continue
        else:
            if district != selected_district:
                continue
        title = row.findtext('TITLE', default='ì •ë³´ ì—†ìŒ')
        place = row.findtext('PLACE', default='ì •ë³´ ì—†ìŒ')
        fee = row.findtext('USE_FEE', default='ì •ë³´ ì—†ìŒ')
        is_free = row.findtext('IS_FREE', default='ì •ë³´ ì—†ìŒ')
        link = row.findtext('HMPG_ADDR', default='ì •ë³´ ì—†ìŒ')
        image = row.findtext('MAIN_IMG', default='ì •ë³´ ì—†ìŒ')
        events.append({
            "title": title,
            "date": date_str,
            "place": place,
            "district": district,
            "fee": fee,
            "is_free": is_free,
            "link": link,
            "image": image
        })
    return events[:10]  # ìµœëŒ€ 10ê°œì˜ ì´ë²¤íŠ¸ë§Œ ë°˜í™˜
# ì‹œê°„ ê´€ë ¨ í•¨ìˆ˜
def get_kst_time():
    kst_timezone = pytz.timezone("Asia/Seoul")
    kst_time = datetime.now(kst_timezone)
    return f"ëŒ€í•œë¯¼êµ­ ê¸°ì¤€ : {kst_time.strftime('%Yë…„ %mì›” %dì¼ %p %I:%M')}ì…ë‹ˆë‹¤. â°\n\n ë” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"

def get_time_by_city(city_name="ì„œìš¸"):
    city_info = weather_api.get_city_info(city_name)
    if not city_info:
        return f"'{city_name}'ì˜ ì‹œê°„ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    tf = TimezoneFinder()
    timezone_str = tf.timezone_at(lng=city_info["lon"], lat=city_info["lat"]) or "Asia/Seoul"
    timezone = pytz.timezone(timezone_str)
    city_time = datetime.now(timezone)
    return f"í˜„ì¬ {city_name} ì‹œê°„: {city_time.strftime('%Yë…„ %mì›” %dì¼ %p %I:%M')}ì…ë‹ˆë‹¤. â°\n\n ë” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"

# ì‚¬ìš©ì ë° ì±„íŒ… ê¸°ë¡ ê´€ë¦¬
def create_or_get_user(nickname):
    user = supabase.table("users").select("*").eq("nickname", nickname).execute()
    if user.data:
        return user.data[0]["id"], True
    new_user = supabase.table("users").insert({"nickname": nickname, "created_at": datetime.now().isoformat()}).execute()
    return new_user.data[0]["id"], False

def save_chat_history(user_id, session_id, question, answer, time_taken):
    if isinstance(answer, dict) and "table" in answer and isinstance(answer["table"], pd.DataFrame):
        answer_to_save = {
            "header": answer["header"],
            "table": answer["table"].to_dict(orient="records"),
            "footer": answer["footer"]
        }
    else:
        answer_to_save = answer
    
    supabase.table("chat_history").insert({
        "user_id": user_id,
        "session_id": session_id,
        "question": question,
        "answer": answer_to_save,
        "time_taken": time_taken,
        "created_at": datetime.now().isoformat()
    }).execute()

def async_save_chat_history(user_id, session_id, question, answer, time_taken):
    threading.Thread(target=save_chat_history, args=(user_id, session_id, question, answer, time_taken)).start()

# Naver API ê²€ìƒ‰ (ì›¹ ê²€ìƒ‰)
def get_naver_api_results(query):
    global naver_request_count
    cache_key = f"naver:{query}"
    cached = cache_handler.get(cache_key)
    if cached:
        return cached
    
    if naver_request_count >= NAVER_DAILY_LIMIT:
        return "ê²€ìƒ‰ í•œë„ ì´ˆê³¼ë¡œ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ğŸ˜“"
    enc_text = urllib.parse.quote(query)
    url = f"https://openapi.naver.com/v1/search/webkr?query={enc_text}&display=5&sort=date"
    request = urllib.request.Request(url)
    request.add_header("X-Naver-Client-Id", NAVER_CLIENT_ID)
    request.add_header("X-Naver-Client-Secret", NAVER_CLIENT_SECRET)
    try:
        response = urllib.request.urlopen(request, timeout=3)
        naver_request_count += 1
        if response.getcode() == 200:
            data = json.loads(response.read().decode('utf-8'))
            results = data.get('items', [])
            if not results:
                return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ğŸ˜“"
            
            response_text = "ğŸŒ **ì›¹ ê²€ìƒ‰ ê²°ê³¼** \n\n"
            response_text += "\n\n".join(
                [f"**ê²°ê³¼ {i}**\n\nğŸ“„ **ì œëª©**: {re.sub(r'<b>|</b>', '', item['title'])}\n\nğŸ“ **ë‚´ìš©**: {re.sub(r'<b>|</b>', '', item.get('description', 'ë‚´ìš© ì—†ìŒ'))[:100]}...\n\nğŸ”— **ë§í¬**: {item.get('link', '')}"
                 for i, item in enumerate(results, 1)]
            ) + "\n\në” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"
            cache_handler.setex(cache_key, 3600, response_text)
            return response_text
    except Exception as e:
        logger.error(f"Naver API ì˜¤ë¥˜: {str(e)}")
        return "ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ğŸ˜“"

# ArXiv ë…¼ë¬¸ ê²€ìƒ‰
def fetch_arxiv_paper(paper):
    return {
        "title": paper.title,
        "authors": ", ".join(str(a) for a in paper.authors),
        "summary": paper.summary[:200],
        "entry_id": paper.entry_id,
        "pdf_url": paper.pdf_url,
        "published": paper.published.strftime('%Y-%m-%d')
    }

def get_arxiv_papers(query, max_results=3):
    cache_key = f"arxiv:{query}:{max_results}"
    cached = cache_handler.get(cache_key)
    if cached:
        return cached
    search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.SubmittedDate)
    with ThreadPoolExecutor() as executor:
        results = list(executor.map(fetch_arxiv_paper, search.results()))
    if not results:
        return "í•´ë‹¹ í‚¤ì›Œë“œë¡œ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    response = "ğŸ“š **Arxiv ë…¼ë¬¸ ê²€ìƒ‰ ê²°ê³¼** ğŸ“š\n\n"
    response += "\n\n".join(
        [f"**ë…¼ë¬¸ {i}**\n\nğŸ“„ **ì œëª©**: {r['title']}\n\nğŸ‘¥ **ì €ì**: {r['authors']}\n\nğŸ“ **ì´ˆë¡**: {r['summary']}...\n\nğŸ”— **ë…¼ë¬¸ í˜ì´ì§€**: {r['entry_id']}\n\nğŸ“… **ì¶œíŒì¼**: {r['published']}"
         for i, r in enumerate(results, 1)]
    ) + "\n\në” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"
    cache_handler.setex(cache_key, 3600, response)
    return response

# PubMed ë…¼ë¬¸ ê²€ìƒ‰
base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/"

def search_pubmed(query, max_results=5):
    search_url = f"{base_url}esearch.fcgi"
    params = {"db": "pubmed", "term": query, "retmode": "json", "retmax": max_results, "api_key": NCBI_KEY}
    response = requests.get(search_url, params=params, timeout=3)
    return response.json()

def get_pubmed_summaries(id_list):
    summary_url = f"{base_url}esummary.fcgi"
    params = {"db": "pubmed", "id": ",".join(id_list), "retmode": "json", "api_key": NCBI_KEY}
    response = requests.get(summary_url, params=params, timeout=3)
    return response.json()

def get_pubmed_abstract(id_list):
    fetch_url = f"{base_url}efetch.fcgi"
    params = {"db": "pubmed", "id": ",".join(id_list), "retmode": "xml", "rettype": "abstract", "api_key": NCBI_KEY}
    response = requests.get(fetch_url, params=params, timeout=3)
    return response.text

def extract_first_two_sentences(abstract_text):
    if not abstract_text or abstract_text.isspace():
        return "No abstract available"
    sentences = [s.strip() for s in abstract_text.split('.') if s.strip()]
    return " ".join(sentences[:2]) + "." if sentences else "No abstract available"

def parse_abstracts(xml_text):
    abstract_dict = {}
    try:
        root = ET.fromstring(xml_text)
        for article in root.findall(".//PubmedArticle"):
            pmid = article.find(".//MedlineCitation/PMID").text
            abstract_elem = article.find(".//Abstract/AbstractText")
            abstract = abstract_elem.text if abstract_elem is not None else "No abstract available"
            abstract_dict[pmid] = extract_first_two_sentences(abstract)
    except ET.ParseError:
        return {}
    return abstract_dict

def get_pubmed_papers(query, max_results=5):
    cache_key = f"pubmed:{query}:{max_results}"
    cached = cache_handler.get(cache_key)
    if cached:
        return cached
    
    search_results = search_pubmed(query, max_results)
    pubmed_ids = search_results["esearchresult"]["idlist"]
    if not pubmed_ids:
        return "í•´ë‹¹ í‚¤ì›Œë“œë¡œ ì˜í•™ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    summaries = get_pubmed_summaries(pubmed_ids)
    abstracts_xml = get_pubmed_abstract(pubmed_ids)
    abstract_dict = parse_abstracts(abstracts_xml)
    response = "ğŸ©º **PubMed ë…¼ë¬¸ ê²€ìƒ‰ ê²°ê³¼** ğŸ©º\n\n"
    response += "\n\n".join(
        [f"**ë…¼ë¬¸ {i}**\n\n"
         f"ğŸ†” **PMID**: {pmid}\n\n"
         f"ğŸ“– **ì œëª©**: {summaries['result'][pmid].get('title', 'No title')}\n\n"
         f"ğŸ“… **ì¶œíŒì¼**: {format_date(summaries['result'][pmid].get('pubdate', 'No date'))}\n\n"
         f"âœï¸ **ì €ì**: {', '.join([author.get('name', '') for author in summaries['result'][pmid].get('authors', [])])}\n\n"
         f"ğŸ“ **ì´ˆë¡**: {abstract_dict.get(pmid, 'No abstract')}\n\n"
         f"ğŸ”— **ë…¼ë¬¸ í˜ì´ì§€**: https://pubmed.ncbi.nlm.nih.gov/{pmid}/"
         for i, pmid in enumerate(pubmed_ids, 1)]
    ) + "\n\në” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"
    cache_handler.setex(cache_key, 3600, response)
    return response
    
# ëŒ€í™”í˜• ì‘ë‹µ (ë¹„ë™ê¸°)
conversation_cache = MemoryCache()
_client_instance = None

# ëŒ€í™”í˜• ì‘ë‹µ í•¨ìˆ˜ ìˆ˜ì •
async def get_conversational_response(query, chat_history):
    cache_key = f"conv:{needs_search(query)}:{query}"
    cached = conversation_cache.get(cache_key)
    if cached:
        return cached
    
    # í˜„ì¬ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
    current_context = None
    if hasattr(st, 'session_state') and 'current_context' in st.session_state:
        current_context_id = st.session_state.current_context
        if current_context_id and current_context_id in st.session_state.search_contexts:
            current_context = st.session_state.search_contexts[current_context_id]
    
    # ìˆœì„œ ê¸°ë°˜ ë§í¬ ìš”ì²­ í™•ì¸ (ì˜ˆ: 3ë²ˆì§¸ ë§í¬ ìš”ì•½í•´ì¤˜)
    is_numbered_request, numbered_url = is_numbered_link_request(query, current_context)
    if is_numbered_request:
        summary = summarize_webpage_content(numbered_url, query)
        conversation_cache.setex(cache_key, 600, summary)
        return summary
    
    # ì¼ë°˜ URL ìš”ì•½ ìš”ì²­ í™•ì¸
    is_url_request, url = is_url_summarization_request(query)
    if is_url_request:
        # URL ìš”ì•½ ì²˜ë¦¬
        summary = summarize_webpage_content(url, query)
        conversation_cache.setex(cache_key, 600, summary)
        return summary
    
    messages = [
        {"role": "system", "content": "ì¹œì ˆí•œ AI ì±—ë´‡ì…ë‹ˆë‹¤. ì ì ˆí•œ ì´ëª¨ì§€ ì‚¬ìš©: âœ…(ì™„ë£Œ), â“(ì§ˆë¬¸), ğŸ˜Š(ì¹œì ˆ)"}
    ]
    
    # í˜„ì¬ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ê°€ ìˆëŠ”ì§€ í™•ì¸
    current_context = None
    if hasattr(st, 'session_state') and 'current_context' in st.session_state:
        current_context_id = st.session_state.current_context
        if current_context_id and current_context_id in st.session_state.search_contexts:
            current_context = st.session_state.search_contexts[current_context_id]
    
    # ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€
    if current_context:
        context_type = current_context["type"]
        context_query = current_context["query"]
        context_result = current_context["result"]
        
        # ì»¨í…ìŠ¤íŠ¸ ìœ í˜•ì— ë”°ë¼ ë‹¤ë¥¸ ì§€ì‹œ ì¶”ê°€
        if context_type == "naver_search":
            # í…Œì´ë¸” ë°ì´í„°ì¸ ê²½ìš° ì²˜ë¦¬
            if isinstance(context_result, dict) and "table" in context_result:
                table_json = context_result["table"].to_json(orient="records")
                context_desc = f"ì‚¬ìš©ìê°€ '{context_query}'ì— ëŒ€í•´ ê²€ìƒ‰í–ˆê³ , ë‹¤ìŒ í…Œì´ë¸” í˜•íƒœì˜ ê²°ê³¼ë¥¼ ë°›ì•˜ìŠµë‹ˆë‹¤: {table_json}"
            else:
                # ì •ê·œ í‘œí˜„ì‹ìœ¼ë¡œ ì›¹ ê²€ìƒ‰ ê²°ê³¼ë§Œ ì¶”ì¶œ
                cleaned_results = re.findall(r"\*\*ê²°ê³¼ \d+\*\*\s*\n\nğŸ“„ \*\*ì œëª©\*\*: (.*?)\n\nğŸ“ \*\*ë‚´ìš©\*\*: (.*?)(?=\n\nğŸ”—|\n\në” ê¶ê¸ˆí•œ)", context_result, re.DOTALL)
                context_desc = f"ì‚¬ìš©ìê°€ '{context_query}'ì— ëŒ€í•´ ì›¹ ê²€ìƒ‰ì„ í–ˆê³ , ë‹¤ìŒ ê²°ê³¼ë¥¼ ë°›ì•˜ìŠµë‹ˆë‹¤:\n\n"
                for i, (title, content) in enumerate(cleaned_results, 1):
                    context_desc += f"{i}. ì œëª©: {title}\n   ë‚´ìš©: {content}\n\n"
                
                # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ URLì„ ì¶”ì¶œí•˜ì—¬ ì›¹í˜ì´ì§€ ìš”ì•½ ì œì•ˆ
                urls_in_context = extract_urls_from_text(context_result)
                if urls_in_context:
                    context_desc += f"\n\nê²€ìƒ‰ ê²°ê³¼ì— ì´ {len(urls_in_context)}ê°œì˜ ë§í¬ê°€ ìˆìŠµë‹ˆë‹¤.\n"
                    context_desc += "íŠ¹ì • ë§í¬ì˜ ì „ì²´ ë‚´ìš©ì´ ê¶ê¸ˆí•˜ì‹œë©´ ë‹¤ìŒê³¼ ê°™ì´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”:\n"
                    context_desc += "- 'ì²« ë²ˆì§¸ ë§í¬ ìš”ì•½í•´ì¤˜' ë˜ëŠ” '3ë²ˆì§¸ ë§í¬ ìš”ì•½í•´ì¤˜'\n"
                    context_desc += "- 'URL + ìš”ì•½í•´ì¤˜' í˜•íƒœë¡œ ì§ì ‘ URL ì§€ì •"
        
        # ë‹¤ë¥¸ ìœ í˜•ì˜ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ (ì•½í’ˆ ì •ë³´, ë…¼ë¬¸ ë“±)
        elif context_type == "drug":
            # ì•½í’ˆ ì •ë³´ì¼ ê²½ìš°
            context_desc = f"ì‚¬ìš©ìê°€ '{context_query}' ì•½í’ˆì— ëŒ€í•œ ì •ë³´ë¥¼ ê²€ìƒ‰í–ˆìŠµë‹ˆë‹¤. ì•½í’ˆ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”."
        
        # ê³µí†µ ì§€ì‹œì‚¬í•­
        system_prompt = (
            "ì¹œì ˆí•œ AI ì±—ë´‡ì…ë‹ˆë‹¤. ì ì ˆí•œ ì´ëª¨ì§€ ì‚¬ìš©: âœ…(ì™„ë£Œ), â“(ì§ˆë¬¸), ğŸ˜Š(ì¹œì ˆ).\n\n"
            f"{context_desc}\n\n"
            "ì‚¬ìš©ìì˜ í›„ì† ì§ˆë¬¸ì€ ì´ ê²€ìƒ‰ ê²°ê³¼ì— ê´€í•œ ê²ƒì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê²€ìƒ‰ ê²°ê³¼ì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.\n"
            "ìš”ì•½ì„ ìš”ì²­ë°›ìœ¼ë©´ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ê³ , ì„¤ëª…ì„ ìš”ì²­ë°›ìœ¼ë©´ ë” ìì„¸í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.\n"
            "ê²€ìƒ‰ ê²°ê³¼ì— ê´€ë ¨ ì •ë³´ê°€ ì—†ë‹¤ë©´ ì •ì§í•˜ê²Œ ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•˜ì„¸ìš”.\n"
            "ì‚¬ìš©ìê°€ 'ì²« ë²ˆì§¸ ë§í¬', '3ë²ˆì§¸ ë§í¬' ë“± ìˆœì„œë¡œ ë§í¬ë¥¼ ì–¸ê¸‰í•˜ë©´ í•´ë‹¹ ìˆœì„œì˜ ì›¹í˜ì´ì§€ ì „ì²´ ë‚´ìš©ì„ ìš”ì•½í•´ë“œë¦°ë‹¤ê³  ì•ˆë‚´í•˜ì„¸ìš”.\n"
            "URLì´ë‚˜ ë§í¬ì— ëŒ€í•œ ì§ˆë¬¸ì„ ë°›ìœ¼ë©´, í•´ë‹¹ ë§í¬ì˜ ì „ì²´ ë‚´ìš©ì„ í™•ì¸í•˜ê³  ì‹¶ë‹¤ë©´ 'ìˆœì„œ + ë§í¬ ìš”ì•½í•´ì¤˜' ë˜ëŠ” 'URL + ìš”ì•½í•´ì¤˜' í˜•íƒœë¡œ ì§ˆë¬¸í•˜ë¼ê³  ì•ˆë‚´í•´ì£¼ì„¸ìš”."
        )
        messages[0]["content"] = system_prompt
    
    # ìµœê·¼ ëŒ€í™” ê¸°ë¡ ì¶”ê°€
    messages.extend([{"role": msg["role"], "content": msg["content"]} 
                    for msg in chat_history[-4:] if "ë” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”?" not in msg["content"]])
    
    # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
    messages.append({"role": "user", "content": query})
    
    # ë¹„ë™ê¸° ì‹¤í–‰ ì „ì— client ê°ì²´ë¥¼ ë¯¸ë¦¬ ê°€ì ¸ì˜´
    try:
        if not hasattr(st, 'session_state') or 'client' not in st.session_state:
            client, _ = select_random_available_provider()
        else:
            client = st.session_state.client
            
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            None, lambda: client.chat.completions.create(
                model="gpt-4o-mini", messages=messages
            )
        )
        result = response.choices[0].message.content if response.choices else "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    except Exception as e:
        logger.error(f"ëŒ€í™” ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        result = "ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    conversation_cache.setex(cache_key, 600, result)
    return result

GREETINGS = ["ì•ˆë…•", "í•˜ì´", "í—¬ë¡œ", "ã…ã…‡", "ì™“ì—…", "í• ë¡±", "í—¤ì´"]
GREETING_RESPONSE = "ì•ˆë…•í•˜ì„¸ìš”! ë°˜ê°‘ìŠµë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ğŸ˜Š"

def process_query(query):
    cache_key = f"query:{hash(query)}"
    cached = cache_handler.get(cache_key)
    if cached is not None:
        return cached
    
    query_type = needs_search(query)
    query_lower = query.strip().lower().replace(" ", "")
    
    with ThreadPoolExecutor() as executor:
        if query_type == "weather":
            future = executor.submit(weather_api.get_city_weather, extract_city_from_query(query))
            result = future.result()
        elif query_type == "tomorrow_weather":
            future = executor.submit(weather_api.get_forecast_by_day, extract_city_from_query(query), 1)
            result = future.result()
        elif query_type == "time":
            if "ì˜¤ëŠ˜ë‚ ì§œ" in query_lower or "í˜„ì¬ë‚ ì§œ" in query_lower or "ê¸ˆì¼ë‚ ì§œ" in query_lower:
                result = get_kst_time()
            else:
                city = extract_city_from_time_query(query)
                future = executor.submit(get_time_by_city, city)
                result = future.result()
        elif query_type == "league_standings":
            league_key = extract_league_from_query(query)
            if league_key:
                league_info = LEAGUE_MAPPING[league_key]
                future = executor.submit(football_api.fetch_league_standings, league_info["code"], league_info["name"])
                result = future.result()
                result = result["error"] if "error" in result else {
                    "header": f"{result['league_name']} ë¦¬ê·¸ ìˆœìœ„",
                    "table": result["data"],
                    "footer": "ë” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"
                }
            else:
                result = "ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¦¬ê·¸ì…ë‹ˆë‹¤. ğŸ˜“ ì§€ì› ë¦¬ê·¸: EPL, LaLiga, Bundesliga, Serie A, Ligue 1"
        elif query_type == "league_scorers":
            league_key = extract_league_from_query(query)
            if league_key:
                league_info = LEAGUE_MAPPING[league_key]
                future = executor.submit(football_api.fetch_league_scorers, league_info["code"], league_info["name"])
                try:
                    result = future.result()
                    result = result["error"] if "error" in result else {
                        "header": f"{result['league_name']} ë¦¬ê·¸ ë“ì ìˆœìœ„ (ìƒìœ„ 10ëª…)",
                        "table": result["data"],
                        "footer": "ë” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"
                    }
                except Exception as e:
                    result = f"ë¦¬ê·¸ ë“ì ìˆœìœ„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)} ğŸ˜“"
            else:
                result = "ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¦¬ê·¸ì…ë‹ˆë‹¤. ğŸ˜“ ì§€ì› ë¦¬ê·¸: EPL, LaLiga, Bundesliga, Serie A, Ligue 1"
        elif query_type == "cl_knockout":
            try:
                future = executor.submit(football_api.fetch_championsleague_knockout_matches)
                results = future.result()
                if isinstance(results, str):  # ì—ëŸ¬ ë©”ì‹œì§€ì¸ ê²½ìš°
                    result = results
                elif not results:
                    result = "ì±”í”¼ì–¸ìŠ¤ë¦¬ê·¸ í† ë„ˆë¨¼íŠ¸ ê²½ê¸° ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
                else:
                    df = pd.DataFrame(results)
                    result = {
                        "header": "ì±”í”¼ì–¸ìŠ¤ë¦¬ê·¸ Knockout Stage ê²°ê³¼",
                        "table": df,
                        "footer": "ë” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"
                    }
            except Exception as e:
                result = f"ì±”í”¼ì–¸ìŠ¤ë¦¬ê·¸ í† ë„ˆë¨¼íŠ¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)} ğŸ˜“"
        elif query_type == "cultural_event":
            # ë¬¸í™”í–‰ì‚¬ ì²˜ë¦¬ ë¡œì§
            target_district = query.replace("ë¬¸í™”í–‰ì‚¬", "").strip()  # "ë¬¸í™”í–‰ì‚¬" í‚¤ì›Œë“œ ì œê±°
            future = executor.submit(get_future_events, CULTURE_API_KEY, target_district)
            events = future.result()
            if isinstance(events, str):  # ì˜¤ë¥˜ ë©”ì‹œì§€ ë°˜í™˜
                return events
            elif not events:  # ê²°ê³¼ê°€ ì—†ì„ ê²½ìš°
                result = "í•´ë‹¹ ì¡°ê±´ì— ë§ëŠ” ë¬¸í™” í–‰ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤."
            else:
                result = "ğŸ­ **ë¬¸í™” í–‰ì‚¬ ì •ë³´** ğŸ­\n\n"
                for i, event in enumerate(events, 1):
                    # ì´ë¯¸ì§€ URLê³¼ ë§í¬ë¥¼ í´ë¦­ ê°€ëŠ¥í•œ ë§í¬ë¡œ ë³€ê²½
                    image_link = f"[ğŸ–¼ï¸ ì´ë¯¸ì§€ ë³´ê¸°]({event['image']})" if event['image'] != 'ì •ë³´ ì—†ìŒ' else "ğŸ–¼ï¸ ì´ë¯¸ì§€ ì—†ìŒ"
                    web_link = f"[ğŸ”— ì›¹ì‚¬ì´íŠ¸]({event['link']})" if event['link'] != 'ì •ë³´ ì—†ìŒ' else "ğŸ”— ë§í¬ ì—†ìŒ"
                    
                    result += (
                        f"### {i}. {event['title']}\n\n"
                        f"ğŸ“… **ë‚ ì§œ**: {event['date']}\n\n"
                        f"ğŸ“ **ì¥ì†Œ**: {event['place']} ({event['district']})\n\n"
                        f"ğŸ’° **ìš”ê¸ˆ**: {event['fee']} ({event['is_free']})\n\n"
                        f"{web_link} | {image_link}\n\n"
                        f"---\n\n"
                    )
                result += "ë” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"
                
        elif query_type == "drug":
            future = executor.submit(drug_api.get_drug_info, query)  # ìˆ˜ì •ëœ ë¶€ë¶„
            result = future.result()
        elif query_type == "arxiv_search":
            keywords = query.replace("ê³µí•™ë…¼ë¬¸", "").replace("arxiv", "").strip()
            future = executor.submit(get_arxiv_papers, keywords)
            result = future.result()
        elif query_type == "pubmed_search":
            keywords = query.replace("ì˜í•™ë…¼ë¬¸", "").strip()
            future = executor.submit(get_pubmed_papers, keywords)
            result = future.result()
        elif query_type == "naver_search":
            search_query = query.lower().replace("ê²€ìƒ‰", "").strip()
            future = executor.submit(get_naver_api_results, search_query)
            result = future.result()
            
            # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ì €ì¥
            context_id = str(uuid.uuid4())
            st.session_state.search_contexts[context_id] = {
                "type": "naver_search",
                "query": search_query,
                "result": result,
                "timestamp": datetime.now().isoformat()
            }
            st.session_state.current_context = context_id
            
            # ë©€í‹°í„´ ëŒ€í™”ë¥¼ ìœ„í•œ ì•ˆë‚´ ì¶”ê°€
            result += "\n\nğŸ’¡ ê²€ìƒ‰ ê²°ê³¼ì— ëŒ€í•´ ë” ì§ˆë¬¸í•˜ì‹œë©´ ë‹µë³€í•´ë“œë¦´ê²Œìš”. ì˜ˆë¥¼ ë“¤ì–´:\n"
            result += "- 'ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš”ì•½í•´'\n"
            result += "- 'ì²« ë²ˆì§¸ ê²°ê³¼ì— ëŒ€í•´ ìì„¸íˆ ì„¤ëª…í•´ì¤˜'\n"
            result += "- '3ë²ˆì§¸ ë§í¬ ìš”ì•½í•´ì¤˜' (í•´ë‹¹ ìˆœì„œ ì›¹í˜ì´ì§€ ì „ì²´ ë‚´ìš© ìš”ì•½)\n"
            result += "- 'URL ìš”ì•½í•´ì¤˜' (íŠ¹ì • ë§í¬ì˜ ì „ì²´ ë‚´ìš© í™•ì¸)"
        elif query_type == "mbti":
            result = (
                "MBTI ê²€ì‚¬ë¥¼ ì›í•˜ì‹œë‚˜ìš”? âœ¨ ì•„ë˜ ì‚¬ì´íŠ¸ì—ì„œ ë¬´ë£Œë¡œ ì„±ê²© ìœ í˜• ê²€ì‚¬ë¥¼ í•  ìˆ˜ ìˆì–´ìš”! ğŸ˜Š\n"
                "[16Personalities MBTI ê²€ì‚¬](https://www.16personalities.com/ko/%EB%AC%B4%EB%A3%8C-%EC%84%B1%EA%B2%A9-%EC%9C%A0%ED%98%95-%EA%B2%80%EC%82%AC) ğŸŒŸ\n"
                "ì´ ì‚¬ì´íŠ¸ëŠ” 16ê°€ì§€ ì„±ê²© ìœ í˜•ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ í…ŒìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ë©°, ê²°ê³¼ì— ë”°ë¼ ì„±ê²© ì„¤ëª…ê³¼ ì¸ê°„ê´€ê³„ ì¡°ì–¸ ë“±ì„ í™•ì¸í•  ìˆ˜ ìˆì–´ìš”! ğŸ’¡"
            )
        elif query_type == "mbti_types":
            specific_type = query_lower.replace("mbti", "").replace("ìœ í˜•", "").replace("ì„¤ëª…", "").strip().upper()
            if specific_type in mbti_descriptions:
                result = f"### ğŸ­ {specific_type} í•œ ì¤„ ì„¤ëª…\n- âœ… **{specific_type}** {mbti_descriptions[specific_type]}"
            else:
                result = mbti_full_description
        elif query_type == "multi_iq":
            result = (
                "ë‹¤ì¤‘ì§€ëŠ¥ ê²€ì‚¬ë¥¼ ì›í•˜ì‹œë‚˜ìš”? ğŸ‰ ì•„ë˜ ì‚¬ì´íŠ¸ì—ì„œ ë¬´ë£Œë¡œ ë‹¤ì¤‘ì§€ëŠ¥ í…ŒìŠ¤íŠ¸ë¥¼ í•´ë³¼ ìˆ˜ ìˆì–´ìš”! ğŸ˜„\n"
                "[Multi IQ Test](https://multiiqtest.com/) ğŸš€\n"
                "ì´ ì‚¬ì´íŠ¸ëŠ” í•˜ì›Œë“œ ê°€ë“œë„ˆì˜ ë‹¤ì¤‘ì§€ëŠ¥ ì´ë¡ ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ í…ŒìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ë©°, ë‹¤ì–‘í•œ ì§€ëŠ¥ ì˜ì—­ì„ í‰ê°€í•´ì¤ë‹ˆë‹¤! ğŸ“šâœ¨"
            )
        elif query_type == "multi_iq_types":
            specific_type = query_lower.replace("ë‹¤ì¤‘ì§€ëŠ¥", "").replace("multi_iq", "").replace("ìœ í˜•", "").replace("ì„¤ëª…", "").strip().replace(" ", "")
            if specific_type in multi_iq_descriptions:
                result = f"### ğŸ¨ {specific_type.replace('ì§€ëŠ¥', ' ì§€ëŠ¥')} í•œ ì¤„ ì„¤ëª…\n- ğŸ“– **{specific_type.replace('ì§€ëŠ¥', ' ì§€ëŠ¥')}** {multi_iq_descriptions[specific_type]['description']}"
            else:
                result = multi_iq_full_description
        elif query_type == "multi_iq_jobs":
            specific_type = query_lower.replace("ë‹¤ì¤‘ì§€ëŠ¥", "").replace("multi_iq", "").replace("ì§ì—…", "").replace("ì¶”ì²œ", "").strip().replace(" ", "")
            if specific_type in multi_iq_descriptions:
                result = f"### ğŸ¨ {specific_type.replace('ì§€ëŠ¥', ' ì§€ëŠ¥')} ì¶”ì²œ ì§ì—…\n- ğŸ“– **{specific_type.replace('ì§€ëŠ¥', ' ì§€ëŠ¥')}**: {multi_iq_descriptions[specific_type]['description']}- **ì¶”ì²œ ì§ì—…**: {multi_iq_descriptions[specific_type]['jobs']}"
            else:
                result = multi_iq_full_description
        elif query_type == "multi_iq_full":
            result = multi_iq_full_description
        elif query_type == "conversation":
            if query_lower in GREETINGS:
                result = GREETING_RESPONSE
            else:
                result = asyncio.run(get_conversational_response(query, st.session_state.messages))
        else:
            result = "ì•„ì§ ì§€ì›í•˜ì§€ ì•ŠëŠ” ê¸°ëŠ¥ì´ì—ìš”. ğŸ˜…"
        
        cache_handler.setex(cache_key, 600, result)
        return result

# ê¸°ì¡´ show_chat_dashboard í•¨ìˆ˜ ë‚´ì—ì„œ ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ ë¶€ë¶„ ìˆ˜ì •
def show_chat_dashboard():
    st.title("Chat with AI ğŸ¤–")
    if st.button("ë„ì›€ë§ â„¹ï¸"):
        st.info(
            "ì±—ë´‡ê³¼ ë” ì‰½ê²Œ ëŒ€í™”í•˜ëŠ” ë°©ë²•ì´ì—ìš”! :\n"
            "1. **ë‚ ì”¨** â˜€ï¸: '[ë„ì‹œëª…] ë‚ ì”¨' (ì˜ˆ: ì„œìš¸ ë‚ ì”¨, ë‚´ì¼ ì„œìš¸ ë‚ ì”¨)\n"
            "2. **ì‹œê°„/ë‚ ì§œ** â±ï¸: '[ë„ì‹œëª…] ì‹œê°„' ë˜ëŠ” 'ì˜¤ëŠ˜ ë‚ ì§œ' (ì˜ˆ: ë§ˆë“œë¦¬ë“œ ì‹œê°„, ê¸ˆì¼ ë‚ ì§œ)\n"
            "3. **ê²€ìƒ‰** ğŸŒ: '[í‚¤ì›Œë“œ] ê²€ìƒ‰í•´' ë˜ëŠ” '[í‚¤ì›Œë“œ] ê²€ìƒ‰í•´ì¤˜' (ì˜ˆ: 2025ë…„ ì„œìš¸ ì „ì‹œíšŒ ê²€ìƒ‰í•´ì¤˜)\n"
            "   - ğŸ”— **ê²€ìƒ‰ í›„ ë§í¬ ë¶„ì„**: 'ì²« ë²ˆì§¸ ë§í¬ ìš”ì•½í•´ì¤˜', '3ë²ˆì§¸ ê²°ê³¼ ë¶„ì„í•´ì¤˜'\n"
            "4. **ì›¹í˜ì´ì§€ ì§ì ‘ ë¶„ì„** ğŸ“„: 'URL ìš”ì•½í•´ì¤˜' ë˜ëŠ” 'URL ë¶„ì„í•´ì¤˜'\n"
            "   - ì˜ˆ: 'https://example.com ìš”ì•½í•´ì¤˜', 'https://deepmind.google/models/gemini/flash/ ë¶„ì„í•´ì¤˜'\n"
            "5. **ì•½í’ˆê²€ìƒ‰** ğŸ’Š: 'ì•½í’ˆê²€ìƒ‰ [ì•½ ì´ë¦„]' (ì˜ˆ: ì•½í’ˆê²€ìƒ‰ ê²Œë³´ë¦°)\n"
            "6. **ê³µí•™ë…¼ë¬¸** ğŸ“š: 'ê³µí•™ë…¼ë¬¸ [í‚¤ì›Œë“œ]' (ì˜ˆ: ê³µí•™ë…¼ë¬¸ Multimodal AI)\n"
            "7. **ì˜í•™ë…¼ë¬¸** ğŸ©º: 'ì˜í•™ë…¼ë¬¸ [í‚¤ì›Œë“œ]' (ì˜ˆ: ì˜í•™ë…¼ë¬¸ cancer therapy)\n"
            "8. **ì¶•êµ¬ ë¦¬ê·¸ ì •ë³´** âš½: '[ë¦¬ê·¸ ì´ë¦„] ë¦¬ê·¸ ìˆœìœ„ ë˜ëŠ” ë¦¬ê·¸ë“ì ìˆœìœ„' (ì˜ˆ: EPL ë¦¬ê·¸ìˆœìœ„, EPL ë¦¬ê·¸ë“ì ìˆœìœ„)\n"
            "   - ì§€ì› ë¦¬ê·¸: EPL, LaLiga, Bundesliga, Serie A, Ligue 1, ChampionsLeague\n"
            "   - **ì±”í”¼ì–¸ìŠ¤ë¦¬ê·¸ ë¦¬ê·¸ ë‹¨ê³„**: 'ì±”í”¼ì–¸ìŠ¤ë¦¬ê·¸ ë¦¬ê·¸ ìˆœìœ„' ë˜ëŠ” 'UCL ë¦¬ê·¸ìˆœìœ„'ë¡œ í™•ì¸\n"
            "   - **ì±”í”¼ì–¸ìŠ¤ë¦¬ê·¸ í† ë„ˆë¨¼íŠ¸**: 'ì±”í”¼ì–¸ìŠ¤ë¦¬ê·¸ í† ë„ˆë¨¼íŠ¸' ë˜ëŠ” 'UCL 16ê°•'(ì˜ˆ: ì±”í”¼ì–¸ìŠ¤ë¦¬ê·¸ 16ê°•)\n"
            "9. **MBTI** âœ¨: 'MBTI ê²€ì‚¬',  'MBTI ìœ í˜•', 'MBTI ì„¤ëª…' (ì˜ˆ: MBTI ê²€ì‚¬, INTJ ì„¤ëª…)\n"
            "10. **ë‹¤ì¤‘ì§€ëŠ¥** ğŸ‰: 'ë‹¤ì¤‘ì§€ëŠ¥ ê²€ì‚¬', 'ë‹¤ì¤‘ì§€ëŠ¥ ìœ í˜•', 'ë‹¤ì¤‘ì§€ëŠ¥ ì§ì—…', (ì˜ˆ: ë‹¤ì¤‘ì§€ëŠ¥ ê²€ì‚¬, ì–¸ì–´ì§€ëŠ¥ ì§ì—…)\n"
            "11. **ë¬¸í™”í–‰ì‚¬** ğŸ­: '[ì§€ì—­êµ¬] ë¬¸í™”í–‰ì‚¬' ë˜ëŠ” 'ë¬¸í™”í–‰ì‚¬' (ì˜ˆ: ê°•ë‚¨êµ¬ ë¬¸í™”í–‰ì‚¬, ë¬¸í™”í–‰ì‚¬)\n\n"
            "ğŸŒŸ **ê³ ê¸‰ ê¸°ëŠ¥**:\n"
            "- ê²€ìƒ‰ í›„ í›„ì† ì§ˆë¬¸ìœ¼ë¡œ íŠ¹ì • ë§í¬ì˜ ì „ì²´ ë‚´ìš© ë¶„ì„ ê°€ëŠ¥\n"
            "- ì›¹í˜ì´ì§€ URLì„ ì§ì ‘ ì œê³µí•˜ì—¬ ë‚´ìš© ìš”ì•½/ë¶„ì„ ê°€ëŠ¥\n"
            "- ë©€í‹°í„´ ëŒ€í™”ë¡œ ì´ì „ ê²€ìƒ‰ ê²°ê³¼ì— ëŒ€í•œ ì¶”ê°€ ì§ˆë¬¸ ê°€ëŠ¥\n\n"
            "ê¶ê¸ˆí•œ ì  ìˆìœ¼ë©´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”! ğŸ˜Š"
        )
   
    for msg in st.session_state.messages[-10:]:
        with st.chat_message(msg['role']):
            if isinstance(msg['content'], dict) and "table" in msg['content']:
                st.markdown(f"### {msg['content']['header']}")
                st.dataframe(pd.DataFrame(msg['content']['table']), use_container_width=True, hide_index=True)
                st.markdown(msg['content']['footer'])
            else:
                st.markdown(msg['content'], unsafe_allow_html=True)
    
    if user_prompt := st.chat_input("ì§ˆë¬¸í•´ ì£¼ì„¸ìš”!"):
        st.chat_message("user").markdown(user_prompt)
        st.session_state.messages.append({"role": "user", "content": user_prompt})
        
        with st.chat_message("assistant"):
            placeholder = st.empty()
            placeholder.markdown("ì‘ë‹µì„ ì¤€ë¹„ ì¤‘ì´ì—ìš”.. â³")
            try:
                start_time = time.time()
                
                # í›„ì† ì§ˆë¬¸ì¸ì§€ í™•ì¸
                if is_followup_question(user_prompt) and st.session_state.current_context:
                    # í›„ì† ì§ˆë¬¸ìœ¼ë¡œ íŒë‹¨ë˜ë©´ ê¸°ì¡´ ì»¨í…ìŠ¤íŠ¸ ìœ ì§€í•˜ê³  LLMì— ì „ë‹¬
                    response = asyncio.run(get_conversational_response(user_prompt, st.session_state.messages))
                else:
                    # ìƒˆë¡œìš´ ì§ˆë¬¸ì´ë©´ ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™”í•˜ê³  ì¼ë°˜ ì²˜ë¦¬
                    if needs_search(user_prompt) is None:
                        st.session_state.current_context = None
                    response = process_query(user_prompt)
                
                time_taken = round(time.time() - start_time, 2)
                
                # ë¡œë”© ë©”ì‹œì§€ ì œê±°
                placeholder.empty()
                
                if isinstance(response, dict) and "table" in response:
                    st.markdown(f"### {response['header']}")
                    st.dataframe(response['table'], use_container_width=True, hide_index=True)
                    st.markdown(response['footer'])
                else:
                    st.markdown(response, unsafe_allow_html=True)
                
                st.session_state.messages.append({"role": "assistant", "content": response})
                async_save_chat_history(st.session_state.user_id, st.session_state.session_id, user_prompt, response, time_taken)
            
            except Exception as e:
                placeholder.empty()
                error_msg = f"ì‘ë‹µì„ ì¤€ë¹„í•˜ë‹¤ ë¬¸ì œ: {str(e)} ğŸ˜“"
                logger.error(f"ëŒ€í™” ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)
                st.markdown(error_msg, unsafe_allow_html=True)
                st.session_state.messages.append({"role": "assistant", "content": error_msg})

def show_login_page():
    st.title("ë¡œê·¸ì¸ ğŸ¤—")
    with st.form("login_form"):
        nickname = st.text_input("ë‹‰ë„¤ì„", placeholder="ì˜ˆ: í›„ì•ˆ")
        submit_button = st.form_submit_button("ì‹œì‘í•˜ê¸° ğŸš€")

        if submit_button and nickname:
            try:
                user_id, is_existing = create_or_get_user(nickname)
                st.session_state.user_id = user_id
                st.session_state.is_logged_in = True
                st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ë„ì›€ë§ë„ í™œìš©í•´ ë³´ì„¸ìš” ğŸ˜Š"}]
                st.session_state.session_id = str(uuid.uuid4())
                st.toast(f"í™˜ì˜í•©ë‹ˆë‹¤, {nickname}ë‹˜! ğŸ‰")
                time.sleep(1)
                st.rerun()
            except Exception:
                st.toast("ë¡œê·¸ì¸ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.", icon="âŒ")

# ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„
def main():
    init_session_state()
    
    if not st.session_state.is_logged_in:
        show_login_page()
    else:
        show_chat_dashboard()

if __name__ == "__main__":
    main()