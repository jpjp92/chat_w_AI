# í™˜ê²½ ë³€ìˆ˜ ë° ì„¤ì •
from config.imports import *
from config.env import *

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.WARNING if os.getenv("ENV") == "production" else logging.INFO)
logger = logging.getLogger("HybridChat")
logging.getLogger("streamlit").setLevel(logging.WARNING)
logging.getLogger("httpx").setLevel(logging.WARNING)

# ìºì‹œ ì„¤ì •
cache = Cache("cache_directory")

class MemoryCache:
    def __init__(self):
        self.cache = {}
        self.expiry = {}
    
    def get(self, key):
        if key in self.cache and time.time() < self.expiry[key]:
            return self.cache[key]
        return cache.get(key)
    
    def setex(self, key, ttl, value):
        self.cache[key] = value
        self.expiry[key] = time.time() + ttl
        cache.set(key, value, expire=ttl)

cache_handler = MemoryCache()

# MBTI ë° ë‹¤ì¤‘ì§€ëŠ¥ ë°ì´í„°
mbti_descriptions = {
    "ISTJ": "(í˜„ì‹¤ì£¼ì˜ì) ğŸ›ï¸ğŸ“šğŸ§‘â€âš–ï¸: ì›ì¹™ì„ ì¤‘ì‹œí•˜ë©° ê¼¼ê¼¼í•œ ê³„íšìœ¼ë¡œ ëª©í‘œë¥¼ ë‹¬ì„±!",
    "ISFJ": "(ë”°ëœ»í•œ ìˆ˜í˜¸ì) ğŸ›¡ï¸ğŸ§¸ğŸ’–: íƒ€ì¸ì„ ë°°ë ¤í•˜ë©° í—Œì‹ ì ì¸ ë„ì›€ì„ ì£¼ëŠ” ì„±ê²©!",
    "INFJ": "(ì‹ ë¹„ë¡œìš´ ì¡°ì–¸ì) ğŸŒ¿ğŸ”®ğŸ“–: ê¹Šì€ í†µì°°ë ¥ìœ¼ë¡œ ì‚¬ëŒë“¤ì—ê²Œ ì˜ê°ì„ ì£¼ëŠ” ì´ìƒì£¼ì˜ì!",
    "INTJ": "(ì „ëµê°€) ğŸ§ â™Ÿï¸ğŸ“ˆ: ë¯¸ë˜ë¥¼ ì„¤ê³„í•˜ë©° ëª©í‘œë¥¼ í–¥í•´ ë‚˜ì•„ê°€ëŠ” ë§ˆìŠ¤í„°ë§ˆì¸ë“œ!",
    "ISTP": "(ë§ŒëŠ¥ ì¬ì£¼ê¾¼) ğŸ”§ğŸ•¶ï¸ğŸï¸: ë¬¸ì œë¥¼ ì‹¤ì§ˆì ìœ¼ë¡œ í•´ê²°í•˜ëŠ” ì‹¤ìš©ì ì¸ ëª¨í—˜ê°€!",
    "ISFP": "(ì˜ˆìˆ ê°€) ğŸ¨ğŸµğŸ¦‹: ê°ì„±ì„ í‘œí˜„í•˜ë©° ììœ ë¡œìš´ ì‚¶ì„ ì¶”êµ¬í•˜ëŠ” ì˜ˆìˆ ê°€!",
    "INFP": "(ì´ìƒì£¼ì˜ì) ğŸŒŒğŸ“œğŸ•Šï¸: ë‚´ë©´ì˜ ê°€ì¹˜ë¥¼ ì¤‘ì‹œí•˜ë©° ì„¸ìƒì„ ë” ë‚˜ì€ ê³³ìœ¼ë¡œ ë§Œë“œëŠ” ëª½ìƒê°€!",
    "INTP": "(ë…¼ë¦¬ì ì¸ ì² í•™ì) ğŸ¤”ğŸ“–âš™ï¸: í˜¸ê¸°ì‹¬ ë§ê³  ë…¼ë¦¬ì ìœ¼ë¡œ ì„¸ìƒì„ íƒêµ¬í•˜ëŠ” ì‚¬ìƒ‰ê°€!",
    "ESTP": "(ëª¨í—˜ê°€) ğŸï¸ğŸ”¥ğŸ¤: ìˆœê°„ì„ ì¦ê¸°ë©° ë„ì „ê³¼ ëª¨í—˜ì„ ì‚¬ë‘í•˜ëŠ” í™œë™ê°€!",
    "ESFP": "(ì‚¬êµì ì¸ ì—°ì˜ˆì¸) ğŸ­ğŸ¤ğŸŠ: ì‚¬ëŒë“¤ê³¼ í•¨ê»˜í•˜ë©° ë¶„ìœ„ê¸°ë¥¼ ë„ìš°ëŠ” íŒŒí‹°ì˜ ì¤‘ì‹¬!",
    "ENFP": "(ììœ ë¡œìš´ ì˜í˜¼) ğŸŒˆğŸš€ğŸ’¡: ì°½ì˜ì ì¸ ì•„ì´ë””ì–´ë¡œ ì„¸ìƒì„ ë°íˆëŠ” ì—´ì •ì ì¸ ì˜í˜¼!",
    "ENTP": "(í† ë¡ ê°€) ğŸ—£ï¸âš¡â™Ÿï¸: ìƒˆë¡œìš´ ì•„ì´ë””ì–´ë¥¼ íƒêµ¬í•˜ë©° ë…¼ìŸì„ ì¦ê¸°ëŠ” í˜ì‹ ê°€!",
    "ESTJ": "(ì—„ê²©í•œ ê´€ë¦¬ì) ğŸ—ï¸ğŸ“ŠğŸ› ï¸: ì²´ê³„ì ìœ¼ë¡œ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ëŠ” ë¦¬ë”ì‹­ì˜ ëŒ€ê°€!",
    "ESFJ": "(ì¹œì ˆí•œ ì™¸êµê´€) ğŸ’ğŸ¤—ğŸ¡: ì‚¬ëŒë“¤ì„ ì—°ê²°í•˜ë©° ë”°ëœ»í•œ ê³µë™ì²´ë¥¼ ë§Œë“œëŠ” ì™¸êµê´€!",
    "ENFJ": "(ì—´ì •ì ì¸ ë¦¬ë”) ğŸŒŸğŸ¤ğŸ«¶: íƒ€ì¸ì„ ì´ëŒë©° ê¸ì •ì ì¸ ë³€í™”ë¥¼ ë§Œë“œëŠ” ì¹´ë¦¬ìŠ¤ë§ˆ ë¦¬ë”!",
    "ENTJ": "(ì•¼ë§ê°€) ğŸ‘‘ğŸ“ˆğŸ”¥: ëª©í‘œë¥¼ í–¥í•´ ëŒì§„í•˜ë©° í° ê·¸ë¦¼ì„ ê·¸ë¦¬ëŠ” ì§€íœ˜ê´€!"
}

multi_iq_descriptions = {
    "ì–¸ì–´ì§€ëŠ¥": {
        "description": "ğŸ“ğŸ“šğŸ“¢: ë§ê³¼ ê¸€ì„ í†µí•´ ìƒê°ì„ í‘œí˜„í•˜ëŠ” ë° íƒì›”!\n",
        "jobs": "ì†Œì„¤ê°€, ì‹œì¸, ì‘ê°€, ë…¼ì„¤ / ë™í™” ì‘ê°€, ë°©ì†¡ì‘ê°€, ì˜í™”ëŒ€ë³¸ì‘ê°€, ì›¹íˆ° ì‘ê°€ / ì•„ë‚˜ìš´ì„œ, ë¦¬í¬í„°, ì„±ìš° / êµì‚¬, êµìˆ˜, ê°•ì‚¬, ë…ì„œ ì§€ë„ì‚¬ / ì–¸ì–´ì¹˜ë£Œì‚¬, ì‹¬ë¦¬ì¹˜ë£Œì‚¬, êµ¬ì—°ë™í™”ê°€"
    },
    "ë…¼ë¦¬ìˆ˜í•™ì§€ëŠ¥": {
        "description": "ğŸ§®ğŸ“ŠğŸ§ : ë¶„ì„ì  ì‚¬ê³ ì™€ ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ì´ ë›°ì–´ë‚¨!\n",
        "jobs": "ê³¼í•™ì, ë¬¼ë¦¬í•™ì, ìˆ˜í•™ì / ì˜ë£Œê³µí•™, ì „ìê³µí•™, ì»´í“¨í„° ê³µí•™, í•­ê³µìš°ì£¼ê³µí•™ / ì• ë„ë¦¬ìŠ¤íŠ¸, ê²½ì˜ ì»¨ì„¤íŒ…, íšŒê³„ì‚¬, ì„¸ë¬´ì‚¬ / íˆ¬ìë¶„ì„ê°€, M&A ì „ë¬¸ê°€ / IT ì»¨ì„¤íŒ…, ì»´í“¨í„° í”„ë¡œê·¸ë˜ë¨¸, web ê°œë°œ / í†µì‹  ì‹ í˜¸ì²˜ë¦¬, í†µê³„í•™, AI ê°œë°œ, ì •ë³´ì²˜ë¦¬, ë¹…ë°ì´í„° ì—…ë¬´ / ì€í–‰ì›, ê¸ˆìœµê¸°ê´€, ê°•ì‚¬, ë¹„í‰ê°€, ë…¼ì„¤ / ë³€í˜¸ì‚¬, ë³€ë¦¬ì‚¬, ê²€ì‚¬, íŒì‚¬ / ì˜ì‚¬, ê±´ì¶•ê°€, ì„¤ê³„ì‚¬"
    },
    "ê³µê°„ì§€ëŠ¥": {
        "description": "ğŸ¨ğŸ“¸ğŸ›ï¸: ê·¸ë¦¼ê³¼ ë””ìì¸ìœ¼ë¡œ ê³µê°„ì„ ì•„ë¦„ë‹µê²Œ í‘œí˜„!\n",
        "jobs": "ì‚¬ì§„ì‚¬, ì´¬ì˜ê¸°ì‚¬, ë§Œí™”ê°€, ì• ë‹ˆë©”ì´ì…˜, í™”ê°€, ì•„í‹°ìŠ¤íŠ¸ / ê±´ì¶• ì„¤ê³„, ì¸í…Œë¦¬ì–´, ë””ìì´ë„ˆ / ì§€ë„ ì œì‘, ì—”ì§€ë‹ˆì–´, ë°œëª…ê°€ / ì „ìê³µí•™, ê¸°ê³„ê³µí•™, í†µì‹ ê³µí•™, ì‚°ì—…ê³µí•™, ë¡œë´‡ ê°œë°œ / ì˜í™”ê°ë…, ë°©ì†¡ í”¼ë””, í‘¸ë“œìŠ¤íƒ€ì¼ë¦¬ìŠ¤íŠ¸ / ê´‘ê³  ì œì‘, ì¸ì‡„ ì—…ë¬´"
    },
    "ìŒì•…ì§€ëŠ¥": {
        "description": "ğŸ¶ğŸ§ğŸ¸: ì†Œë¦¬ì™€ ë¦¬ë“¬ì„ ëŠë¼ê³  ì°½ì¡°í•˜ëŠ” ìŒì•…ì  ì¬ëŠ¥!\n",
        "jobs": "ìŒì•…êµì‚¬, ìŒí–¥ì‚¬, ì‘ê³¡ê°€, ì‘ì‚¬ê°€, í¸ê³¡ê°€, ê°€ìˆ˜, ì„±ì•…ê°€ / ì•…ê¸° ì—°ì£¼ / ë™ì‹œí†µì—­ì‚¬, ì„±ìš° / ë®¤ì§€ì»¬ ë°°ìš° / ë°œë ˆ, ë¬´ìš© / ìŒí–¥ ë¶€ë¬¸, ì—°ì˜ˆ ê¸°íšì‚¬ / DJ, ê°œì¸ ìŒì•… ë°©ì†¡, ê°€ìˆ˜ ë§¤ë‹ˆì§€ë¨¼íŠ¸"
    },
    "ì‹ ì²´ìš´ë™ì§€ëŠ¥": {
        "description": "ğŸ€ğŸ¤¸â€â™‚ï¸ğŸ†: ëª¸ì„ í™œìš©í•´ ìŠ¤í¬ì¸ ì™€ ì›€ì§ì„ì—ì„œ ë‘ê°!\n",
        "jobs": "ì™¸ê³¼ì˜ì‚¬, ì¹˜ê¸°ê³µì‚¬, í•œì˜ì‚¬, ìˆ˜ì˜ì‚¬, ê°„í˜¸ì‚¬, ëŒ€ì²´ì˜í•™ / ë¬¼ë¦¬ì¹˜ë£Œì‚¬, ì‘ì—…ì¹˜ë£Œì‚¬ / ì•…ê¸° ì—°ì£¼, ì„±ì•…ê°€, ê°€ìˆ˜, ë¬´ìš©, ì—°ê·¹ / ìŠ¤í¬ì¸ , ì²´ìœ¡êµì‚¬, ëª¨ë¸ / ê²½ì°°, ê²½í˜¸ì›, êµ°ì¸, ì†Œë°©ê´€ / ë†ì—…, ì„ì—…, ìˆ˜ì‚°ì—…, ì¶•ì‚°ì—… / ê³µì˜ˆ, ì•¡ì„¸ì„œë¦¬ ì œì‘, ê°€êµ¬ ì œì‘"
    },
    "ëŒ€ì¸ê´€ê³„ì§€ëŠ¥": {
        "description": "ğŸ¤ğŸ—£ï¸ğŸ’¬: ì‚¬ëŒë“¤ê³¼ ì†Œí†µí•˜ë©° ê´€ê³„ë¥¼ ì˜ ë§ºëŠ” ëŠ¥ë ¥!\n",
        "jobs": "ë³€í˜¸ì‚¬, ê²€ì‚¬, íŒì‚¬, ë²•ë¬´ì‚¬ / êµì‚¬, êµìˆ˜, ê°•ì‚¬ / í™ë³´ ì—…ë¬´, ë§ˆì¼€íŒ… / ì§€ë°°ì¸, ë¹„ì„œ, ìŠ¹ë¬´ì›, íŒë§¤ì—…ë¬´ / ê¸°ì, ë¦¬í¬í„°, ë³´í—˜ì„œë¹„ìŠ¤ / ì™¸êµê´€, êµ­ì œê³µë¬´ì›, ê²½ì°° / ë³‘ì›ì½”ë””ë„¤ì´í„°, ê°„í˜¸ì‚¬ / í˜¸í…”ë¦¬ì–´, í•™ìŠµì§€ êµì‚¬, ì›¨ë”©í”Œë˜ë„ˆ, ì›ƒìŒì¹˜ë£Œì‚¬, ì„±ì§ì"
    },
    "ìê¸°ì´í•´ì§€ëŠ¥": {
        "description": "ğŸ§˜â€â™‚ï¸ğŸ’­ğŸ“–: ìì‹ ì„ ê¹Šì´ ì´í•´í•˜ê³  ì„±ì°°í•˜ëŠ” ë‚´ë©´ì˜ í˜!\n",
        "jobs": "ë³€í˜¸ì‚¬, ê²€ì‚¬, íŒì‚¬, ë³€ë¦¬ì‚¬, í‰ë¡ ê°€, ë…¼ì„¤ / êµì‚¬, êµìˆ˜, ì‹¬ë¦¬ìƒë‹´ì‚¬ / ìŠ¤í¬ì¸  ê°ë…, ì½”ì¹˜, ì‹¬íŒ, ìŠ¤í¬ì¸  í•´ì„¤ê°€ / í˜‘ìƒê°€, CEO, CTO, ì»¨ì„¤íŒ…, ë§ˆì¼€íŒ…, íšŒì‚¬ ê²½ì˜ / ê¸°ì, ì•„ë‚˜ìš´ì„œ, ìš”ë¦¬ì‚¬, ì‹¬ì‚¬ìœ„ì› / ì˜ì‚¬, ì œì•½ ë¶„ì•¼ ì—°êµ¬ì› / ì„±ì§ì, ì² í•™ì, íˆ¬ìë¶„ì„ê°€, ìì‚°ê´€ë¦¬ / ì˜í™”ê°ë…, ì‘ê°€, ê±´ì¶•ê°€"
    },
    "ìì—°ì¹œí™”ì§€ëŠ¥": {
        "description": "ğŸŒ¿ğŸ¦ğŸŒ: ìì—°ê³¼ ë™ë¬¼ì„ ì‚¬ë‘í•˜ë©° í™˜ê²½ì— ë¯¼ê°í•œ ì¬ëŠ¥!\n",
        "jobs": "ì˜ì‚¬, ê°„í˜¸ì‚¬, ë¬¼ë¦¬ì¹˜ë£Œ, ì„ìƒë³‘ë¦¬ / ìˆ˜ì˜ì‚¬, ë™ë¬¼ ì‚¬ìœ¡, ê³¤ì¶© ì‚¬ìœ¡ / ê±´ì¶• ì„¤ê³„, ê°ë¦¬, ì¸¡ëŸ‰ì‚¬, ì¡°ê²½ ë””ìì¸ / ì²œë¬¸í•™ì, ì§€ì§ˆí•™ì / ìƒëª…ê³µí•™, ê¸°ê³„ ê³µí•™, ìƒë¬¼ê³µí•™, ì „ìê³µí•™ / ì˜ì‚¬, ê°„í˜¸ì‚¬, ì•½ì œì‚¬, ì„ìƒë³‘ë¦¬ / íŠ¹ìˆ˜ì‘ë¬¼ ì¬ë°°, ë†ì—…, ì„ì—…, ì¶•ì‚°ì—…, ì›ì˜ˆ, í”Œë¡œë¦¬ìŠ¤íŠ¸"
    }
}

mbti_full_description = """
### ğŸ“ MBTI ìœ í˜•ë³„ í•œ ì¤„ ì„¤ëª…
#### ğŸ”¥ ì™¸í–¥í˜• (E) vs â„ï¸ ë‚´í–¥í˜• (I)  
- **E (ì™¸í–¥í˜•)** ğŸ‰ğŸ—£ï¸ğŸš€ğŸŒ: ì‚¬ëŒë“¤ê³¼ ì–´ìš¸ë¦¬ë©° ì—ë„ˆì§€ë¥¼ ì–»ëŠ” ì‚¬êµì ì¸ ì„±ê²©!  
- **I (ë‚´í–¥í˜•)** ğŸ“šğŸ›‹ï¸ğŸŒ™ğŸ¤«: í˜¼ìë§Œì˜ ì‹œê°„ì„ ì¦ê¸°ë©° ë‚´ë©´ì— ì§‘ì¤‘í•˜ëŠ” ì„±ê²©!  
#### ğŸ“Š ì§ê´€í˜• (N) vs ğŸ§ ê°ê°í˜• (S)  
- **N (ì§ê´€í˜•)** ğŸ’¡âœ¨ğŸ¨ğŸ”®: ì°½ì˜ì ì´ê³  í° ê·¸ë¦¼ì„ ë³´ë©° ì•„ì´ë””ì–´ë¥¼ ì¤‘ì‹œ!  
- **S (ê°ê°í˜•)** ğŸ”ğŸ“ğŸ› ï¸ğŸ½ï¸: í˜„ì‹¤ì ì´ê³  êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í–‰ë™!  
#### ğŸ¤ ê°ì •í˜• (F) vs âš–ï¸ ì‚¬ê³ í˜• (T)  
- **F (ê°ì •í˜•)** â¤ï¸ğŸ¥°ğŸŒ¸ğŸ«‚: ê³µê°ê³¼ ì‚¬ëŒ ì¤‘ì‹¬ìœ¼ë¡œ ë”°ëœ»í•œ ê²°ì •ì„ ë‚´ë¦¼!  
- **T (ì‚¬ê³ í˜•)** ğŸ§ âš™ï¸ğŸ“ŠğŸ“: ë…¼ë¦¬ì™€ ê°ê´€ì  íŒë‹¨ìœ¼ë¡œ ë¬¸ì œë¥¼ í•´ê²°!  
#### â³ íŒë‹¨í˜• (J) vs ğŸŒŠ ì¸ì‹í˜• (P)  
- **J (ê³„íší˜•)** ğŸ“…ğŸ“ŒğŸ“âœ…: ì²´ê³„ì ì´ê³  ê³„íšì ìœ¼ë¡œ ì¼ì„ ì²˜ë¦¬í•˜ëŠ” ìŠ¤íƒ€ì¼!  
- **P (ì¦‰í¥í˜•)** ğŸ­ğŸ¢ğŸŒªï¸ğŸŒ: ìœ ì—°í•˜ê³  ë³€í™”ì— ì˜ ì ì‘í•˜ëŠ” ììœ ë¡œìš´ ìŠ¤íƒ€ì¼!  
#### ğŸ­ MBTI ìœ í˜•ë³„ í•œ ì¤„ ì„¤ëª…  
- âœ… **ISTJ** (í˜„ì‹¤ì£¼ì˜ì) ğŸ›ï¸ğŸ“šğŸ§‘â€âš–ï¸: ì›ì¹™ì„ ì¤‘ì‹œí•˜ë©° ê¼¼ê¼¼í•œ ê³„íšìœ¼ë¡œ ëª©í‘œë¥¼ ë‹¬ì„±!  
- âœ… **ISFJ** (ë”°ëœ»í•œ ìˆ˜í˜¸ì) ğŸ›¡ï¸ğŸ§¸ğŸ’–: íƒ€ì¸ì„ ë°°ë ¤í•˜ë©° í—Œì‹ ì ì¸ ë„ì›€ì„ ì£¼ëŠ” ì„±ê²©!  
- âœ… **INFJ** (ì‹ ë¹„ë¡œìš´ ì¡°ì–¸ì) ğŸŒ¿ğŸ”®ğŸ“–: ê¹Šì€ í†µì°°ë ¥ìœ¼ë¡œ ì‚¬ëŒë“¤ì—ê²Œ ì˜ê°ì„ ì£¼ëŠ” ì´ìƒì£¼ì˜ì!  
- âœ… **INTJ** (ì „ëµê°€) ğŸ§ â™Ÿï¸ğŸ“ˆ: ë¯¸ë˜ë¥¼ ì„¤ê³„í•˜ë©° ëª©í‘œë¥¼ í–¥í•´ ë‚˜ì•„ê°€ëŠ” ë§ˆìŠ¤í„°ë§ˆì¸ë“œ!  
- âœ… **ISTP** (ë§ŒëŠ¥ ì¬ì£¼ê¾¼) ğŸ”§ğŸ•¶ï¸ğŸï¸: ë¬¸ì œë¥¼ ì‹¤ì§ˆì ìœ¼ë¡œ í•´ê²°í•˜ëŠ” ì‹¤ìš©ì ì¸ ëª¨í—˜ê°€!  
- âœ… **ISFP** (ì˜ˆìˆ ê°€) ğŸ¨ğŸµğŸ¦‹: ê°ì„±ì„ í‘œí˜„í•˜ë©° ììœ ë¡œìš´ ì‚¶ì„ ì¶”êµ¬í•˜ëŠ” ì˜ˆìˆ ê°€!  
- âœ… **INFP** (ì´ìƒì£¼ì˜ì) ğŸŒŒğŸ“œğŸ•Šï¸: ë‚´ë©´ì˜ ê°€ì¹˜ë¥¼ ì¤‘ì‹œí•˜ë©° ì„¸ìƒì„ ë” ë‚˜ì€ ê³³ìœ¼ë¡œ ë§Œë“œëŠ” ëª½ìƒê°€!  
- âœ… **INTP** (ë…¼ë¦¬ì ì¸ ì² í•™ì) ğŸ¤”ğŸ“–âš™ï¸: í˜¸ê¸°ì‹¬ ë§ê³  ë…¼ë¦¬ì ìœ¼ë¡œ ì„¸ìƒì„ íƒêµ¬í•˜ëŠ” ì‚¬ìƒ‰ê°€!  
- âœ… **ESTP** (ëª¨í—˜ê°€) ğŸï¸ğŸ”¥ğŸ¤: ìˆœê°„ì„ ì¦ê¸°ë©° ë„ì „ê³¼ ëª¨í—˜ì„ ì‚¬ë‘í•˜ëŠ” í™œë™ê°€!  
- âœ… **ESFP** (ì‚¬êµì ì¸ ì—°ì˜ˆì¸) ğŸ­ğŸ¤ğŸŠ: ì‚¬ëŒë“¤ê³¼ í•¨ê»˜í•˜ë©° ë¶„ìœ„ê¸°ë¥¼ ë„ìš°ëŠ” íŒŒí‹°ì˜ ì¤‘ì‹¬!  
- âœ… **ENFP** (ììœ ë¡œìš´ ì˜í˜¼) ğŸŒˆğŸš€ğŸ’¡: ì°½ì˜ì ì¸ ì•„ì´ë””ì–´ë¡œ ì„¸ìƒì„ ë°íˆëŠ” ì—´ì •ì ì¸ ì˜í˜¼!  
- âœ… **ENTP** (í† ë¡ ê°€) ğŸ—£ï¸âš¡â™Ÿï¸: ìƒˆë¡œìš´ ì•„ì´ë””ì–´ë¥¼ íƒêµ¬í•˜ë©° ë…¼ìŸì„ ì¦ê¸°ëŠ” í˜ì‹ ê°€!  
- âœ… **ESTJ** (ì—„ê²©í•œ ê´€ë¦¬ì) ğŸ—ï¸ğŸ“ŠğŸ› ï¸: ì²´ê³„ì ìœ¼ë¡œ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ëŠ” ë¦¬ë”ì‹­ì˜ ëŒ€ê°€!  
- âœ… **ESFJ** (ì¹œì ˆí•œ ì™¸êµê´€) ğŸ’ğŸ¤—ğŸ¡: ì‚¬ëŒë“¤ì„ ì—°ê²°í•˜ë©° ë”°ëœ»í•œ ê³µë™ì²´ë¥¼ ë§Œë“œëŠ” ì™¸êµê´€!  
- âœ… **ENFJ** (ì—´ì •ì ì¸ ë¦¬ë”) ğŸŒŸğŸ¤ğŸ«¶: íƒ€ì¸ì„ ì´ëŒë©° ê¸ì •ì ì¸ ë³€í™”ë¥¼ ë§Œë“œëŠ” ì¹´ë¦¬ìŠ¤ë§ˆ ë¦¬ë”!  
- âœ… **ENTJ** (ì•¼ë§ê°€) ğŸ‘‘ğŸ“ˆğŸ”¥: ëª©í‘œë¥¼ í–¥í•´ ëŒì§„í•˜ë©° í° ê·¸ë¦¼ì„ ê·¸ë¦¬ëŠ” ì§€íœ˜ê´€!
"""

multi_iq_full_description = """
### ğŸ¨ ë‹¤ì¤‘ì§€ëŠ¥ ìœ í˜•ë³„ í•œ ì¤„ ì„¤ëª… ë° ì¶”ì²œ ì§ì—…  
- ğŸ“– **ì–¸ì–´ ì§€ëŠ¥** ğŸ“ğŸ“šğŸ“¢: ë§ê³¼ ê¸€ì„ í†µí•´ ìƒê°ì„ í‘œí˜„í•˜ëŠ” ë° íƒì›”!  
    - **ì¶”ì²œ ì§ì—…**: ì†Œì„¤ê°€, ì‹œì¸, ì‘ê°€, ë…¼ì„¤ / ë™í™” ì‘ê°€, ë°©ì†¡ì‘ê°€, ì˜í™”ëŒ€ë³¸ì‘ê°€, ì›¹íˆ° ì‘ê°€ / ì•„ë‚˜ìš´ì„œ, ë¦¬í¬í„°, ì„±ìš° / êµì‚¬, êµìˆ˜, ê°•ì‚¬, ë…ì„œ ì§€ë„ì‚¬ / ì–¸ì–´ì¹˜ë£Œì‚¬, ì‹¬ë¦¬ì¹˜ë£Œì‚¬, êµ¬ì—°ë™í™”ê°€  
- ğŸ”¢ **ë…¼ë¦¬-ìˆ˜í•™ ì§€ëŠ¥** ğŸ§®ğŸ“ŠğŸ§ : ë¶„ì„ì  ì‚¬ê³ ì™€ ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ì´ ë›°ì–´ë‚¨!  
    - **ì¶”ì²œ ì§ì—…**: ê³¼í•™ì, ë¬¼ë¦¬í•™ì, ìˆ˜í•™ì / ì˜ë£Œê³µí•™, ì „ìê³µí•™, ì»´í“¨í„° ê³µí•™, í•­ê³µìš°ì£¼ê³µí•™ / ì• ë„ë¦¬ìŠ¤íŠ¸, ê²½ì˜ ì»¨ì„¤íŒ…, íšŒê³„ì‚¬, ì„¸ë¬´ì‚¬ / íˆ¬ìë¶„ì„ê°€, M&A ì „ë¬¸ê°€ / IT ì»¨ì„¤íŒ…, ì»´í“¨í„° í”„ë¡œê·¸ë˜ë¨¸, web ê°œë°œ / í†µì‹  ì‹ í˜¸ì²˜ë¦¬, í†µê³„í•™, AI ê°œë°œ, ì •ë³´ì²˜ë¦¬, ë¹…ë°ì´í„° ì—…ë¬´ / ì€í–‰ì›, ê¸ˆìœµê¸°ê´€, ê°•ì‚¬, ë¹„í‰ê°€, ë…¼ì„¤ / ë³€í˜¸ì‚¬, ë³€ë¦¬ì‚¬, ê²€ì‚¬, íŒì‚¬ / ì˜ì‚¬, ê±´ì¶•ê°€, ì„¤ê³„ì‚¬  
- ğŸ¨ **ê³µê°„ ì§€ëŠ¥** ğŸ¨ğŸ“¸ğŸ›ï¸: ê·¸ë¦¼ê³¼ ë””ìì¸ìœ¼ë¡œ ê³µê°„ì„ ì•„ë¦„ë‹µê²Œ í‘œí˜„!  
    - **ì¶”ì²œ ì§ì—…**: ì‚¬ì§„ì‚¬, ì´¬ì˜ê¸°ì‚¬, ë§Œí™”ê°€, ì• ë‹ˆë©”ì´ì…˜, í™”ê°€, ì•„í‹°ìŠ¤íŠ¸ / ê±´ì¶• ì„¤ê³„, ì¸í…Œë¦¬ì–´, ë””ìì´ë„ˆ / ì§€ë„ ì œì‘, ì—”ì§€ë‹ˆì–´, ë°œëª…ê°€ / ì „ìê³µí•™, ê¸°ê³„ê³µí•™, í†µì‹ ê³µí•™, ì‚°ì—…ê³µí•™, ë¡œë´‡ ê°œë°œ / ì˜í™”ê°ë…, ë°©ì†¡ í”¼ë””, í‘¸ë“œìŠ¤íƒ€ì¼ë¦¬ìŠ¤íŠ¸ / ê´‘ê³  ì œì‘, ì¸ì‡„ ì—…ë¬´  
- ğŸµ **ìŒì•… ì§€ëŠ¥** ğŸ¶ğŸ§ğŸ¸: ì†Œë¦¬ì™€ ë¦¬ë“¬ì„ ëŠë¼ê³  ì°½ì¡°í•˜ëŠ” ìŒì•…ì  ì¬ëŠ¥!  
    - **ì¶”ì²œ ì§ì—…**: ìŒì•…êµì‚¬, ìŒí–¥ì‚¬, ì‘ê³¡ê°€, ì‘ì‚¬ê°€, í¸ê³¡ê°€, ê°€ìˆ˜, ì„±ì•…ê°€ / ì•…ê¸° ì—°ì£¼ / ë™ì‹œí†µì—­ì‚¬, ì„±ìš° / ë®¤ì§€ì»¬ ë°°ìš° / ë°œë ˆ, ë¬´ìš© / ìŒí–¥ ë¶€ë¬¸, ì—°ì˜ˆ ê¸°íšì‚¬ / DJ, ê°œì¸ ìŒì•… ë°©ì†¡, ê°€ìˆ˜ ë§¤ë‹ˆì§€ë¨¼íŠ¸  
- ğŸƒ **ì‹ ì²´-ìš´ë™ ì§€ëŠ¥** ğŸ€ğŸ¤¸â€â™‚ï¸ğŸ†: ëª¸ì„ í™œìš©í•´ ìŠ¤í¬ì¸ ì™€ ì›€ì§ì„ì—ì„œ ë‘ê°!  
    - **ì¶”ì²œ ì§ì—…**: ì™¸ê³¼ì˜ì‚¬, ì¹˜ê¸°ê³µì‚¬, í•œì˜ì‚¬, ìˆ˜ì˜ì‚¬, ê°„í˜¸ì‚¬, ëŒ€ì²´ì˜í•™ / ë¬¼ë¦¬ì¹˜ë£Œì‚¬, ì‘ì—…ì¹˜ë£Œì‚¬ / ì•…ê¸° ì—°ì£¼, ì„±ì•…ê°€, ê°€ìˆ˜, ë¬´ìš©, ì—°ê·¹ / ìŠ¤í¬ì¸ , ì²´ìœ¡êµì‚¬, ëª¨ë¸ / ê²½ì°°, ê²½í˜¸ì›, êµ°ì¸, ì†Œë°©ê´€ / ë†ì—…, ì„ì—…, ìˆ˜ì‚°ì—…, ì¶•ì‚°ì—… / ê³µì˜ˆ, ì•¡ì„¸ì„œë¦¬ ì œì‘, ê°€êµ¬ ì œì‘  
- ğŸ¤ **ëŒ€ì¸ê´€ê³„ ì§€ëŠ¥** ğŸ¤ğŸ—£ï¸ğŸ’¬: ì‚¬ëŒë“¤ê³¼ ì†Œí†µí•˜ë©° ê´€ê³„ë¥¼ ì˜ ë§ºëŠ” ëŠ¥ë ¥!  
    - **ì¶”ì²œ ì§ì—…**: ë³€í˜¸ì‚¬, ê²€ì‚¬, íŒì‚¬, ë²•ë¬´ì‚¬ / êµì‚¬, êµìˆ˜, ê°•ì‚¬ / í™ë³´ ì—…ë¬´, ë§ˆì¼€íŒ… / ì§€ë°°ì¸, ë¹„ì„œ, ìŠ¹ë¬´ì›, íŒë§¤ì—…ë¬´ / ê¸°ì, ë¦¬í¬í„°, ë³´í—˜ì„œë¹„ìŠ¤ / ì™¸êµê´€, êµ­ì œê³µë¬´ì›, ê²½ì°° / ë³‘ì›ì½”ë””ë„¤ì´í„°, ê°„í˜¸ì‚¬ / í˜¸í…”ë¦¬ì–´, í•™ìŠµì§€ êµì‚¬, ì›¨ë”©í”Œë˜ë„ˆ, ì›ƒìŒì¹˜ë£Œì‚¬, ì„±ì§ì  
- ğŸ§˜ **ìê¸° ì´í•´ ì§€ëŠ¥** ğŸ§˜â€â™‚ï¸ğŸ’­ğŸ“–: ìì‹ ì„ ê¹Šì´ ì´í•´í•˜ê³  ì„±ì°°í•˜ëŠ” ë‚´ë©´ì˜ í˜!  
    - **ì¶”ì²œ ì§ì—…**: ë³€í˜¸ì‚¬, ê²€ì‚¬, íŒì‚¬, ë³€ë¦¬ì‚¬, í‰ë¡ ê°€, ë…¼ì„¤ / êµì‚¬, êµìˆ˜, ì‹¬ë¦¬ìƒë‹´ì‚¬ / ìŠ¤í¬ì¸  ê°ë…, ì½”ì¹˜, ì‹¬íŒ, ìŠ¤í¬ì¸  í•´ì„¤ê°€ / í˜‘ìƒê°€, CEO, CTO, ì»¨ì„¤íŒ…, ë§ˆì¼€íŒ…, íšŒì‚¬ ê²½ì˜ / ê¸°ì, ì•„ë‚˜ìš´ì„œ, ìš”ë¦¬ì‚¬, ì‹¬ì‚¬ìœ„ì› / ì˜ì‚¬, ì œì•½ ë¶„ì•¼ ì—°êµ¬ì› / ì„±ì§ì, ì² í•™ì, íˆ¬ìë¶„ì„ê°€, ìì‚°ê´€ë¦¬ / ì˜í™”ê°ë…, ì‘ê°€, ê±´ì¶•ê°€  
- ğŸŒ± **ìì—° ì¹œí™” ì§€ëŠ¥** ğŸŒ¿ğŸ¦ğŸŒ: ìì—°ê³¼ ë™ë¬¼ì„ ì‚¬ë‘í•˜ë©° í™˜ê²½ì— ë¯¼ê°í•œ ì¬ëŠ¥!  
    - **ì¶”ì²œ ì§ì—…**: ì˜ì‚¬, ê°„í˜¸ì‚¬, ë¬¼ë¦¬ì¹˜ë£Œ, ì„ìƒë³‘ë¦¬ / ìˆ˜ì˜ì‚¬, ë™ë¬¼ ì‚¬ìœ¡, ê³¤ì¶© ì‚¬ìœ¡ / ê±´ì¶• ì„¤ê³„, ê°ë¦¬, ì¸¡ëŸ‰ì‚¬, ì¡°ê²½ ë””ìì¸ / ì²œë¬¸í•™ì, ì§€ì§ˆí•™ì / ìƒëª…ê³µí•™, ê¸°ê³„ ê³µí•™, ìƒë¬¼ê³µí•™, ì „ìê³µí•™ / ì˜ì‚¬, ê°„í˜¸ì‚¬, ì•½ì œì‚¬, ì„ìƒë³‘ë¦¬ / íŠ¹ìˆ˜ì‘ë¬¼ ì¬ë°°, ë†ì—…, ì„ì—…, ì¶•ì‚°ì—…, ì›ì˜ˆ, í”Œë¡œë¦¬ìŠ¤íŠ¸  
"""

# WeatherAPI í´ë˜ìŠ¤
class WeatherAPI:
    def __init__(self, cache_ttl=600):
        self.cache = cache_handler
        self.cache_ttl = cache_ttl

    def fetch_weather(self, url, params):
        session = requests.Session()
        retry_strategy = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("https://", adapter)
        try:
            response = session.get(url, params=params, timeout=3)
            response.raise_for_status()
            return response.json()
        except:
            return self.cache.get(f"weather:{params.get('q', '')}") or "ë‚ ì”¨ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    @lru_cache(maxsize=100)
    def get_city_info(self, city_name):
        cache_key = f"city_info:{city_name}"
        cached = self.cache.get(cache_key)
        if cached:
            return cached
        url = "http://api.openweathermap.org/geo/1.0/direct"
        params = {'q': city_name, 'limit': 1, 'appid': WEATHER_API_KEY}
        data = self.fetch_weather(url, params)
        if data and isinstance(data, list) and len(data) > 0:
            city_info = {"name": data[0]["name"], "lat": data[0]["lat"], "lon": data[0]["lon"]}
            self.cache.setex(cache_key, 86400, city_info)
            return city_info
        return None

    def get_city_weather(self, city_name):
        cache_key = f"weather:{city_name}"
        cached_data = self.cache.get(cache_key)
        if cached_data:
            return cached_data
        
        city_info = self.get_city_info(city_name)
        if not city_info:
            return f"'{city_name}'ì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        url = "https://api.openweathermap.org/data/2.5/weather"
        params = {'lat': city_info["lat"], 'lon': city_info["lon"], 'appid': WEATHER_API_KEY, 'units': 'metric', 'lang': 'kr'}
        data = self.fetch_weather(url, params)
        if isinstance(data, str):
            return data
        weather_emojis = {'Clear': 'â˜€ï¸', 'Clouds': 'â˜ï¸', 'Rain': 'ğŸŒ§ï¸', 'Snow': 'â„ï¸', 'Thunderstorm': 'â›ˆï¸', 'Drizzle': 'ğŸŒ¦ï¸', 'Mist': 'ğŸŒ«ï¸'}
        weather_emoji = weather_emojis.get(data['weather'][0]['main'], 'ğŸŒ¤ï¸')
        result = (
            f"í˜„ì¬ {data['name']}, {data['sys']['country']} ë‚ ì”¨ {weather_emoji}\n"
            f"ë‚ ì”¨: {data['weather'][0]['description']}\n"
            f"ì˜¨ë„: {data['main']['temp']}Â°C\n"
            f"ì²´ê°: {data['main']['feels_like']}Â°C\n"
            f"ìŠµë„: {data['main']['humidity']}%\n"
            f"í’ì†: {data['wind']['speed']}m/s\n"
            f"ë” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"
        )
        self.cache.setex(cache_key, self.cache_ttl, result)
        return result

    def get_forecast_by_day(self, city_name, days_from_today=1):
        cache_key = f"forecast:{city_name}:{days_from_today}"
        cached_data = self.cache.get(cache_key)
        if cached_data:
            return cached_data
        
        city_info = self.get_city_info(city_name)
        if not city_info:
            return f"'{city_name}'ì˜ ë‚ ì”¨ ì˜ˆë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        url = "https://api.openweathermap.org/data/2.5/forecast"
        params = {'lat': city_info["lat"], 'lon': city_info["lon"], 'appid': WEATHER_API_KEY, 'units': 'metric', 'lang': 'kr'}
        data = self.fetch_weather(url, params)
        if isinstance(data, str):
            return data
        target_date = (datetime.now() + timedelta(days=days_from_today)).strftime('%Y-%m-%d')
        forecast_text = f"{city_info['name']}ì˜ {target_date} ë‚ ì”¨ ì˜ˆë³´ ğŸŒ¤ï¸\n\n"
        weather_emojis = {'Clear': 'â˜€ï¸', 'Clouds': 'â˜ï¸', 'Rain': 'ğŸŒ§ï¸', 'Snow': 'â„ï¸', 'Thunderstorm': 'â›ˆï¸', 'Drizzle': 'ğŸŒ¦ï¸', 'Mist': 'ğŸŒ«ï¸'}
        
        found = False
        for forecast in data['list']:
            dt = datetime.fromtimestamp(forecast['dt']).strftime('%Y-%m-%d')
            if dt == target_date:
                found = True
                time_only = datetime.fromtimestamp(forecast['dt']).strftime('%H:%M')
                weather_emoji = weather_emojis.get(forecast['weather'][0]['main'], 'ğŸŒ¤ï¸')
                forecast_text += (
                    f"â° {time_only} {forecast['weather'][0]['description']} {weather_emoji} "
                    f"{forecast['main']['temp']}Â°C ğŸ’§{forecast['main']['humidity']}% ğŸŒ¬ï¸{forecast['wind']['speed']}m/s\n\n"
                )
        
        result = forecast_text + "ë” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š" if found else f"'{city_name}'ì˜ {target_date} ë‚ ì”¨ ì˜ˆë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        self.cache.setex(cache_key, self.cache_ttl, result)
        return result

    def get_weekly_forecast(self, city_name):
        cache_key = f"weekly_forecast:{city_name}"
        cached_data = self.cache.get(cache_key)
        if cached_data:
            return cached_data
        
        city_info = self.get_city_info(city_name)
        if not city_info:
            return f"'{city_name}'ì˜ ì£¼ê°„ ì˜ˆë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        url = "https://api.openweathermap.org/data/2.5/forecast"
        params = {'lat': city_info["lat"], 'lon': city_info["lon"], 'appid': WEATHER_API_KEY, 'units': 'metric', 'lang': 'kr'}
        data = self.fetch_weather(url, params)
        if isinstance(data, str):
            return data
        today = datetime.now().date()
        week_end = today + timedelta(days=6)
        daily_forecast = {}
        weekdays_kr = ["ì›”ìš”ì¼", "í™”ìš”ì¼", "ìˆ˜ìš”ì¼", "ëª©ìš”ì¼", "ê¸ˆìš”ì¼", "í† ìš”ì¼", "ì¼ìš”ì¼"]
        today_weekday = today.weekday()
        
        for forecast in data['list']:
            dt = datetime.fromtimestamp(forecast['dt']).date()
            if today <= dt <= week_end:
                dt_str = dt.strftime('%Y-%m-%d')
                if dt_str not in daily_forecast:
                    weekday_idx = (today_weekday + (dt - today).days) % 7
                    daily_forecast[dt_str] = {
                        'weekday': weekdays_kr[weekday_idx],
                        'temp_min': forecast['main']['temp_min'],
                        'temp_max': forecast['main']['temp_max'],
                        'weather': forecast['weather'][0]['description']
                    }
                else:
                    daily_forecast[dt_str]['temp_min'] = min(daily_forecast[dt_str]['temp_min'], forecast['main']['temp_min'])
                    daily_forecast[dt_str]['temp_max'] = max(daily_forecast[dt_str]['temp_max'], forecast['main']['temp_max'])
        
        today_str = today.strftime('%Y-%m-%d')
        today_weekday_str = weekdays_kr[today_weekday]
        forecast_text = f"{today_str}({today_weekday_str}) ê¸°ì¤€ {city_info['name']}ì˜ ì£¼ê°„ ë‚ ì”¨ ì˜ˆë³´ ğŸŒ¤ï¸\n"
        weather_emojis = {'Clear': 'â˜€ï¸', 'Clouds': 'â˜ï¸', 'Rain': 'ğŸŒ§ï¸', 'Snow': 'â„ï¸', 'Thunderstorm': 'â›ˆï¸', 'Drizzle': 'ğŸŒ¦ï¸', 'Mist': 'ğŸŒ«ï¸'}
        
        for date, info in daily_forecast.items():
            weather_emoji = weather_emojis.get(info['weather'].split()[0], 'ğŸŒ¤ï¸')
            forecast_text += (
                f"\n{info['weekday']}: {info['weather']} {weather_emoji} "
                f"ìµœì € {info['temp_min']}Â°C ìµœê³  {info['temp_max']}Â°C\n\n"
            )
        
        result = forecast_text + "\në” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"
        self.cache.setex(cache_key, self.cache_ttl, result)
        return result

# FootballAPI í´ë˜ìŠ¤
class FootballAPI:
    def __init__(self, api_key, cache_ttl=600):
        self.api_key = api_key
        self.base_url = "https://api.football-data.org/v4/competitions"
        self.cache = cache_handler
        self.cache_ttl = cache_ttl

    def fetch_league_standings(self, league_code, league_name):
        cache_key = f"league_standings:{league_code}"
        cached_data = self.cache.get(cache_key)
        if cached_data is not None:
            return cached_data

        url = f"{self.base_url}/{league_code}/standings"
        headers = {'X-Auth-Token': self.api_key}
        
        try:
            time.sleep(1)
            response = requests.get(url, headers=headers, timeout=3)
            response.raise_for_status()
            data = response.json()
            
            standings = data['standings'][0]['table'] if league_code not in ["CL"] else data['standings']
            if league_code in ["CL"]:
                standings_data = []
                for group in standings:
                    for team in group['table']:
                        standings_data.append({
                            'ìˆœìœ„': team['position'],
                            'ê·¸ë£¹': group['group'],
                            'íŒ€': team['team']['name'],
                            'ê²½ê¸°': team['playedGames'],
                            'ìŠ¹': team['won'],
                            'ë¬´': team['draw'],
                            'íŒ¨': team['lost'],
                            'ë“ì ': team['goalsFor'],
                            'ì‹¤ì ': team['goalsAgainst'],
                            'ë“ì‹¤ì°¨': team['goalsFor'] - team['goalsAgainst'],
                            'í¬ì¸íŠ¸': team['points']
                        })
                df = pd.DataFrame(standings_data)
            else:
                df = pd.DataFrame([
                    {
                        'ìˆœìœ„': team['position'],
                        'íŒ€': team['team']['name'],
                        'ê²½ê¸°': team['playedGames'],
                        'ìŠ¹': team['won'],
                        'ë¬´': team['draw'],
                        'íŒ¨': team['lost'],
                        'ë“ì ': team['goalsFor'],
                        'ì‹¤ì ': team['goalsAgainst'],
                        'ë“ì‹¤ì°¨': team['goalsFor'] - team['goalsAgainst'],
                        'í¬ì¸íŠ¸': team['points']
                    } for team in standings
                ])
            
            result = {"league_name": league_name, "data": df}
            self.cache.setex(cache_key, self.cache_ttl, result)
            return result
        
        except requests.exceptions.RequestException as e:
            return {"league_name": league_name, "error": f"{league_name} ë¦¬ê·¸ ìˆœìœ„ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)} ğŸ˜“"}

    def fetch_league_scorers(self, league_code, league_name):
        cache_key = f"league_scorers:{league_code}"
        cached_data = self.cache.get(cache_key)
        if cached_data is not None:
            return cached_data

        url = f"{self.base_url}/{league_code}/scorers"
        headers = {'X-Auth-Token': self.api_key}
        
        try:
            time.sleep(1)
            response = requests.get(url, headers=headers, timeout=3)
            response.raise_for_status()
            data = response.json()
            
            scorers = [{"ìˆœìœ„": i+1, "ì„ ìˆ˜": s['player']['name'], "íŒ€": s['team']['name'], "ë“ì ": s['goals']} 
                       for i, s in enumerate(data['scorers'][:10])]
            df = pd.DataFrame(scorers)
            result = {"league_name": league_name, "data": df}
            self.cache.setex(cache_key, self.cache_ttl, result)
            return result
        
        except requests.exceptions.RequestException as e:
            return {"league_name": league_name, "error": f"{league_name} ë¦¬ê·¸ ë“ì ìˆœìœ„ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)} ğŸ˜“"}

# ì´ˆê¸°í™”
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
client = Client(exclude_providers=["OpenaiChat", "Copilot", "Liaobots", "Jmuz", "PollinationsAI", "ChatGptEs"])
weather_api = WeatherAPI()
football_api = FootballAPI(api_key=SPORTS_API_KEY)
naver_request_count = 0
NAVER_DAILY_LIMIT = 25000
st.set_page_config(page_title="AI ì±—ë´‡", page_icon="ğŸ¤–")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
def init_session_state():
    if "is_logged_in" not in st.session_state:
        st.session_state.is_logged_in = False
    if "user_id" not in st.session_state:
        st.session_state.user_id = None
    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?ğŸ˜Š"}]
    if "session_id" not in st.session_state:
        st.session_state.session_id = str(uuid.uuid4())

# ë„ì‹œ ë° ì‹œê°„ ì¶”ì¶œ
CITY_PATTERNS = [
    re.compile(r'(?:ì˜¤ëŠ˜|ë‚´ì¼|ëª¨ë ˆ|ì´ë²ˆ ì£¼|ì£¼ê°„)?\s*([ê°€-í£a-zA-Z\s]{2,20}(?:ì‹œ|êµ°|city)?)ì˜?\s*ë‚ ì”¨', re.IGNORECASE),
    re.compile(r'(?:ì˜¤ëŠ˜|ë‚´ì¼|ëª¨ë ˆ|ì´ë²ˆ ì£¼|ì£¼ê°„)?\s*([ê°€-í£a-zA-Z\s]{2,20}(?:ì‹œ|êµ°|city)?)\s*ë‚ ì”¨', re.IGNORECASE),
]
def extract_city_from_query(query):
    for pattern in CITY_PATTERNS:
        match = pattern.search(query)
        if match:
            city = match.group(1).strip()
            if city not in ["ì˜¤ëŠ˜", "ë‚´ì¼", "ëª¨ë ˆ", "ì´ë²ˆ ì£¼", "ì£¼ê°„", "í˜„ì¬"]:
                return city
    return "ì„œìš¸"

TIME_CITY_PATTERNS = [
    re.compile(r'([ê°€-í£a-zA-Z]{2,20}(?:ì‹œ|êµ°)?)ì˜?\s*ì‹œê°„'),
    re.compile(r'([ê°€-í£a-zA-Z]{2,20}(?:ì‹œ|êµ°)?)\s*ì‹œê°„'),
]
def extract_city_from_time_query(query):
    for pattern in TIME_CITY_PATTERNS:
        match = pattern.search(query)
        if match:
            city = match.group(1).strip()
            if city != "í˜„ì¬":
                return city
    return "ì„œìš¸"

LEAGUE_MAPPING = {
    "epl": {"name": "í”„ë¦¬ë¯¸ì–´ë¦¬ê·¸ (ì˜êµ­)", "code": "PL"},
    "laliga": {"name": "ë¼ë¦¬ê°€ (ìŠ¤í˜ì¸)", "code": "PD"},
    "bundesliga": {"name": "ë¶„ë°ìŠ¤ë¦¬ê°€ (ë…ì¼)", "code": "BL1"},
    "seriea": {"name": "ì„¸ë¦¬ì— A (ì´íƒˆë¦¬ì•„)", "code": "SA"},
    "ligue1": {"name": "ë¦¬ê·¸ 1 (í”„ë‘ìŠ¤)", "code": "FL1"},
    "championsleague": {"name": "ì±”í”¼ì–¸ìŠ¤ ë¦¬ê·¸", "code": "CL"}
}

def extract_league_from_query(query):
    query_lower = query.lower().replace(" ", "")
    league_keywords = {
        "epl": ["epl", "í”„ë¦¬ë¯¸ì–´ë¦¬ê·¸"],
        "laliga": ["laliga", "ë¼ë¦¬ê°€"],
        "bundesliga": ["bundesliga", "ë¶„ë°ìŠ¤ë¦¬ê°€"],
        "seriea": ["seriea", "ì„¸ë¦¬ì—a"],
        "ligue1": ["ligue1", "ë¦¬ê·¸1"],
        "championsleague": ["championsleague", "ì±”í”¼ì–¸ìŠ¤ë¦¬ê·¸", "ucl"]
    }
    for league_key, keywords in league_keywords.items():
        if any(keyword in query_lower for keyword in keywords):
            return league_key
    return None

def get_kst_time():
    kst_timezone = pytz.timezone("Asia/Seoul")
    kst_time = datetime.now(kst_timezone)
    return f"ëŒ€í•œë¯¼êµ­ ê¸°ì¤€ : {kst_time.strftime('%Yë…„ %mì›” %dì¼ %p %I:%M')}ì…ë‹ˆë‹¤. â°\n\n ë” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"

def get_time_by_city(city_name="ì„œìš¸"):
    city_info = weather_api.get_city_info(city_name)
    if not city_info:
        return f"'{city_name}'ì˜ ì‹œê°„ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    tf = TimezoneFinder()
    timezone_str = tf.timezone_at(lng=city_info["lon"], lat=city_info["lat"]) or "Asia/Seoul"
    timezone = pytz.timezone(timezone_str)
    city_time = datetime.now(timezone)
    return f"í˜„ì¬ {city_name} ì‹œê°„: {city_time.strftime('%Yë…„ %mì›” %dì¼ %p %I:%M')}ì…ë‹ˆë‹¤. â°\n\n ë” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"

# ì‚¬ìš©ì ë° ì±„íŒ… ê¸°ë¡ ê´€ë¦¬
def create_or_get_user(nickname):
    user = supabase.table("users").select("*").eq("nickname", nickname).execute()
    if user.data:
        return user.data[0]["id"], True
    new_user = supabase.table("users").insert({"nickname": nickname, "created_at": datetime.now().isoformat()}).execute()
    return new_user.data[0]["id"], False


def save_chat_history(user_id, session_id, question, answer, time_taken):
    # generatorì´ë©´ contentë¥¼ ë³‘í•©í•´ì„œ ë¬¸ìì—´ë¡œ ë³€í™˜
    if hasattr(answer, '__iter__') and not isinstance(answer, (str, dict, list)):
        try:
            answer = ''.join([chunk.choices[0].delta.content for chunk in answer if hasattr(chunk.choices[0].delta, 'content') and chunk.choices[0].delta.content])
        except Exception as e:
            answer = f"[streaming ì‘ë‹µ ì˜¤ë¥˜: {str(e)}]"

    supabase.table("chat_history").insert({
        "user_id": user_id,
        "session_id": session_id,
        "question": question,
        "answer": answer,
        "time_taken": time_taken,
        "created_at": datetime.now().isoformat()
    }).execute()

# def save_chat_history(user_id, session_id, question, answer, time_taken):
#     if isinstance(answer, dict) and "table" in answer and isinstance(answer["table"], pd.DataFrame):
#         answer_to_save = {
#             "header": answer["header"],
#             "table": answer["table"].to_dict(orient="records"),
#             "footer": answer["footer"]
#         }
#     else:
#         answer_to_save = answer
    
#     supabase.table("chat_history").insert({
#         "user_id": user_id,
#         "session_id": session_id,
#         "question": question,
#         "answer": answer_to_save,
#         "time_taken": time_taken,
#         "created_at": datetime.now().isoformat()
#     }).execute()

def async_save_chat_history(user_id, session_id, question, answer, time_taken):
    threading.Thread(target=save_chat_history, args=(user_id, session_id, question, answer, time_taken)).start()

# ì˜ì•½í’ˆ ê²€ìƒ‰
def get_drug_info(drug_query):
    drug_name = drug_query.replace("ì•½í’ˆê²€ìƒ‰", "").strip()
    cache_key = f"drug:{drug_name}"
    cached = cache_handler.get(cache_key)
    if cached:
        return cached
    
    url = 'http://apis.data.go.kr/1471000/DrbEasyDrugInfoService/getDrbEasyDrugList'
    params = {'serviceKey': DRUG_API_KEY, 'pageNo': '1', 'numOfRows': '1', 'itemName': urllib.parse.quote(drug_name), 'type': 'json'}
    try:
        response = requests.get(url, params=params, timeout=3)
        response.raise_for_status()
        data = response.json()
        if 'body' in data and 'items' in data['body'] and data['body']['items']:
            item = data['body']['items'][0]
            efcy = item.get('efcyQesitm', 'ì •ë³´ ì—†ìŒ')[:150] + ("..." if len(item.get('efcyQesitm', '')) > 150 else "")
            use_method = item.get('useMethodQesitm', 'ì •ë³´ ì—†ìŒ')[:150] + ("..." if len(item.get('useMethodQesitm', '')) > 150 else "")
            atpn = item.get('atpnQesitm', 'ì •ë³´ ì—†ìŒ')[:150] + ("..." if len(item.get('atpnQesitm', '')) > 150 else "")
            
            result = (
                f"ğŸ’Š **ì˜ì•½í’ˆ ì •ë³´** ğŸ’Š\n\n"
                f"âœ… **ì•½í’ˆëª…**: {item.get('itemName', 'ì •ë³´ ì—†ìŒ')}\n\n"
                f"âœ… **ì œì¡°ì‚¬**: {item.get('entpName', 'ì •ë³´ ì—†ìŒ')}\n\n"
                f"âœ… **íš¨ëŠ¥**: {efcy}\n\n"
                f"âœ… **ìš©ë²•ìš©ëŸ‰**: {use_method}\n\n"
                f"âœ… **ì£¼ì˜ì‚¬í•­**: {atpn}\n\n"
                f"ë” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"
            )
            cache_handler.setex(cache_key, 86400, result)
            return result
        return f"'{drug_name}'ì˜ ê³µì‹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    except Exception as e:
        logger.error(f"ì•½í’ˆ API ì˜¤ë¥˜: {str(e)}")
        return f"'{drug_name}'ì˜ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ğŸ˜“"

# Naver API ê²€ìƒ‰
def get_naver_api_results(query):
    global naver_request_count
    cache_key = f"naver:{query}"
    cached = cache_handler.get(cache_key)
    if cached:
        return cached
    
    if naver_request_count >= NAVER_DAILY_LIMIT:
        return "ê²€ìƒ‰ í•œë„ ì´ˆê³¼ë¡œ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ğŸ˜“"
    enc_text = urllib.parse.quote(query)
    url = f"https://openapi.naver.com/v1/search/webkr?query={enc_text}&display=5&sort=date"
    request = urllib.request.Request(url)
    request.add_header("X-Naver-Client-Id", NAVER_CLIENT_ID)
    request.add_header("X-Naver-Client-Secret", NAVER_CLIENT_SECRET)
    try:
        response = urllib.request.urlopen(request, timeout=3)
        naver_request_count += 1
        if response.getcode() == 200:
            data = json.loads(response.read().decode('utf-8'))
            results = data.get('items', [])
            if not results:
                return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ğŸ˜“"
            
            response_text = "ğŸŒ **ì›¹ ê²€ìƒ‰ ê²°ê³¼** \n\n"
            response_text += "\n\n".join(
                [f"**ê²°ê³¼ {i}**\n\nğŸ“„ **ì œëª©**: {re.sub(r'<b>|</b>', '', item['title'])}\n\nğŸ“ **ë‚´ìš©**: {re.sub(r'<b>|</b>', '', item.get('description', 'ë‚´ìš© ì—†ìŒ'))[:100]}...\n\nğŸ”— **ë§í¬**: {item.get('link', '')}"
                 for i, item in enumerate(results, 1)]
            ) + "\n\në” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"
            cache_handler.setex(cache_key, 3600, response_text)
            return response_text
    except Exception as e:
        logger.error(f"Naver API ì˜¤ë¥˜: {str(e)}")
        return "ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ğŸ˜“"

# ArXiv ë…¼ë¬¸ ê²€ìƒ‰
def fetch_arxiv_paper(paper):
    return {
        "title": paper.title,
        "authors": ", ".join(str(a) for a in paper.authors),
        "summary": paper.summary[:200],
        "entry_id": paper.entry_id,
        "pdf_url": paper.pdf_url,
        "published": paper.published.strftime('%Y-%m-%d')
    }

def get_arxiv_papers(query, max_results=3):
    cache_key = f"arxiv:{query}:{max_results}"
    cached = cache_handler.get(cache_key)
    if cached:
        return cached
    search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.SubmittedDate)
    with ThreadPoolExecutor() as executor:
        results = list(executor.map(fetch_arxiv_paper, search.results()))
    if not results:
        return "í•´ë‹¹ í‚¤ì›Œë“œë¡œ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    response = "ğŸ“š **Arxiv ë…¼ë¬¸ ê²€ìƒ‰ ê²°ê³¼** ğŸ“š\n\n"
    response += "\n\n".join(
        [f"**ë…¼ë¬¸ {i}**\n\nğŸ“„ **ì œëª©**: {r['title']}\n\nğŸ‘¥ **ì €ì**: {r['authors']}\n\nğŸ“ **ì´ˆë¡**: {r['summary']}...\n\nğŸ”— **ë…¼ë¬¸ í˜ì´ì§€**: {r['entry_id']}\n\nğŸ“… **ì¶œíŒì¼**: {r['published']}"
         for i, r in enumerate(results, 1)]
    ) + "\n\në” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"
    cache_handler.setex(cache_key, 3600, response)
    return response

# PubMed ë…¼ë¬¸ ê²€ìƒ‰
base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/"

def search_pubmed(query, max_results=5):
    search_url = f"{base_url}esearch.fcgi"
    params = {"db": "pubmed", "term": query, "retmode": "json", "retmax": max_results, "api_key": NCBI_KEY}
    response = requests.get(search_url, params=params, timeout=3)
    return response.json()

def get_pubmed_summaries(id_list):
    summary_url = f"{base_url}esummary.fcgi"
    params = {"db": "pubmed", "id": ",".join(id_list), "retmode": "json", "api_key": NCBI_KEY}
    response = requests.get(summary_url, params=params, timeout=3)
    return response.json()

def get_pubmed_abstract(id_list):
    fetch_url = f"{base_url}efetch.fcgi"
    params = {"db": "pubmed", "id": ",".join(id_list), "retmode": "xml", "rettype": "abstract", "api_key": NCBI_KEY}
    response = requests.get(fetch_url, params=params, timeout=3)
    return response.text

def extract_first_two_sentences(abstract_text):
    if not abstract_text or abstract_text.isspace():
        return "No abstract available"
    sentences = [s.strip() for s in abstract_text.split('.') if s.strip()]
    return " ".join(sentences[:2]) + "." if sentences else "No abstract available"

def parse_abstracts(xml_text):
    abstract_dict = {}
    try:
        root = ET.fromstring(xml_text)
        for article in root.findall(".//PubmedArticle"):
            pmid = article.find(".//MedlineCitation/PMID").text
            abstract_elem = article.find(".//Abstract/AbstractText")
            abstract = abstract_elem.text if abstract_elem is not None else "No abstract available"
            abstract_dict[pmid] = extract_first_two_sentences(abstract)
    except ET.ParseError:
        return {}
    return abstract_dict

def get_pubmed_papers(query, max_results=5):
    cache_key = f"pubmed:{query}:{max_results}"
    cached = cache_handler.get(cache_key)
    if cached:
        return cached
    
    search_results = search_pubmed(query, max_results)
    pubmed_ids = search_results["esearchresult"]["idlist"]
    if not pubmed_ids:
        return "í•´ë‹¹ í‚¤ì›Œë“œë¡œ ì˜í•™ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    summaries = get_pubmed_summaries(pubmed_ids)
    abstracts_xml = get_pubmed_abstract(pubmed_ids)
    abstract_dict = parse_abstracts(abstracts_xml)
    
    response = "ğŸ“š **PubMed ë…¼ë¬¸ ê²€ìƒ‰ ê²°ê³¼** ğŸ“š\n\n"
    response += "\n\n".join(
        [f"**ë…¼ë¬¸ {i}**\n\nğŸ†” **PMID**: {pmid}\n\nğŸ“– **ì œëª©**: {summaries['result'][pmid].get('title', 'No title')}\n\nğŸ“… **ì¶œíŒì¼**: {summaries['result'][pmid].get('pubdate', 'No date')}\n\nâœï¸ **ì €ì**: {', '.join([author.get('name', '') for author in summaries['result'][pmid].get('authors', [])])}\n\nğŸ“ **ì´ˆë¡**: {abstract_dict.get(pmid, 'No abstract')}"
         for i, pmid in enumerate(pubmed_ids, 1)]
    ) + "\n\në” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"
    cache_handler.setex(cache_key, 3600, response)
    return response

# ëŒ€í™”í˜• ì‘ë‹µ (ìŠ¤íŠ¸ë¦¬ë° ì ìš©)
conversation_cache = MemoryCache()
async def get_conversational_response(query, messages):
    cache_key = f"conv:{needs_search(query)}:{query}"
    cached = conversation_cache.get(cache_key)
    if cached:
        return cached, False
    
    system_message = {"role": "system", "content": "ì¹œì ˆí•œ AI ì±—ë´‡ì…ë‹ˆë‹¤. ì ì ˆí•œ ì´ëª¨ì§€ ì‚¬ìš©: âœ…(ì™„ë£Œ), â“(ì§ˆë¬¸), ğŸ˜Š(ì¹œì ˆ)"}
    conversation_history = [system_message] + messages[-2:] + [{"role": "user", "content": query}]
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=conversation_history,
            web_search=False,
            stream=True
        )
        return response, True
    except Exception as e:
        logger.error(f"ëŒ€í™” ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return f"ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)} ğŸ˜“", False

GREETINGS = ["ì•ˆë…•", "í•˜ì´", "í—¬ë¡œ", "ã…ã…‡", "ì™“ì—…", "í• ë¡±", "í—¤ì´"]
GREETING_RESPONSE = "ì•ˆë…•í•˜ì„¸ìš”! ë°˜ê°‘ìŠµë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ğŸ˜Š"

@lru_cache(maxsize=100)
def needs_search(query):
    query_lower = query.strip().lower().replace(" ", "")
    if "ë‚ ì”¨" in query_lower:
        return "weather" if "ë‚´ì¼" not in query_lower else "tomorrow_weather"
    if "ì‹œê°„" in query_lower or "ë‚ ì§œ" in query_lower:
        return "time"
    if "ë¦¬ê·¸ìˆœìœ„" in query_lower:
        return "league_standings"
    if "ë¦¬ê·¸ë“ì ìˆœìœ„" in query_lower or "ë“ì ìˆœìœ„" in query_lower:
        return "league_scorers"
    if "ì•½í’ˆê²€ìƒ‰" in query_lower:
        return "drug"
    if "ê³µí•™ë…¼ë¬¸" in query_lower or "arxiv" in query_lower:
        return "arxiv_search"
    if "ì˜í•™ë…¼ë¬¸" in query_lower:
        return "pubmed_search"
    if "ê²€ìƒ‰" in query_lower:
        return "naver_search"
    if "mbti" in query_lower:
        if "ìœ í˜•" in query_lower or "ì„¤ëª…" in query_lower:
            return "mbti_types"
        return "mbti"
    if "ë‹¤ì¤‘ì§€ëŠ¥" in query_lower or "multi_iq" in query_lower:
        if "ìœ í˜•" in query_lower or "ì„¤ëª…" in query_lower:
            return "multi_iq_types"
        if "ì§ì—…" in query_lower or "ì¶”ì²œ" in query_lower:
            return "multi_iq_jobs"
        return "multi_iq"
    if any(greeting in query_lower for greeting in GREETINGS):
        return "conversation"
    return "conversation"

def process_query(query, messages):
    cache_key = f"query:{hash(query)}"
    cached = cache_handler.get(cache_key)
    if cached is not None:
        return cached, False
    
    query_type = needs_search(query)
    query_lower = query.strip().lower().replace(" ", "")
    
    with ThreadPoolExecutor() as executor:
        if query_type == "weather":
            future = executor.submit(weather_api.get_city_weather, extract_city_from_query(query))
            result = future.result()
            cache_handler.setex(cache_key, 600, result)
            return result, False
        elif query_type == "tomorrow_weather":
            future = executor.submit(weather_api.get_forecast_by_day, extract_city_from_query(query), 1)
            result = future.result()
            cache_handler.setex(cache_key, 600, result)
            return result, False
        elif query_type == "time":
            if "ì˜¤ëŠ˜ë‚ ì§œ" in query_lower or "í˜„ì¬ë‚ ì§œ" in query_lower or "ê¸ˆì¼ë‚ ì§œ" in query_lower:
                result = get_kst_time()
            else:
                city = extract_city_from_time_query(query)
                future = executor.submit(get_time_by_city, city)
                result = future.result()
            cache_handler.setex(cache_key, 600, result)
            return result, False
        elif query_type == "league_standings":
            league_key = extract_league_from_query(query)
            if league_key:
                league_info = LEAGUE_MAPPING[league_key]
                future = executor.submit(football_api.fetch_league_standings, league_info["code"], league_info["name"])
                result = future.result()
                result = result["error"] if "error" in result else {
                    "header": f"{result['league_name']} ë¦¬ê·¸ ìˆœìœ„",
                    "table": result["data"],
                    "footer": "ë” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"
                }
                cache_handler.setex(cache_key, 600, result)
            else:
                result = "ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¦¬ê·¸ì…ë‹ˆë‹¤. ğŸ˜“ ì§€ì› ë¦¬ê·¸: EPL, LaLiga, Bundesliga, Serie A, Ligue 1"
            return result, False
        elif query_type == "league_scorers":
            league_key = extract_league_from_query(query)
            if league_key:
                league_info = LEAGUE_MAPPING[league_key]
                future = executor.submit(football_api.fetch_league_scorers, league_info["code"], league_info["name"])
                try:
                    result = future.result()
                    result = result["error"] if "error" in result else {
                        "header": f"{result['league_name']} ë¦¬ê·¸ ë“ì ìˆœìœ„ (ìƒìœ„ 10ëª…)",
                        "table": result["data"],
                        "footer": "ë” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"
                    }
                    cache_handler.setex(cache_key, 600, result)
                except Exception as e:
                    result = f"ë¦¬ê·¸ ë“ì ìˆœìœ„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)} ğŸ˜“"
            else:
                result = "ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¦¬ê·¸ì…ë‹ˆë‹¤. ğŸ˜“ ì§€ì› ë¦¬ê·¸: EPL, LaLiga, Bundesliga, Serie A, Ligue 1"
            return result, False
        elif query_type == "drug":
            future = executor.submit(get_drug_info, query)
            result = future.result()
            cache_handler.setex(cache_key, 600, result)
            return result, False
        elif query_type == "arxiv_search":
            keywords = query.replace("ê³µí•™ë…¼ë¬¸", "").replace("arxiv", "").strip()
            future = executor.submit(get_arxiv_papers, keywords)
            result = future.result()
            cache_handler.setex(cache_key, 600, result)
            return result, False
        elif query_type == "pubmed_search":
            keywords = query.replace("ì˜í•™ë…¼ë¬¸", "").strip()
            future = executor.submit(get_pubmed_papers, keywords)
            result = future.result()
            cache_handler.setex(cache_key, 600, result)
            return result, False
        elif query_type == "naver_search":
            search_query = query.lower().replace("ê²€ìƒ‰", "").strip()
            future = executor.submit(get_naver_api_results, search_query)
            result = future.result()
            cache_handler.setex(cache_key, 600, result)
            return result, False
        elif query_type == "mbti":
            result = (
                "MBTI ê²€ì‚¬ë¥¼ ì›í•˜ì‹œë‚˜ìš”? âœ¨ ì•„ë˜ ì‚¬ì´íŠ¸ì—ì„œ ë¬´ë£Œë¡œ ì„±ê²© ìœ í˜• ê²€ì‚¬ë¥¼ í•  ìˆ˜ ìˆì–´ìš”! ğŸ˜Š\n"
                "[16Personalities MBTI ê²€ì‚¬](https://www.16personalities.com/ko/%EB%AC%B4%EB%A3%8C-%EC%84%B1%EA%B2%A9-%EC%9C%A0%ED%98%95-%EA%B2%80%EC%82%AC) ğŸŒŸ\n"
                "ì´ ì‚¬ì´íŠ¸ëŠ” 16ê°€ì§€ ì„±ê²© ìœ í˜•ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ í…ŒìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ë©°, ê²°ê³¼ì— ë”°ë¼ ì„±ê²© ì„¤ëª…ê³¼ ì¸ê°„ê´€ê³„ ì¡°ì–¸ ë“±ì„ í™•ì¸í•  ìˆ˜ ìˆì–´ìš”! ğŸ’¡"
            )
            cache_handler.setex(cache_key, 600, result)
            return result, False
        elif query_type == "mbti_types":
            specific_type = query_lower.replace("mbti", "").replace("ìœ í˜•", "").replace("ì„¤ëª…", "").strip().upper()
            if specific_type in mbti_descriptions:
                result = f"### ğŸ­ {specific_type} í•œ ì¤„ ì„¤ëª…\n- âœ… **{specific_type}** {mbti_descriptions[specific_type]}"
            else:
                result = mbti_full_description
            cache_handler.setex(cache_key, 600, result)
            return result, False
        elif query_type == "multi_iq":
            result = (
                "ë‹¤ì¤‘ì§€ëŠ¥ ê²€ì‚¬ë¥¼ ì›í•˜ì‹œë‚˜ìš”? ğŸ‰ ì•„ë˜ ì‚¬ì´íŠ¸ì—ì„œ ë¬´ë£Œë¡œ ë‹¤ì¤‘ì§€ëŠ¥ í…ŒìŠ¤íŠ¸ë¥¼ í•´ë³¼ ìˆ˜ ìˆì–´ìš”! ğŸ˜„\n"
                "[Multi IQ Test](https://multiiqtest.com/) ğŸš€\n"
                "ì´ ì‚¬ì´íŠ¸ëŠ” í•˜ì›Œë“œ ê°€ë“œë„ˆì˜ ë‹¤ì¤‘ì§€ëŠ¥ ì´ë¡ ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ í…ŒìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ë©°, ë‹¤ì–‘í•œ ì§€ëŠ¥ ì˜ì—­ì„ í‰ê°€í•´ì¤ë‹ˆë‹¤! ğŸ“šâœ¨"
            )
            cache_handler.setex(cache_key, 600, result)
            return result, False
        elif query_type == "multi_iq_types":
            specific_type = query_lower.replace("ë‹¤ì¤‘ì§€ëŠ¥", "").replace("multi_iq", "").replace("ìœ í˜•", "").replace("ì„¤ëª…", "").strip().replace(" ", "")
            if specific_type in multi_iq_descriptions:
                result = f"### ğŸ¨ {specific_type.replace('ì§€ëŠ¥', ' ì§€ëŠ¥')} í•œ ì¤„ ì„¤ëª…\n- ğŸ“– **{specific_type.replace('ì§€ëŠ¥', ' ì§€ëŠ¥')}** {multi_iq_descriptions[specific_type]['description']}"
            else:
                result = multi_iq_full_description
            cache_handler.setex(cache_key, 600, result)
            return result, False
        elif query_type == "multi_iq_jobs":
            specific_type = query_lower.replace("ë‹¤ì¤‘ì§€ëŠ¥", "").replace("multi_iq", "").replace("ì§ì—…", "").replace("ì¶”ì²œ", "").strip().replace(" ", "")
            if specific_type in multi_iq_descriptions:
                result = f"### ğŸ¨ {specific_type.replace('ì§€ëŠ¥', ' ì§€ëŠ¥')} ì¶”ì²œ ì§ì—…\n- ğŸ“– **{specific_type.replace('ì§€ëŠ¥', ' ì§€ëŠ¥')}**: {multi_iq_descriptions[specific_type]['description']}- **ì¶”ì²œ ì§ì—…**: {multi_iq_descriptions[specific_type]['jobs']}"
            else:
                result = multi_iq_full_description
            cache_handler.setex(cache_key, 600, result)
            return result, False
        elif query_type == "conversation":
            if query_lower in GREETINGS:
                result = GREETING_RESPONSE
                cache_handler.setex(cache_key, 600, result)
                return result, False
            elif "ì˜¤ëŠ˜ë‚ ì§œ" in query_lower or "í˜„ì¬ë‚ ì§œ" in query_lower or "ê¸ˆì¼ë‚ ì§œ" in query_lower:
                result = get_kst_time()
                cache_handler.setex(cache_key, 600, result)
                return result, False
            else:
                response, is_stream = asyncio.run(get_conversational_response(query, messages))
                return response, is_stream
        else:
            result = "ì•„ì§ ì§€ì›í•˜ì§€ ì•ŠëŠ” ê¸°ëŠ¥ì´ì—ìš”. ğŸ˜…"
            cache_handler.setex(cache_key, 600, result)
            return result, False

def show_chat_dashboard():
    st.title("Chat with AI ğŸ¤–")
    
    # ë„ì›€ë§ ë²„íŠ¼
    if st.button("ë„ì›€ë§ â„¹ï¸"):
        st.info(
            "ì±—ë´‡ê³¼ ë” ì‰½ê²Œ ëŒ€í™”í•˜ëŠ” ë°©ë²•ì´ì—ìš”! ğŸ‘‡:\n"
            "1. **ë‚ ì”¨** â˜€ï¸: '[ë„ì‹œëª…] ë‚ ì”¨' (ì˜ˆ: ì„œìš¸ ë‚ ì”¨)\n"
            "2. **ì‹œê°„/ë‚ ì§œ** â±ï¸: '[ë„ì‹œëª…] ì‹œê°„' ë˜ëŠ” 'ì˜¤ëŠ˜ ë‚ ì§œ' (ì˜ˆ: ë¶€ì‚° ì‹œê°„, ê¸ˆì¼ ë‚ ì§œ)\n"
            "3. **ë¦¬ê·¸ìˆœìœ„** âš½: '[ë¦¬ê·¸ ì´ë¦„] ë¦¬ê·¸ ìˆœìœ„ ë˜ëŠ” ë¦¬ê·¸ë“ì ìˆœìœ„' (ì˜ˆ: EPL ë¦¬ê·¸ìˆœìœ„, EPL ë¦¬ê·¸ë“ì ìˆœìœ„)\n"
            "   - ì§€ì› ë¦¬ê·¸: EPL, LaLiga, Bundesliga, Serie A, Ligue 1, ChampionsLeague\n"
            "4. **ì•½í’ˆê²€ìƒ‰** ğŸ’Š: 'ì•½í’ˆê²€ìƒ‰ [ì•½ ì´ë¦„]' (ì˜ˆ: ì•½í’ˆê²€ìƒ‰ ê²Œë³´ë¦°)\n"
            "5. **ê³µí•™ë…¼ë¬¸** ğŸ“š: 'ê³µí•™ë…¼ë¬¸ [í‚¤ì›Œë“œ]' (ì˜ˆ: ê³µí•™ë…¼ë¬¸ Multimodal AI)\n"
            "6. **ì˜í•™ë…¼ë¬¸** ğŸ©º: 'ì˜í•™ë…¼ë¬¸ [í‚¤ì›Œë“œ]' (ì˜ˆ: ì˜í•™ë…¼ë¬¸ cancer therapy)\n"
            "7. **ê²€ìƒ‰** ğŸŒ: 'ê²€ìƒ‰ í‚¤ì›Œë“œ' (ì˜ˆ: ê²€ìƒ‰ ìµœê·¼ ì „ì‹œíšŒ ì¶”ì²œ)\n"
            "8. **MBTI** âœ¨: 'MBTI' ë˜ëŠ” 'MBTI ìœ í˜•' (ì˜ˆ: MBTI ê²€ì‚¬, INTJ ì„¤ëª…)\n"
            "9. **ë‹¤ì¤‘ì§€ëŠ¥** ğŸ‰: 'ë‹¤ì¤‘ì§€ëŠ¥' ë˜ëŠ” 'ë‹¤ì¤‘ì§€ëŠ¥ ìœ í˜•' (ì˜ˆ: ë‹¤ì¤‘ì§€ëŠ¥ ê²€ì‚¬, ì–¸ì–´ì§€ëŠ¥ ì§ì—…)\n\n"
            "ê¶ê¸ˆí•œ ì  ìˆìœ¼ë©´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”! ğŸ˜Š"
        )
    
    # ìµœê·¼ ë©”ì‹œì§€ í‘œì‹œ
    for msg in st.session_state.messages[-10:]:
        with st.chat_message(msg['role']):
            if isinstance(msg['content'], dict) and "table" in msg['content']:
                st.markdown(f"### {msg['content']['header']}")
                st.dataframe(pd.DataFrame(msg['content']['table']), use_container_width=True, hide_index=True)
                st.markdown(msg['content']['footer'])
            else:
                st.markdown(msg['content'], unsafe_allow_html=True)
    
    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    if user_prompt := st.chat_input("ì§ˆë¬¸í•´ ì£¼ì„¸ìš”!"):
        st.session_state.messages.append({"role": "user", "content": user_prompt})
        
        with st.chat_message("user"):
            st.markdown(user_prompt)
        
        with st.chat_message("assistant"):
            # Spinner ì ìš©
            with st.spinner("ì‘ë‹µì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤... â³"):
                try:
                    start_time = time.time()
                    response, is_stream = process_query(user_prompt, st.session_state.messages)
                    time_taken = round(time.time() - start_time, 2)
                    
                    # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
                    if is_stream:
                        chatbot_response = ""
                        message_placeholder = st.empty()
                        for chunk in response:
                            if hasattr(chunk, 'choices') and len(chunk.choices) > 0 and hasattr(chunk.choices[0], 'delta') and hasattr(chunk.choices[0].delta, 'content'):
                                content = chunk.choices[0].delta.content
                                if content is not None:
                                    chatbot_response += content
                                    message_placeholder.markdown(chatbot_response + "â–Œ")
                            else:
                                logger.warning(f"ì˜ˆìƒì¹˜ ëª»í•œ ì²­í¬ êµ¬ì¡°: {chunk}")
                        message_placeholder.markdown(chatbot_response)
                        st.session_state.messages.append({"role": "assistant", "content": chatbot_response})
                    else:
                        # ì •ì  ì‘ë‹µ ì²˜ë¦¬
                        if isinstance(response, dict) and "table" in response:
                            st.markdown(f"### {response['header']}")
                            st.dataframe(response['table'], use_container_width=True, hide_index=True)
                            st.markdown(response['footer'])
                        else:
                            st.markdown(response, unsafe_allow_html=True)
                        st.session_state.messages.append({"role": "assistant", "content": response})
                    
                    # ëŒ€í™” ê¸°ë¡ ë¹„ë™ê¸° ì €ì¥
                    async_save_chat_history(st.session_state.user_id, st.session_state.session_id, user_prompt, response, time_taken)
                
                except Exception as e:
                    error_msg = f"ì‘ë‹µì„ ì¤€ë¹„í•˜ë‹¤ ë¬¸ì œê°€ ìƒê²¼ì–´ìš”: {str(e)} ğŸ˜“"
                    logger.error(f"ëŒ€í™” ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)
                    st.markdown(error_msg, unsafe_allow_html=True)
                    st.session_state.messages.append({"role": "assistant", "content": error_msg})



def show_login_page():
    st.title("ë¡œê·¸ì¸ ğŸ¤—")
    with st.form("login_form"):
        nickname = st.text_input("ë‹‰ë„¤ì„", placeholder="ì˜ˆ: í›„ì•ˆ")
        submit_button = st.form_submit_button("ì‹œì‘í•˜ê¸° ğŸš€")
        
        if submit_button and nickname:
            try:
                user_id, is_existing = create_or_get_user(nickname)
                st.session_state.user_id = user_id
                st.session_state.is_logged_in = True
                st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ë„ì›€ë§ë„ í™œìš©í•´ ë³´ì„¸ìš” ğŸ˜Š"}]
                st.session_state.session_id = str(uuid.uuid4())
                st.toast(f"í™˜ì˜í•©ë‹ˆë‹¤, {nickname}ë‹˜! ğŸ‰")
                time.sleep(1)
                st.rerun()
            except Exception:
                st.toast("ë¡œê·¸ì¸ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.", icon="âŒ")

def main():
    init_session_state()
    if not st.session_state.is_logged_in:
        show_login_page()
    else:
        show_chat_dashboard()

if __name__ == "__main__":
    main()
