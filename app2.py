# set lib
from config.imports import *
from config.env import *

# set logger
logging.basicConfig(level=logging.WARNING if os.getenv("ENV") == "production" else logging.INFO)
logger = logging.getLogger("HybridChat")
logging.getLogger("streamlit").setLevel(logging.WARNING)
logging.getLogger("httpx").setLevel(logging.WARNING)

# set cach
cache = Cache("cache_directory")

class MemoryCache:
    def __init__(self):
        self.cache = {}
        self.expiry = {}
    
    def get(self, key):
        if key in self.cache and time.time() < self.expiry[key]:
            return self.cache[key]
        return cache.get(key)
    
    def setex(self, key, ttl, value):
        self.cache[key] = value
        self.expiry[key] = time.time() + ttl
        cache.set(key, value, expire=ttl)

cache_handler = MemoryCache()

# ë‚ ì§œ ì¼ê´„ì  ìˆ˜ì • 
def format_date(fordate):
    if fordate == 'No date':
        return 'ë‚ ì§œ ì—†ìŒ'
    try:
        date_obj = datetime.strptime(fordate, '%Y %b %d')
        return date_obj.strftime('%Y.%m.%d')
    except ValueError:
        return fordate

# JSON íŒŒì¼ì—ì„œ MBTI ë° ë‹¤ì¤‘ì§€ëŠ¥ ë°ì´í„° ë¡œë“œ (ìºì‹± ì ìš©)
def load_personality_data():
    cache_key = "personality_data"
    cached_data = cache_handler.get(cache_key)
    if cached_data:
        return cached_data
    
    try:
        with open("config/personality_multi_data.json", "r", encoding="utf-8") as f:
            data = json.load(f)
        cache_handler.setex(cache_key, 86400, data)  # 24ì‹œê°„ ìºì‹±
        return data
    except FileNotFoundError:
        logger.error("personality_multi_data.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        raise
    except json.JSONDecodeError:
        logger.error("personality_multi_data.json íŒŒì¼ì˜ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤.")
        raise

# ë°ì´í„° ë¡œë“œ
personality_data = load_personality_data()
mbti_descriptions = personality_data["mbti_descriptions"]
multi_iq_descriptions = personality_data["multi_iq_descriptions"]
mbti_full_description = personality_data["mbti_full_description"]
multi_iq_full_description = personality_data["multi_iq_full_description"]

# WeatherAPI í´ë˜ìŠ¤
class WeatherAPI:
    def __init__(self, cache_ttl=600):
        self.cache = cache_handler
        self.cache_ttl = cache_ttl

    def fetch_weather(self, url, params):
        session = requests.Session()
        retry_strategy = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("https://", adapter)
        try:
            response = session.get(url, params=params, timeout=3)
            response.raise_for_status()
            return response.json()
        except:
            return self.cache.get(f"weather:{params.get('q', '')}") or "ë‚ ì”¨ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    @lru_cache(maxsize=100)
    def get_city_info(self, city_name):
        cache_key = f"city_info:{city_name}"
        cached = self.cache.get(cache_key)
        if cached:
            return cached
        url = "http://api.openweathermap.org/geo/1.0/direct"
        params = {'q': city_name, 'limit': 1, 'appid': WEATHER_API_KEY}
        data = self.fetch_weather(url, params)
        if data and isinstance(data, list) and len(data) > 0:
            city_info = {"name": data[0]["name"], "lat": data[0]["lat"], "lon": data[0]["lon"]}
            self.cache.setex(cache_key, 86400, city_info)
            return city_info
        return None

    def get_city_weather(self, city_name):
        cache_key = f"weather:{city_name}"
        cached_data = self.cache.get(cache_key)
        if cached_data:
            return cached_data
        
        city_info = self.get_city_info(city_name)
        if not city_info:
            return f"'{city_name}'ì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        url = "https://api.openweathermap.org/data/2.5/weather"
        params = {'lat': city_info["lat"], 'lon': city_info["lon"], 'appid': WEATHER_API_KEY, 'units': 'metric', 'lang': 'kr'}
        data = self.fetch_weather(url, params)
        if isinstance(data, str):
            return data
        weather_emojis = {'Clear': 'â˜€ï¸', 'Clouds': 'â˜ï¸', 'Rain': 'ğŸŒ§ï¸', 'Snow': 'â„ï¸', 'Thunderstorm': 'â›ˆï¸', 'Drizzle': 'ğŸŒ¦ï¸', 'Mist': 'ğŸŒ«ï¸'}
        weather_emoji = weather_emojis.get(data['weather'][0]['main'], 'ğŸŒ¤ï¸')
        result = (
            f"í˜„ì¬ {data['name']}, {data['sys']['country']} ë‚ ì”¨ {weather_emoji}\n"
            f"ë‚ ì”¨: {data['weather'][0]['description']}\n"
            f"ì˜¨ë„: {data['main']['temp']}Â°C\n"
            f"ì²´ê°: {data['main']['feels_like']}Â°C\n"
            f"ìŠµë„: {data['main']['humidity']}%\n"
            f"í’ì†: {data['wind']['speed']}m/s\n"
            f"ë” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"
        )
        # ìºì‹œ TTL ê°’ í™•ì¸ ë° ì¡°ì •
        self.cache.setex(cache_key, self.cache_ttl, result)
        return result
    def get_forecast_by_day(self, city_name, days_from_today=1):
        cache_key = f"forecast:{city_name}:{days_from_today}"
        cached_data = self.cache.get(cache_key)
        if cached_data:
            return cached_data
        
        city_info = self.get_city_info(city_name)
        if not city_info:
            return f"'{city_name}'ì˜ ë‚ ì”¨ ì˜ˆë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        url = "https://api.openweathermap.org/data/2.5/forecast"
        params = {'lat': city_info["lat"], 'lon': city_info["lon"], 'appid': WEATHER_API_KEY, 'units': 'metric', 'lang': 'kr'}
        data = self.fetch_weather(url, params)
        if isinstance(data, str):
            return data
        target_date = (datetime.now() + timedelta(days=days_from_today)).strftime('%Y-%m-%d')
        forecast_text = f"{city_info['name']}ì˜ {target_date} ë‚ ì”¨ ì˜ˆë³´ ğŸŒ¤ï¸\n\n"
        weather_emojis = {'Clear': 'â˜€ï¸', 'Clouds': 'â˜ï¸', 'Rain': 'ğŸŒ§ï¸', 'Snow': 'â„ï¸', 'Thunderstorm': 'â›ˆï¸', 'Drizzle': 'ğŸŒ¦ï¸', 'Mist': 'ğŸŒ«ï¸'}
        
        found = False
        for forecast in data['list']:
            dt = datetime.fromtimestamp(forecast['dt']).strftime('%Y-%m-%d')
            if dt == target_date:
                found = True
                time_only = datetime.fromtimestamp(forecast['dt']).strftime('%H:%M')
                weather_emoji = weather_emojis.get(forecast['weather'][0]['main'], 'ğŸŒ¤ï¸')
                forecast_text += (
                    f"â° {time_only} {forecast['weather'][0]['description']} {weather_emoji} "
                    f"{forecast['main']['temp']}Â°C ğŸ’§{forecast['main']['humidity']}% ğŸŒ¬ï¸{forecast['wind']['speed']}m/s\n\n"
                )
        
        result = forecast_text + "ë” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š" if found else f"'{city_name}'ì˜ {target_date} ë‚ ì”¨ ì˜ˆë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        # ìºì‹œ TTL ê°’ í™•ì¸ ë° ì¡°ì •
        self.cache.setex(cache_key, self.cache_ttl, result)
        return result

    def get_weekly_forecast(self, city_name):
        cache_key = f"weekly_forecast:{city_name}"
        cached_data = self.cache.get(cache_key)
        if cached_data:
            return cached_data
        
        city_info = self.get_city_info(city_name)
        if not city_info:
            return f"'{city_name}'ì˜ ì£¼ê°„ ì˜ˆë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        url = "https://api.openweathermap.org/data/2.5/forecast"
        params = {'lat': city_info["lat"], 'lon': city_info["lon"], 'appid': WEATHER_API_KEY, 'units': 'metric', 'lang': 'kr'}
        data = self.fetch_weather(url, params)
        if isinstance(data, str):
            return data
        today = datetime.now().date()
        week_end = today + timedelta(days=6)
        daily_forecast = {}
        weekdays_kr = ["ì›”ìš”ì¼", "í™”ìš”ì¼", "ìˆ˜ìš”ì¼", "ëª©ìš”ì¼", "ê¸ˆìš”ì¼", "í† ìš”ì¼", "ì¼ìš”ì¼"]
        today_weekday = today.weekday()
        
        for forecast in data['list']:
            dt = datetime.fromtimestamp(forecast['dt']).date()
            if today <= dt <= week_end:
                dt_str = dt.strftime('%Y-%m-%d')
                if dt_str not in daily_forecast:
                    weekday_idx = (today_weekday + (dt - today).days) % 7
                    daily_forecast[dt_str] = {
                        'weekday': weekdays_kr[weekday_idx],
                        'temp_min': forecast['main']['temp_min'],
                        'temp_max': forecast['main']['temp_max'],
                        'weather': forecast['weather'][0]['description']
                    }
                else:
                    daily_forecast[dt_str]['temp_min'] = min(daily_forecast[dt_str]['temp_min'], forecast['main']['temp_min'])
                    daily_forecast[dt_str]['temp_max'] = max(daily_forecast[dt_str]['temp_max'], forecast['main']['temp_max'])
        
        today_str = today.strftime('%Y-%m-%d')
        today_weekday_str = weekdays_kr[today_weekday]
        forecast_text = f"{today_str}({today_weekday_str}) ê¸°ì¤€ {city_info['name']}ì˜ ì£¼ê°„ ë‚ ì”¨ ì˜ˆë³´ ğŸŒ¤ï¸\n"
        weather_emojis = {'Clear': 'â˜€ï¸', 'Clouds': 'â˜ï¸', 'Rain': 'ğŸŒ§ï¸', 'Snow': 'â„ï¸', 'Thunderstorm': 'â›ˆï¸', 'Drizzle': 'ğŸŒ¦ï¸', 'Mist': 'ğŸŒ«ï¸'}
        
        for date, info in daily_forecast.items():
            weather_emoji = weather_emojis.get(info['weather'].split()[0], 'ğŸŒ¤ï¸')
            forecast_text += (
                f"\n{info['weekday']}: {info['weather']} {weather_emoji} "
                f"ìµœì € {info['temp_min']}Â°C ìµœê³  {info['temp_max']}Â°C\n\n"
            )
        
        result = forecast_text + "\në” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"
        # ìºì‹œ TTL ê°’ í™•ì¸ ë° ì¡°ì •
        self.cache.setex(cache_key, self.cache_ttl, result)
        return result

# FootballAPI í´ë˜ìŠ¤
class FootballAPI:
    def __init__(self, api_key, cache_ttl=600):
        self.api_key = api_key
        self.base_url = "https://api.football-data.org/v4/competitions"
        self.cache = cache_handler
        self.cache_ttl = cache_ttl

    def fetch_league_standings(self, league_code, league_name):
        cache_key = f"league_standings:{league_code}"
        cached_data = self.cache.get(cache_key)
        if cached_data is not None:
            return cached_data

        url = f"{self.base_url}/{league_code}/standings"
        headers = {'X-Auth-Token': self.api_key}
        
        try:
            time.sleep(1)
            response = requests.get(url, headers=headers, timeout=2)
            response.raise_for_status()
            data = response.json()
            
            standings = data['standings'][0]['table'] if league_code not in ["CL"] else data['standings']
            if league_code in ["CL"]:
                standings_data = []
                for group in standings:
                    for team in group['table']:
                        standings_data.append({
                            'ìˆœìœ„': team['position'],
                            'ê·¸ë£¹': group['group'],
                            'íŒ€': team['team']['name'],
                            'ê²½ê¸°': team['playedGames'],
                            'ìŠ¹': team['won'],
                            'ë¬´': team['draw'],
                            'íŒ¨': team['lost'],
                            'ë“ì ': team['goalsFor'],
                            'ì‹¤ì ': team['goalsAgainst'],
                            'ë“ì‹¤ì°¨': team['goalsFor'] - team['goalsAgainst'],
                            'í¬ì¸íŠ¸': team['points']
                        })
                df = pd.DataFrame(standings_data)
            else:
                df = pd.DataFrame([
                    {
                        'ìˆœìœ„': team['position'],
                        'íŒ€': team['team']['name'],
                        'ê²½ê¸°': team['playedGames'],
                        'ìŠ¹': team['won'],
                        'ë¬´': team['draw'],
                        'íŒ¨': team['lost'],
                        'ë“ì ': team['goalsFor'],
                        'ì‹¤ì ': team['goalsAgainst'],
                        'ë“ì‹¤ì°¨': team['goalsFor'] - team['goalsAgainst'],
                        'í¬ì¸íŠ¸': team['points']
                    } for team in standings
                ])
            
            result = {"league_name": league_name, "data": df}
            self.cache.setex(cache_key, self.cache_ttl, result)
            return result
        
        except requests.exceptions.RequestException as e:
            return {"league_name": league_name, "error": f"{league_name} ë¦¬ê·¸ ìˆœìœ„ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)} ğŸ˜“"}

    def fetch_league_scorers(self, league_code, league_name):
        cache_key = f"league_scorers:{league_code}"
        cached_data = self.cache.get(cache_key)
        if cached_data is not None:
            return cached_data

        url = f"{self.base_url}/{league_code}/scorers"
        headers = {'X-Auth-Token': self.api_key}
        
        try:
            time.sleep(1)
            response = requests.get(url, headers=headers, timeout=2)
            response.raise_for_status()
            data = response.json()
            
            scorers = [{"ìˆœìœ„": i+1, "ì„ ìˆ˜": s['player']['name'], "íŒ€": s['team']['name'], "ë“ì ": s['goals']} 
                       for i, s in enumerate(data['scorers'][:10])]
            df = pd.DataFrame(scorers)
            result = {"league_name": league_name, "data": df}
            self.cache.setex(cache_key, self.cache_ttl, result)
            return result
        
        except requests.exceptions.RequestException as e:
            return {"league_name": league_name, "error": f"{league_name} ë¦¬ê·¸ ë“ì ìˆœìœ„ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)} ğŸ˜“"}

    def fetch_championsleague_knockout_matches(self):
        """
        ì±”í”¼ì–¸ìŠ¤ë¦¬ê·¸ì˜ knockout(í† ë„ˆë¨¼íŠ¸) ìŠ¤í…Œì´ì§€ ê²½ê¸° ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        url = f"{self.base_url}/CL/matches"
        headers = {'X-Auth-Token': self.api_key}
        KNOCKOUT_STAGES = [
            "LAST_16", "QUARTER_FINALS", "SEMI_FINALS", "FINAL", "THIRD_PLACE"
        ]
        try:
            response = requests.get(url, headers=headers, timeout=3)
            response.raise_for_status()
            data = response.json()
            
            # ë””ë²„ê¹…ìš© - API ì‘ë‹µ í™•ì¸
            # print(json.dumps(data['matches'][0], indent=2, ensure_ascii=False))
            
            knockout_matches = [
                m for m in data['matches']
                if m.get('stage') in KNOCKOUT_STAGES
            ]
            results = []
            for m in knockout_matches:
                home = m.get('homeTeam', {}).get('name', 'ë¯¸ì •')
                away = m.get('awayTeam', {}).get('name', 'ë¯¸ì •')
                
                # ìŠ¤ì½”ì–´ í™•ì¸ (fullTime â†’ halfTime â†’ extraTime â†’ penalties ìˆœìœ¼ë¡œ í™•ì¸)
                score_home = m.get('score', {}).get('fullTime', {}).get('home')
                score_away = m.get('score', {}).get('fullTime', {}).get('away')
                
                if score_home is None or score_away is None:
                    score_home = m.get('score', {}).get('halfTime', {}).get('home')
                    score_away = m.get('score', {}).get('halfTime', {}).get('away')
                
                if score_home is None or score_away is None:
                    score_home = m.get('score', {}).get('extraTime', {}).get('home')
                    score_away = m.get('score', {}).get('extraTime', {}).get('away')
                
                if score_home is None or score_away is None:
                    score_home = m.get('score', {}).get('penalties', {}).get('home')
                    score_away = m.get('score', {}).get('penalties', {}).get('away')
            
                # ê²½ê¸° ìƒíƒœ í™•ì¸
                match_status = m.get('status', '')
                
                # ìŠ¤ì½”ì–´ ë¬¸ìì—´ ìƒì„±
                if match_status == 'FINISHED':
                    score_str = f"{score_home if score_home is not None else 0} : {score_away if score_away is not None else 0}"
                elif match_status == 'SCHEDULED':
                    score_str = "ì˜ˆì •ëœ ê²½ê¸°"
                else:
                    score_str = f"{score_home if score_home is not None else '-'} : {score_away if score_away is not None else '-'}"
                
                results.append({
                    "ë¼ìš´ë“œ": m.get('stage', 'ë¯¸ì •'),
                    "ë‚ ì§œ": m.get('utcDate', '')[:10] if m.get('utcDate') else 'ë¯¸ì •',
                    "í™ˆíŒ€": home,
                    "ì›ì •íŒ€": away,
                    "ìŠ¤ì½”ì–´": score_str,
                    "ìƒíƒœ": match_status
                })
            return results
        except Exception as e:
            return f"ì±”í”¼ì–¸ìŠ¤ë¦¬ê·¸ í† ë„ˆë¨¼íŠ¸ ê²½ê¸° ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜: {str(e)}"
# ìµœì ì˜ í”„ë¡œë°”ì´ë” ì„ íƒ í•¨ìˆ˜
def select_best_provider_with_priority():
    """
    ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ê°€ì¥ ì í•©í•œ í”„ë¡œë°”ì´ë”ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.
    """
    providers = ["GeekGpt", "Liaobots", "Raycast", "Phind"]  # ìš°ì„ ìˆœìœ„ ì„¤ì •
    for provider in providers:
        try:
            client = Client(include_providers=[provider])
            # í…ŒìŠ¤íŠ¸ ìš”ì²­ (ì±—ë´‡ì˜ ì—­í• ì— ë§ëŠ” ë©”ì‹œì§€ ì‚¬ìš©)
            client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì±—ë´‡ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì ì ˆíˆ ì‘ë‹µí•˜ì„¸ìš”."},
                ]
            )
            logger.info(f"ì„ íƒëœ í”„ë¡œë°”ì´ë”: {provider}")
            return client
        except Exception as e:
            logger.warning(f"{provider} í”„ë¡œë°”ì´ë”ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}")
    raise RuntimeError("ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡œë°”ì´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")

def select_random_available_provider():
    providers = ["GeekGpt", "Liaobots", "Raycast"]
    random.shuffle(providers)  # ëœë¤ ìˆœì„œë¡œ ì„ê¸°
    for provider in providers:
        try:
            client = Client(include_providers=[provider])
            # ì‹¤ì œë¡œ ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìš”ì²­
            client.chat.completions.create(
                model="gpt-4",
                messages=[{"role": "system", "content": "í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ì…ë‹ˆë‹¤."}]
            )
            logger.info(f"ì„ íƒëœ í”„ë¡œë°”ì´ë”(ëœë¤): {provider}")
            return client, provider
        except Exception as e:
            logger.warning(f"{provider} í”„ë¡œë°”ì´ë”ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}")
    raise RuntimeError("ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡œë°”ì´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ì´ˆê¸°í™”
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
# ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰í•˜ë„ë¡ ìˆ˜ì •
# ì „ì—­ ë³€ìˆ˜ë¡œ ì„ ì–¸ëœ client ê°ì²´ë¥¼ ì´ˆê¸°í™”í•  ë•Œë§Œ ì‚¬ìš©
# client = select_best_provider_with_priority()
weather_api = WeatherAPI()
football_api = FootballAPI(api_key=SPORTS_API_KEY)
naver_request_count = 0
NAVER_DAILY_LIMIT = 25000
st.set_page_config(page_title="AI ì±—ë´‡", page_icon="ğŸ¤–")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
def init_session_state():
    if "is_logged_in" not in st.session_state:
        st.session_state.is_logged_in = False
    if "user_id" not in st.session_state:
        st.session_state.user_id = None
    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?ğŸ˜Š"}]
    if "session_id" not in st.session_state:
        st.session_state.session_id = str(uuid.uuid4())
    if "client" not in st.session_state or "provider_name" not in st.session_state:
        client, provider_name = select_random_available_provider()
        st.session_state.client = client
        st.session_state.provider_name = provider_name

# ë„ì‹œ ë° ì‹œê°„ ì¶”ì¶œ
CITY_PATTERNS = [
    re.compile(r'(?:ì˜¤ëŠ˜|ë‚´ì¼|ëª¨ë ˆ|ì´ë²ˆ ì£¼|ì£¼ê°„)?\s*([ê°€-í£a-zA-Z\s]{2,20}(?:ì‹œ|êµ°|city)?)ì˜?\s*ë‚ ì”¨', re.IGNORECASE),
    re.compile(r'(?:ì˜¤ëŠ˜|ë‚´ì¼|ëª¨ë ˆ|ì´ë²ˆ ì£¼|ì£¼ê°„)?\s*([ê°€-í£a-zA-Z\s]{2,20}(?:ì‹œ|êµ°|city)?)\s*ë‚ ì”¨', re.IGNORECASE),
]
def extract_city_from_query(query):
    for pattern in CITY_PATTERNS:
        match = pattern.search(query)
        if match:
            city = match.group(1).strip()
            if city not in ["ì˜¤ëŠ˜", "ë‚´ì¼", "ëª¨ë ˆ", "ì´ë²ˆ ì£¼", "ì£¼ê°„", "í˜„ì¬"]:
                return city
    return "ì„œìš¸"

TIME_CITY_PATTERNS = [
    re.compile(r'([ê°€-í£a-zA-Z]{2,20}(?:ì‹œ|êµ°)?)ì˜?\s*ì‹œê°„'),
    re.compile(r'([ê°€-í£a-zA-Z]{2,20}(?:ì‹œ|êµ°)?)\s*ì‹œê°„'),
]
def extract_city_from_time_query(query):
    for pattern in TIME_CITY_PATTERNS:
        match = pattern.search(query)
        if match:
            city = match.group(1).strip()
            if city != "í˜„ì¬":
                return city
    return "ì„œìš¸"

LEAGUE_MAPPING = {
    "epl": {"name": "í”„ë¦¬ë¯¸ì–´ë¦¬ê·¸ (ì˜êµ­)", "code": "PL"},
    "laliga": {"name": "ë¼ë¦¬ê°€ (ìŠ¤í˜ì¸)", "code": "PD"},
    "bundesliga": {"name": "ë¶„ë°ìŠ¤ë¦¬ê°€ (ë…ì¼)", "code": "BL1"},
    "seriea": {"name": "ì„¸ë¦¬ì— A (ì´íƒˆë¦¬ì•„)", "code": "SA"},
    "ligue1": {"name": "ë¦¬ê·¸ 1 (í”„ë‘ìŠ¤)", "code": "FL1"},
    "championsleague": {"name": "ì±”í”¼ì–¸ìŠ¤ ë¦¬ê·¸", "code": "CL"}
}

def extract_league_from_query(query):
    query_lower = query.lower().replace(" ", "")
    league_keywords = {
        "epl": ["epl", "í”„ë¦¬ë¯¸ì–´ë¦¬ê·¸"],
        "laliga": ["laliga", "ë¼ë¦¬ê°€"],
        "bundesliga": ["bundesliga", "ë¶„ë°ìŠ¤ë¦¬ê°€"],
        "seriea": ["seriea", "ì„¸ë¦¬ì—a"],
        "ligue1": ["ligue1", "ë¦¬ê·¸1"],
        "championsleague": ["championsleague", "ì±”í”¼ì–¸ìŠ¤ë¦¬ê·¸", "ucl"]
    }
    for league_key, keywords in league_keywords.items():
        if any(keyword in query_lower for keyword in keywords):
            return league_key
    return None

def get_kst_time():
    kst_timezone = pytz.timezone("Asia/Seoul")
    kst_time = datetime.now(kst_timezone)
    return f"ëŒ€í•œë¯¼êµ­ ê¸°ì¤€ : {kst_time.strftime('%Yë…„ %mì›” %dì¼ %p %I:%M')}ì…ë‹ˆë‹¤. â°\n\n ë” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"

def get_time_by_city(city_name="ì„œìš¸"):
    city_info = weather_api.get_city_info(city_name)
    if not city_info:
        return f"'{city_name}'ì˜ ì‹œê°„ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    tf = TimezoneFinder()
    timezone_str = tf.timezone_at(lng=city_info["lon"], lat=city_info["lat"]) or "Asia/Seoul"
    timezone = pytz.timezone(timezone_str)
    city_time = datetime.now(timezone)
    return f"í˜„ì¬ {city_name} ì‹œê°„: {city_time.strftime('%Yë…„ %mì›” %dì¼ %p %I:%M')}ì…ë‹ˆë‹¤. â°\n\n ë” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"

# ì‚¬ìš©ì ë° ì±„íŒ… ê¸°ë¡ ê´€ë¦¬
def create_or_get_user(nickname):
    user = supabase.table("users").select("*").eq("nickname", nickname).execute()
    if user.data:
        return user.data[0]["id"], True
    new_user = supabase.table("users").insert({"nickname": nickname, "created_at": datetime.now().isoformat()}).execute()
    return new_user.data[0]["id"], False

def save_chat_history(user_id, session_id, question, answer, time_taken):
    if isinstance(answer, dict) and "table" in answer and isinstance(answer["table"], pd.DataFrame):
        answer_to_save = {
            "header": answer["header"],
            "table": answer["table"].to_dict(orient="records"),
            "footer": answer["footer"]
        }
    else:
        answer_to_save = answer
    
    supabase.table("chat_history").insert({
        "user_id": user_id,
        "session_id": session_id,
        "question": question,
        "answer": answer_to_save,
        "time_taken": time_taken,
        "created_at": datetime.now().isoformat()
    }).execute()

def async_save_chat_history(user_id, session_id, question, answer, time_taken):
    threading.Thread(target=save_chat_history, args=(user_id, session_id, question, answer, time_taken)).start()

# ì˜ì•½í’ˆ ê²€ìƒ‰
def get_drug_info(drug_query):
    drug_name = drug_query.replace("ì•½í’ˆê²€ìƒ‰", "").strip()
    cache_key = f"drug:{drug_name}"
    cached = cache_handler.get(cache_key)
    if cached:
        return cached
    
    url = 'http://apis.data.go.kr/1471000/DrbEasyDrugInfoService/getDrbEasyDrugList'
    params = {'serviceKey': DRUG_API_KEY, 'pageNo': '1', 'numOfRows': '1', 'itemName': urllib.parse.quote(drug_name), 'type': 'json'}
    try:
        response = requests.get(url, params=params, timeout=3)
        response.raise_for_status()
        data = response.json()
        if 'body' in data and 'items' in data['body'] and data['body']['items']:
            item = data['body']['items'][0]
            efcy = item.get('efcyQesitm', 'ì •ë³´ ì—†ìŒ')[:150] + ("..." if len(item.get('efcyQesitm', '')) > 150 else "")
            use_method = item.get('useMethodQesitm', 'ì •ë³´ ì—†ìŒ')[:150] + ("..." if len(item.get('useMethodQesitm', '')) > 150 else "")
            atpn = item.get('atpnQesitm', 'ì •ë³´ ì—†ìŒ')[:150] + ("..." if len(item.get('atpnQesitm', '')) > 150 else "")
            
            result = (
                f"ğŸ’Š **ì˜ì•½í’ˆ ì •ë³´** ğŸ’Š\n\n"
                f"âœ… **ì•½í’ˆëª…**: {item.get('itemName', 'ì •ë³´ ì—†ìŒ')}\n\n"
                f"âœ… **ì œì¡°ì‚¬**: {item.get('entpName', 'ì •ë³´ ì—†ìŒ')}\n\n"
                f"âœ… **íš¨ëŠ¥**: {efcy}\n\n"
                f"âœ… **ìš©ë²•ìš©ëŸ‰**: {use_method}\n\n"
                f"âœ… **ì£¼ì˜ì‚¬í•­**: {atpn}\n\n"
                f"ë” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"
            )
            cache_handler.setex(cache_key, 86400, result)
            return result
        return f"'{drug_name}'ì˜ ê³µì‹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    except Exception as e:
        logger.error(f"ì•½í’ˆ API ì˜¤ë¥˜: {str(e)}")
        return f"'{drug_name}'ì˜ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ğŸ˜“"

# Naver API ê²€ìƒ‰ (ì›¹ ê²€ìƒ‰)
def get_naver_api_results(query):
    global naver_request_count
    cache_key = f"naver:{query}"
    cached = cache_handler.get(cache_key)
    if cached:
        return cached
    
    if naver_request_count >= NAVER_DAILY_LIMIT:
        return "ê²€ìƒ‰ í•œë„ ì´ˆê³¼ë¡œ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ğŸ˜“"
    enc_text = urllib.parse.quote(query)
    url = f"https://openapi.naver.com/v1/search/webkr?query={enc_text}&display=5&sort=date"
    request = urllib.request.Request(url)
    request.add_header("X-Naver-Client-Id", NAVER_CLIENT_ID)
    request.add_header("X-Naver-Client-Secret", NAVER_CLIENT_SECRET)
    try:
        response = urllib.request.urlopen(request, timeout=3)
        naver_request_count += 1
        if response.getcode() == 200:
            data = json.loads(response.read().decode('utf-8'))
            results = data.get('items', [])
            if not results:
                return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ğŸ˜“"
            
            response_text = "ğŸŒ **ì›¹ ê²€ìƒ‰ ê²°ê³¼** \n\n"
            response_text += "\n\n".join(
                [f"**ê²°ê³¼ {i}**\n\nğŸ“„ **ì œëª©**: {re.sub(r'<b>|</b>', '', item['title'])}\n\nğŸ“ **ë‚´ìš©**: {re.sub(r'<b>|</b>', '', item.get('description', 'ë‚´ìš© ì—†ìŒ'))[:100]}...\n\nğŸ”— **ë§í¬**: {item.get('link', '')}"
                 for i, item in enumerate(results, 1)]
            ) + "\n\në” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"
            cache_handler.setex(cache_key, 3600, response_text)
            return response_text
    except Exception as e:
        logger.error(f"Naver API ì˜¤ë¥˜: {str(e)}")
        return "ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ğŸ˜“"

# ArXiv ë…¼ë¬¸ ê²€ìƒ‰
def fetch_arxiv_paper(paper):
    return {
        "title": paper.title,
        "authors": ", ".join(str(a) for a in paper.authors),
        "summary": paper.summary[:200],
        "entry_id": paper.entry_id,
        "pdf_url": paper.pdf_url,
        "published": paper.published.strftime('%Y-%m-%d')
    }

def get_arxiv_papers(query, max_results=3):
    cache_key = f"arxiv:{query}:{max_results}"
    cached = cache_handler.get(cache_key)
    if cached:
        return cached
    search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.SubmittedDate)
    with ThreadPoolExecutor() as executor:
        results = list(executor.map(fetch_arxiv_paper, search.results()))
    if not results:
        return "í•´ë‹¹ í‚¤ì›Œë“œë¡œ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    response = "ğŸ“š **Arxiv ë…¼ë¬¸ ê²€ìƒ‰ ê²°ê³¼** ğŸ“š\n\n"
    response += "\n\n".join(
        [f"**ë…¼ë¬¸ {i}**\n\nğŸ“„ **ì œëª©**: {r['title']}\n\nğŸ‘¥ **ì €ì**: {r['authors']}\n\nğŸ“ **ì´ˆë¡**: {r['summary']}...\n\nğŸ”— **ë…¼ë¬¸ í˜ì´ì§€**: {r['entry_id']}\n\nğŸ“… **ì¶œíŒì¼**: {r['published']}"
         for i, r in enumerate(results, 1)]
    ) + "\n\në” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"
    cache_handler.setex(cache_key, 3600, response)
    return response

# PubMed ë…¼ë¬¸ ê²€ìƒ‰
base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/"

def search_pubmed(query, max_results=5):
    search_url = f"{base_url}esearch.fcgi"
    params = {"db": "pubmed", "term": query, "retmode": "json", "retmax": max_results, "api_key": NCBI_KEY}
    response = requests.get(search_url, params=params, timeout=3)
    return response.json()

def get_pubmed_summaries(id_list):
    summary_url = f"{base_url}esummary.fcgi"
    params = {"db": "pubmed", "id": ",".join(id_list), "retmode": "json", "api_key": NCBI_KEY}
    response = requests.get(summary_url, params=params, timeout=3)
    return response.json()

def get_pubmed_abstract(id_list):
    fetch_url = f"{base_url}efetch.fcgi"
    params = {"db": "pubmed", "id": ",".join(id_list), "retmode": "xml", "rettype": "abstract", "api_key": NCBI_KEY}
    response = requests.get(fetch_url, params=params, timeout=3)
    return response.text

def extract_first_two_sentences(abstract_text):
    if not abstract_text or abstract_text.isspace():
        return "No abstract available"
    sentences = [s.strip() for s in abstract_text.split('.') if s.strip()]
    return " ".join(sentences[:2]) + "." if sentences else "No abstract available"

def parse_abstracts(xml_text):
    abstract_dict = {}
    try:
        root = ET.fromstring(xml_text)
        for article in root.findall(".//PubmedArticle"):
            pmid = article.find(".//MedlineCitation/PMID").text
            abstract_elem = article.find(".//Abstract/AbstractText")
            abstract = abstract_elem.text if abstract_elem is not None else "No abstract available"
            abstract_dict[pmid] = extract_first_two_sentences(abstract)
    except ET.ParseError:
        return {}
    return abstract_dict

def get_pubmed_papers(query, max_results=5):
    cache_key = f"pubmed:{query}:{max_results}"
    cached = cache_handler.get(cache_key)
    if cached:
        return cached
    
    search_results = search_pubmed(query, max_results)
    pubmed_ids = search_results["esearchresult"]["idlist"]
    if not pubmed_ids:
        return "í•´ë‹¹ í‚¤ì›Œë“œë¡œ ì˜í•™ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    summaries = get_pubmed_summaries(pubmed_ids)
    abstracts_xml = get_pubmed_abstract(pubmed_ids)
    abstract_dict = parse_abstracts(abstracts_xml)
    response = "ğŸ©º **PubMed ë…¼ë¬¸ ê²€ìƒ‰ ê²°ê³¼** ğŸ©º\n\n"
    response += "\n\n".join(
        [f"**ë…¼ë¬¸ {i}**\n\n"
         f"ğŸ†” **PMID**: {pmid}\n\n"
         f"ğŸ“– **ì œëª©**: {summaries['result'][pmid].get('title', 'No title')}\n\n"
         f"ğŸ“… **ì¶œíŒì¼**: {format_date(summaries['result'][pmid].get('pubdate', 'No date'))}\n\n"
         f"âœï¸ **ì €ì**: {', '.join([author.get('name', '') for author in summaries['result'][pmid].get('authors', [])])}\n\n"
         f"ğŸ“ **ì´ˆë¡**: {abstract_dict.get(pmid, 'No abstract')}\n\n"
         f"ğŸ”— **ë…¼ë¬¸ í˜ì´ì§€**: https://pubmed.ncbi.nlm.nih.gov/{pmid}/"
         for i, pmid in enumerate(pubmed_ids, 1)]
    ) + "\n\në” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"
    cache_handler.setex(cache_key, 3600, response)
    return response
    
# ëŒ€í™”í˜• ì‘ë‹µ (ë¹„ë™ê¸°)
conversation_cache = MemoryCache()
_client_instance = None

def get_client():
    global _client_instance
    if _client_instance is None:
        client, provider_name = select_random_available_provider()
        _client_instance = client
        # ì„¸ì…˜ ìƒíƒœê°€ ì‚¬ìš© ê°€ëŠ¥í•œ ì»¨í…ìŠ¤íŠ¸ì—ì„œë§Œ ì—…ë°ì´íŠ¸
        if hasattr(st, 'session_state'):
            st.session_state.provider_name = provider_name
    return _client_instance

async def get_conversational_response(query, chat_history):
    cache_key = f"conv:{needs_search(query)}:{query}"
    cached = conversation_cache.get(cache_key)
    if cached:
        return cached
    
    messages = [
        {"role": "system", "content": "ì¹œì ˆí•œ AI ì±—ë´‡ì…ë‹ˆë‹¤. ì ì ˆí•œ ì´ëª¨ì§€ ì‚¬ìš©: âœ…(ì™„ë£Œ), â“(ì§ˆë¬¸), ğŸ˜Š(ì¹œì ˆ)"},
        {"role": "user", "content": query}
    ] + [{"role": msg["role"], "content": msg["content"]} 
         for msg in chat_history[-2:] if "ë” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”?" not in msg["content"]]
    
    # ë¹„ë™ê¸° ì‹¤í–‰ ì „ì— client ê°ì²´ë¥¼ ë¯¸ë¦¬ ê°€ì ¸ì˜´
    try:
        if not hasattr(st, 'session_state') or 'client' not in st.session_state:
            client, _ = select_random_available_provider()
        else:
            client = st.session_state.client
            
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            None, lambda: client.chat.completions.create(
                model="gpt-4o-mini", messages=messages
            )
        )
        result = response.choices[0].message.content if response.choices else "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    except Exception as e:
        logger.error(f"ëŒ€í™” ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        result = "ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    conversation_cache.setex(cache_key, 600, result)
    return result

GREETINGS = ["ì•ˆë…•", "í•˜ì´", "í—¬ë¡œ", "ã…ã…‡", "ì™“ì—…", "í• ë¡±", "í—¤ì´"]
GREETING_RESPONSE = "ì•ˆë…•í•˜ì„¸ìš”! ë°˜ê°‘ìŠµë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ğŸ˜Š"

@lru_cache(maxsize=100)
def needs_search(query):
    query_lower = query.strip().lower().replace(" ", "")
    if "ë‚ ì”¨" in query_lower:
        return "weather" if "ë‚´ì¼" not in query_lower else "tomorrow_weather"
    if "ì‹œê°„" in query_lower or "ë‚ ì§œ" in query_lower:
        return "time"
    if "ë¦¬ê·¸ìˆœìœ„" in query_lower:
        return "league_standings"
    if "ë¦¬ê·¸ë“ì ìˆœìœ„" in query_lower or "ë“ì ìˆœìœ„" in query_lower:
        return "league_scorers"
    if ("ì±”í”¼ì–¸ìŠ¤ë¦¬ê·¸" in query_lower or "ucl" in query_lower) and (
        "í† ë„ˆë¨¼íŠ¸" in query_lower or "knockout" in query_lower or "16ê°•" in query_lower or "8ê°•" in query_lower or "4ê°•" in query_lower or "ê²°ìŠ¹" in query_lower):
        return "cl_knockout"
    if "ì•½í’ˆê²€ìƒ‰" in query_lower:
        return "drug"
    if "ê³µí•™ë…¼ë¬¸" in query_lower or "arxiv" in query_lower:
        return "arxiv_search"
    if "ì˜í•™ë…¼ë¬¸" in query_lower:
        return "pubmed_search"
    if "ê²€ìƒ‰í•´ì¤˜" in query_lower or "ê²€ìƒ‰í•´" in query_lower:
        return "naver_search"

    # MBTI ê´€ë ¨
    if "mbtiê²€ì‚¬" in query_lower:
        return "mbti"
    if "mbtiìœ í˜•ì„¤ëª…" in query_lower or "mbtiìœ í˜•" in query_lower or "mbtiì„¤ëª…" in query_lower:
        return "mbti_types"
    
    # ë‹¤ì¤‘ì§€ëŠ¥ ê´€ë ¨
    if "ë‹¤ì¤‘ì§€ëŠ¥ìœ í˜•ì„¤ëª…" in query_lower or "ë‹¤ì¤‘ì§€ëŠ¥ìœ í˜•" in query_lower or "ë‹¤ì¤‘ì§€ëŠ¥ì„¤ëª…" in query_lower or \
       "ë‹¤ì¤‘ì§€ëŠ¥ ìœ í˜• ì„¤ëª…" in query.strip().lower() or "ë‹¤ì¤‘ì§€ëŠ¥ ìœ í˜•" in query.strip().lower():
        return "multi_iq_types"
    if "ë‹¤ì¤‘ì§€ëŠ¥ì§ì—…" in query_lower or "ë‹¤ì¤‘ì§€ëŠ¥ì¶”ì²œ" in query_lower or \
       "ë‹¤ì¤‘ì§€ëŠ¥ ì§ì—…" in query.strip().lower() or "ë‹¤ì¤‘ì§€ëŠ¥ ì¶”ì²œ" in query.strip().lower():
        return "multi_iq_jobs"
    if "ë‹¤ì¤‘ì§€ëŠ¥ê²€ì‚¬" in query_lower or "ë‹¤ì¤‘ì§€ëŠ¥ ê²€ì‚¬" in query.strip().lower():
        return "multi_iq"
    if "ë‹¤ì¤‘ì§€ëŠ¥" in query_lower:
        return "multi_iq_full"
    
    if any(greeting in query_lower for greeting in GREETINGS):
        return "conversation"
    return "conversation"

def process_query(query):
    cache_key = f"query:{hash(query)}"
    cached = cache_handler.get(cache_key)
    if cached is not None:
        return cached
    
    query_type = needs_search(query)
    query_lower = query.strip().lower().replace(" ", "")
    
    with ThreadPoolExecutor() as executor:
        if query_type == "weather":
            future = executor.submit(weather_api.get_city_weather, extract_city_from_query(query))
            result = future.result()
        elif query_type == "tomorrow_weather":
            future = executor.submit(weather_api.get_forecast_by_day, extract_city_from_query(query), 1)
            result = future.result()
        elif query_type == "time":
            if "ì˜¤ëŠ˜ë‚ ì§œ" in query_lower or "í˜„ì¬ë‚ ì§œ" in query_lower or "ê¸ˆì¼ë‚ ì§œ" in query_lower:
                result = get_kst_time()
            else:
                city = extract_city_from_time_query(query)
                future = executor.submit(get_time_by_city, city)
                result = future.result()
        elif query_type == "league_standings":
            league_key = extract_league_from_query(query)
            if league_key:
                league_info = LEAGUE_MAPPING[league_key]
                future = executor.submit(football_api.fetch_league_standings, league_info["code"], league_info["name"])
                result = future.result()
                result = result["error"] if "error" in result else {
                    "header": f"{result['league_name']} ë¦¬ê·¸ ìˆœìœ„",
                    "table": result["data"],
                    "footer": "ë” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"
                }
            else:
                result = "ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¦¬ê·¸ì…ë‹ˆë‹¤. ğŸ˜“ ì§€ì› ë¦¬ê·¸: EPL, LaLiga, Bundesliga, Serie A, Ligue 1"
        elif query_type == "league_scorers":
            league_key = extract_league_from_query(query)
            if league_key:
                league_info = LEAGUE_MAPPING[league_key]
                future = executor.submit(football_api.fetch_league_scorers, league_info["code"], league_info["name"])
                try:
                    result = future.result()
                    result = result["error"] if "error" in result else {
                        "header": f"{result['league_name']} ë¦¬ê·¸ ë“ì ìˆœìœ„ (ìƒìœ„ 10ëª…)",
                        "table": result["data"],
                        "footer": "ë” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"
                    }
                except Exception as e:
                    result = f"ë¦¬ê·¸ ë“ì ìˆœìœ„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)} ğŸ˜“"
            else:
                result = "ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¦¬ê·¸ì…ë‹ˆë‹¤. ğŸ˜“ ì§€ì› ë¦¬ê·¸: EPL, LaLiga, Bundesliga, Serie A, Ligue 1"
        elif query_type == "cl_knockout":
            future = executor.submit(football_api.fetch_championsleague_knockout_matches)
            results = future.result()
            if isinstance(results, str):
                result = results
            else:
                if not results:
                    result = "ì±”í”¼ì–¸ìŠ¤ë¦¬ê·¸ í† ë„ˆë¨¼íŠ¸ ê²½ê¸° ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
                else:
                    df = pd.DataFrame(results)
                    result = {
                        "header": "ì±”í”¼ì–¸ìŠ¤ë¦¬ê·¸ Knockout Stage ê²°ê³¼",
                        "table": df,
                        "footer": "ë” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"
                    }
        
                
        elif query_type == "drug":
            future = executor.submit(get_drug_info, query)
            result = future.result()
        elif query_type == "arxiv_search":
            keywords = query.replace("ê³µí•™ë…¼ë¬¸", "").replace("arxiv", "").strip()
            future = executor.submit(get_arxiv_papers, keywords)
            result = future.result()
        elif query_type == "pubmed_search":
            keywords = query.replace("ì˜í•™ë…¼ë¬¸", "").strip()
            future = executor.submit(get_pubmed_papers, keywords)
            result = future.result()
        elif query_type == "naver_search":
            search_query = query.lower().replace("ê²€ìƒ‰", "").strip()
            future = executor.submit(get_naver_api_results, search_query)
            result = future.result()
        elif query_type == "mbti":
            result = (
                "MBTI ê²€ì‚¬ë¥¼ ì›í•˜ì‹œë‚˜ìš”? âœ¨ ì•„ë˜ ì‚¬ì´íŠ¸ì—ì„œ ë¬´ë£Œë¡œ ì„±ê²© ìœ í˜• ê²€ì‚¬ë¥¼ í•  ìˆ˜ ìˆì–´ìš”! ğŸ˜Š\n"
                "[16Personalities MBTI ê²€ì‚¬](https://www.16personalities.com/ko/%EB%AC%B4%EB%A3%8C-%EC%84%B1%EA%B2%A9-%EC%9C%A0%ED%98%95-%EA%B2%80%EC%82%AC) ğŸŒŸ\n"
                "ì´ ì‚¬ì´íŠ¸ëŠ” 16ê°€ì§€ ì„±ê²© ìœ í˜•ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ í…ŒìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ë©°, ê²°ê³¼ì— ë”°ë¼ ì„±ê²© ì„¤ëª…ê³¼ ì¸ê°„ê´€ê³„ ì¡°ì–¸ ë“±ì„ í™•ì¸í•  ìˆ˜ ìˆì–´ìš”! ğŸ’¡"
            )
        elif query_type == "mbti_types":
            specific_type = query_lower.replace("mbti", "").replace("ìœ í˜•", "").replace("ì„¤ëª…", "").strip().upper()
            if specific_type in mbti_descriptions:
                result = f"### ğŸ­ {specific_type} í•œ ì¤„ ì„¤ëª…\n- âœ… **{specific_type}** {mbti_descriptions[specific_type]}"
            else:
                result = mbti_full_description
        elif query_type == "multi_iq":
            result = (
                "ë‹¤ì¤‘ì§€ëŠ¥ ê²€ì‚¬ë¥¼ ì›í•˜ì‹œë‚˜ìš”? ğŸ‰ ì•„ë˜ ì‚¬ì´íŠ¸ì—ì„œ ë¬´ë£Œë¡œ ë‹¤ì¤‘ì§€ëŠ¥ í…ŒìŠ¤íŠ¸ë¥¼ í•´ë³¼ ìˆ˜ ìˆì–´ìš”! ğŸ˜„\n"
                "[Multi IQ Test](https://multiiqtest.com/) ğŸš€\n"
                "ì´ ì‚¬ì´íŠ¸ëŠ” í•˜ì›Œë“œ ê°€ë“œë„ˆì˜ ë‹¤ì¤‘ì§€ëŠ¥ ì´ë¡ ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ í…ŒìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ë©°, ë‹¤ì–‘í•œ ì§€ëŠ¥ ì˜ì—­ì„ í‰ê°€í•´ì¤ë‹ˆë‹¤! ğŸ“šâœ¨"
            )
        elif query_type == "multi_iq_types":
            specific_type = query_lower.replace("ë‹¤ì¤‘ì§€ëŠ¥", "").replace("multi_iq", "").replace("ìœ í˜•", "").replace("ì„¤ëª…", "").strip().replace(" ", "")
            if specific_type in multi_iq_descriptions:
                result = f"### ğŸ¨ {specific_type.replace('ì§€ëŠ¥', ' ì§€ëŠ¥')} í•œ ì¤„ ì„¤ëª…\n- ğŸ“– **{specific_type.replace('ì§€ëŠ¥', ' ì§€ëŠ¥')}** {multi_iq_descriptions[specific_type]['description']}"
            else:
                result = multi_iq_full_description
        elif query_type == "multi_iq_jobs":
            specific_type = query_lower.replace("ë‹¤ì¤‘ì§€ëŠ¥", "").replace("multi_iq", "").replace("ì§ì—…", "").replace("ì¶”ì²œ", "").strip().replace(" ", "")
            if specific_type in multi_iq_descriptions:
                result = f"### ğŸ¨ {specific_type.replace('ì§€ëŠ¥', ' ì§€ëŠ¥')} ì¶”ì²œ ì§ì—…\n- ğŸ“– **{specific_type.replace('ì§€ëŠ¥', ' ì§€ëŠ¥')}**: {multi_iq_descriptions[specific_type]['description']}- **ì¶”ì²œ ì§ì—…**: {multi_iq_descriptions[specific_type]['jobs']}"
            else:
                result = multi_iq_full_description
        elif query_type == "multi_iq_full":
            result = multi_iq_full_description
        elif query_type == "conversation":
            if query_lower in GREETINGS:
                result = GREETING_RESPONSE
            else:
                result = asyncio.run(get_conversational_response(query, st.session_state.messages))
        else:
            result = "ì•„ì§ ì§€ì›í•˜ì§€ ì•ŠëŠ” ê¸°ëŠ¥ì´ì—ìš”. ğŸ˜…"
        
        cache_handler.setex(cache_key, 600, result)
        return result

def show_chat_dashboard():
    st.title("Chat with AIğŸ¤–")
    if st.button("ë„ì›€ë§ â„¹ï¸"):
        st.info(
            "ì±—ë´‡ê³¼ ë” ì‰½ê²Œ ëŒ€í™”í•˜ëŠ” ë°©ë²•ì´ì—ìš”! :\n"
            "1. **ë‚ ì”¨** â˜€ï¸: '[ë„ì‹œëª…] ë‚ ì”¨' (ì˜ˆ: ì„œìš¸ ë‚ ì”¨, ë‚´ì¼ ì„œìš¸ ë‚ ì”¨)\n"
            "2. **ì‹œê°„/ë‚ ì§œ** â±ï¸: '[ë„ì‹œëª…] ì‹œê°„' ë˜ëŠ” 'ì˜¤ëŠ˜ ë‚ ì§œ' (ì˜ˆ: ë§ˆë“œë¦¬ë“œ ì‹œê°„, ê¸ˆì¼ ë‚ ì§œ)\n"
            "3. **ê²€ìƒ‰** ğŸŒ: '[í‚¤ì›Œë“œ] ê²€ìƒ‰í•´' ë˜ëŠ” '[í‚¤ì›Œë“œ] ê²€ìƒ‰í•´ì¤˜' (ì˜ˆ: 2025ë…„ ì„œìš¸ ì „ì‹œíšŒ ê²€ìƒ‰í•´ì¤˜)\n"
            "4. **ì•½í’ˆê²€ìƒ‰** ğŸ’Š: 'ì•½í’ˆê²€ìƒ‰ [ì•½ ì´ë¦„]' (ì˜ˆ: ì•½í’ˆê²€ìƒ‰ ê²Œë³´ë¦°)\n"
            "5. **ê³µí•™ë…¼ë¬¸** ğŸ“š: 'ê³µí•™ë…¼ë¬¸ [í‚¤ì›Œë“œ]' (ì˜ˆ: ê³µí•™ë…¼ë¬¸ Multimodal AI)\n"
            "6. **ì˜í•™ë…¼ë¬¸** ğŸ©º: 'ì˜í•™ë…¼ë¬¸ [í‚¤ì›Œë“œ]' (ì˜ˆ: ì˜í•™ë…¼ë¬¸ cancer therapy)\n"
            "7. **ë¦¬ê·¸ìˆœìœ„** âš½: '[ë¦¬ê·¸ ì´ë¦„] ë¦¬ê·¸ ìˆœìœ„ ë˜ëŠ” ë¦¬ê·¸ë“ì ìˆœìœ„' (ì˜ˆ: EPL ë¦¬ê·¸ìˆœìœ„, EPL ë¦¬ê·¸ë“ì ìˆœìœ„)\n"
            "   - ì§€ì› ë¦¬ê·¸: EPL, LaLiga, Bundesliga, Serie A, Ligue 1, ChampionsLeague\n"
            "8. **MBTI** âœ¨: 'MBTI ê²€ì‚¬',  'MBTI ìœ í˜•', 'MBTI ì„¤ëª…' (ì˜ˆ: MBTI ê²€ì‚¬, INTJ ì„¤ëª…)\n"
            "9. **ë‹¤ì¤‘ì§€ëŠ¥** ğŸ‰: 'ë‹¤ì¤‘ì§€ëŠ¥ ê²€ì‚¬', 'ë‹¤ì¤‘ì§€ëŠ¥ ìœ í˜•', 'ë‹¤ì¤‘ì§€ëŠ¥ ì§ì—…', (ì˜ˆ: ë‹¤ì¤‘ì§€ëŠ¥ ê²€ì‚¬, ì–¸ì–´ì§€ëŠ¥ ì§ì—…)\n\n"
            "ê¶ê¸ˆí•œ ì  ìˆìœ¼ë©´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”! ğŸ˜Š"
        )
   
    for msg in st.session_state.messages[-10:]:
        with st.chat_message(msg['role']):
            if isinstance(msg['content'], dict) and "table" in msg['content']:
                st.markdown(f"### {msg['content']['header']}")
                st.dataframe(pd.DataFrame(msg['content']['table']), use_container_width=True, hide_index=True)
                st.markdown(msg['content']['footer'])
            else:
                st.markdown(msg['content'], unsafe_allow_html=True)
    
    if user_prompt := st.chat_input("ì§ˆë¬¸í•´ ì£¼ì„¸ìš”!"):
        st.chat_message("user").markdown(user_prompt)
        st.session_state.messages.append({"role": "user", "content": user_prompt})
        with st.chat_message("assistant"):
            placeholder = st.empty()
            placeholder.markdown("ì‘ë‹µì„ ì¤€ë¹„ ì¤‘ì´ì—ìš”.. â³")
            try:
                start_time = time.time()
                response = process_query(user_prompt)
                time_taken = round(time.time() - start_time, 2)
                
                placeholder.empty()
                if isinstance(response, dict) and "table" in response:
                    st.markdown(f"### {response['header']}")
                    st.dataframe(response['table'], use_container_width=True, hide_index=True)
                    st.markdown(response['footer'])
                else:
                    st.markdown(response, unsafe_allow_html=True)
                
                st.session_state.messages.append({"role": "assistant", "content": response})
                async_save_chat_history(st.session_state.user_id, st.session_state.session_id, user_prompt, response, time_taken)
            
            except Exception as e:
                placeholder.empty()
                error_msg = f"ì‘ë‹µì„ ì¤€ë¹„í•˜ë‹¤ ë¬¸ì œ: {str(e)} ğŸ˜“"
                logger.error(f"ëŒ€í™” ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)
                st.markdown(error_msg, unsafe_allow_html=True)
                st.session_state.messages.append({"role": "assistant", "content": error_msg})

def show_login_page():
    st.title("ë¡œê·¸ì¸ ğŸ¤—")
    with st.form("login_form"):
        nickname = st.text_input("ë‹‰ë„¤ì„", placeholder="ì˜ˆ: í›„ì•ˆ")
        submit_button = st.form_submit_button("ì‹œì‘í•˜ê¸° ğŸš€")
        
        if submit_button and nickname:
            try:
                user_id, is_existing = create_or_get_user(nickname)
                st.session_state.user_id = user_id
                st.session_state.is_logged_in = True
                st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ë„ì›€ë§ë„ í™œìš©í•´ ë³´ì„¸ìš” ğŸ˜Š"}]
                st.session_state.session_id = str(uuid.uuid4())
                st.toast(f"í™˜ì˜í•©ë‹ˆë‹¤, {nickname}ë‹˜! ğŸ‰")
                time.sleep(1)
                st.rerun()
            except Exception:
                st.toast("ë¡œê·¸ì¸ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.", icon="âŒ")

def main():
    init_session_state()  # ë°˜ë“œì‹œ ì²« ì¤„ì—ì„œ í˜¸ì¶œ
    if not st.session_state.is_logged_in:
        show_login_page()
    else:
        show_chat_dashboard()

if __name__ == "__main__":
    main()