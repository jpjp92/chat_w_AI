# # app2.py

# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì •
from config.imports import *
from config.env import *

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.WARNING if os.getenv("ENV") == "production" else logging.INFO)
logger = logging.getLogger("HybridChat")

# ìºì‹œ ì„¤ì •
cache = Cache("cache_directory")

class MemoryCache:
    def __init__(self):
        self.cache = {}
        self.expiry = {}
    
    def get(self, key):
        if key in self.cache and time.time() < self.expiry[key]:
            return self.cache[key]
        return cache.get(key)
    
    def setex(self, key, ttl, value):
        self.cache[key] = value
        self.expiry[key] = time.time() + ttl
        cache.set(key, value, expire=ttl)

cache_handler = MemoryCache()

# WeatherAPI í´ë˜ìŠ¤ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
class WeatherAPI:
    def __init__(self, cache_ttl=600):
        self.cache = cache_handler
        self.cache_ttl = cache_ttl

    def fetch_weather(self, url, params):
        session = requests.Session()
        retry_strategy = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("https://", adapter)
        response = session.get(url, params=params, timeout=5)
        response.raise_for_status()
        return response.json()

    @lru_cache(maxsize=100)
    def get_city_info(self, city_name):
        cache_key = f"city_info:{city_name}"
        cached = self.cache.get(cache_key)
        if cached:
            return cached
        url = "http://api.openweathermap.org/geo/1.0/direct"
        params = {'q': city_name, 'limit': 1, 'appid': WEATHER_API_KEY}
        data = self.fetch_weather(url, params)
        if data and len(data) > 0:
            city_info = {"name": data[0]["name"], "lat": data[0]["lat"], "lon": data[0]["lon"]}
            self.cache.setex(cache_key, 86400, city_info)
            return city_info
        return None

    def get_city_weather(self, city_name):
        cache_key = f"weather:{city_name}"
        cached_data = self.cache.get(cache_key)
        if cached_data:
            return cached_data
        
        city_info = self.get_city_info(city_name)
        if not city_info:
            return f"'{city_name}'ì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        url = "https://api.openweathermap.org/data/2.5/weather"
        params = {'lat': city_info["lat"], 'lon': city_info["lon"], 'appid': WEATHER_API_KEY, 'units': 'metric', 'lang': 'kr'}
        data = self.fetch_weather(url, params)
        weather_emojis = {'Clear': 'â˜€ï¸', 'Clouds': 'â˜ï¸', 'Rain': 'ğŸŒ§ï¸', 'Snow': 'â„ï¸', 'Thunderstorm': 'â›ˆï¸', 'Drizzle': 'ğŸŒ¦ï¸', 'Mist': 'ğŸŒ«ï¸'}
        weather_emoji = weather_emojis.get(data['weather'][0]['main'], 'ğŸŒ¤ï¸')
        result = (
            f"í˜„ì¬ {data['name']}, {data['sys']['country']} ë‚ ì”¨ {weather_emoji}\n"
            f"ë‚ ì”¨: {data['weather'][0]['description']}\n"
            f"ì˜¨ë„: {data['main']['temp']}Â°C\n"
            f"ì²´ê°: {data['main']['feels_like']}Â°C\n"
            f"ìŠµë„: {data['main']['humidity']}%\n"
            f"í’ì†: {data['wind']['speed']}m/s\n"
            f"ë” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"
        )
        self.cache.setex(cache_key, self.cache_ttl, result)
        return result
        
    def get_forecast_by_day(self, city_name, days_from_today=1):
        cache_key = f"forecast:{city_name}:{days_from_today}"
        cached_data = self.cache.get(cache_key)
        if cached_data:
            return cached_data
        
        city_info = self.get_city_info(city_name)
        if not city_info:
            return f"'{city_name}'ì˜ ë‚ ì”¨ ì˜ˆë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        url = "https://api.openweathermap.org/data/2.5/forecast"
        params = {'lat': city_info["lat"], 'lon': city_info["lon"], 'appid': WEATHER_API_KEY, 'units': 'metric', 'lang': 'kr'}
        data = self.fetch_weather(url, params)
        target_date = (datetime.now() + timedelta(days=days_from_today)).strftime('%Y-%m-%d')
        forecast_text = f"{city_info['name']}ì˜ {target_date} ë‚ ì”¨ ì˜ˆë³´ ğŸŒ¤ï¸\n\n"
        weather_emojis = {'Clear': 'â˜€ï¸', 'Clouds': 'â˜ï¸', 'Rain': 'ğŸŒ§ï¸', 'Snow': 'â„ï¸', 'Thunderstorm': 'â›ˆï¸', 'Drizzle': 'ğŸŒ¦ï¸', 'Mist': 'ğŸŒ«ï¸'}
        
        found = False
        for forecast in data['list']:
            dt = datetime.fromtimestamp(forecast['dt']).strftime('%Y-%m-%d')
            if dt == target_date:
                found = True
                time_only = datetime.fromtimestamp(forecast['dt']).strftime('%H:%M')
                weather_emoji = weather_emojis.get(forecast['weather'][0]['main'], 'ğŸŒ¤ï¸')
                forecast_text += (
                    f"â° {time_only} {forecast['weather'][0]['description']} {weather_emoji} "
                    f"{forecast['main']['temp']}Â°C ğŸ’§{forecast['main']['humidity']}% ğŸŒ¬ï¸{forecast['wind']['speed']}m/s\n\n"
                )
        
        result = forecast_text + "ë” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š" if found else f"'{city_name}'ì˜ {target_date} ë‚ ì”¨ ì˜ˆë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        self.cache.setex(cache_key, self.cache_ttl, result)
        return result

    def get_weekly_forecast(self, city_name):
        cache_key = f"weekly_forecast:{city_name}"
        cached_data = self.cache.get(cache_key)
        if cached_data:
            return cached_data
        
        city_info = self.get_city_info(city_name)
        if not city_info:
            return f"'{city_name}'ì˜ ì£¼ê°„ ì˜ˆë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        url = "https://api.openweathermap.org/data/2.5/forecast"
        params = {'lat': city_info["lat"], 'lon': city_info["lon"], 'appid': WEATHER_API_KEY, 'units': 'metric', 'lang': 'kr'}
        data = self.fetch_weather(url, params)
        today = datetime.now().date()
        week_end = today + timedelta(days=6)
        daily_forecast = {}
        weekdays_kr = ["ì›”ìš”ì¼", "í™”ìš”ì¼", "ìˆ˜ìš”ì¼", "ëª©ìš”ì¼", "ê¸ˆìš”ì¼", "í† ìš”ì¼", "ì¼ìš”ì¼"]
        today_weekday = today.weekday()
        
        for forecast in data['list']:
            dt = datetime.fromtimestamp(forecast['dt']).date()
            if today <= dt <= week_end:
                dt_str = dt.strftime('%Y-%m-%d')
                if dt_str not in daily_forecast:
                    weekday_idx = (today_weekday + (dt - today).days) % 7
                    daily_forecast[dt_str] = {
                        'weekday': weekdays_kr[weekday_idx],
                        'temp_min': forecast['main']['temp_min'],
                        'temp_max': forecast['main']['temp_max'],
                        'weather': forecast['weather'][0]['description']
                    }
                else:
                    daily_forecast[dt_str]['temp_min'] = min(daily_forecast[dt_str]['temp_min'], forecast['main']['temp_min'])
                    daily_forecast[dt_str]['temp_max'] = max(daily_forecast[dt_str]['temp_max'], forecast['main']['temp_max'])
        
        today_str = today.strftime('%Y-%m-%d')
        today_weekday_str = weekdays_kr[today_weekday]
        forecast_text = f"{today_str}({today_weekday_str}) ê¸°ì¤€ {city_info['name']}ì˜ ì£¼ê°„ ë‚ ì”¨ ì˜ˆë³´ ğŸŒ¤ï¸\n"
        weather_emojis = {'Clear': 'â˜€ï¸', 'Clouds': 'â˜ï¸', 'Rain': 'ğŸŒ§ï¸', 'Snow': 'â„ï¸', 'Thunderstorm': 'â›ˆï¸', 'Drizzle': 'ğŸŒ¦ï¸', 'Mist': 'ğŸŒ«ï¸'}
        
        for date, info in daily_forecast.items():
            weather_emoji = weather_emojis.get(info['weather'].split()[0], 'ğŸŒ¤ï¸')
            forecast_text += (
                f"\n{info['weekday']}: {info['weather']} {weather_emoji} "
                f"ìµœì € {info['temp_min']}Â°C ìµœê³  {info['temp_max']}Â°C\n\n"
            )
        
        result = forecast_text + "\në” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"
        self.cache.setex(cache_key, self.cache_ttl, result)
        return result

# SportsAPI í´ë˜ìŠ¤ ì¶”ê°€
class SportsAPI:
    def __init__(self, api_key, cache_ttl=86400):
        self.cache = cache_handler
        self.cache_ttl = cache_ttl
        self.base_url = "https://www.thesportsdb.com/api/v1/json"
        self.api_key = api_key

    def fetch_team_id(self, team_name):
        """íŒ€ ì´ë¦„ì„ í†µí•´ íŒ€ IDë¥¼ ì¡°íšŒ"""
        cache_key = f"team_id:{team_name}"
        cached_team_id = self.cache.get(cache_key)
        if cached_team_id:
            return cached_team_id

        url = f"{self.base_url}/{self.api_key}/searchteams.php"
        params = {'t': team_name.replace(' ', '%20')}
        try:
            response = requests.get(url, params=params, timeout=5)
            response.raise_for_status()
            data = response.json()
            if data['teams']:
                team_id = data['teams'][0]['idTeam']
                self.cache.setex(cache_key, self.cache_ttl, team_id)
                return team_id
            else:
                logger.warning(f"íŒ€ {team_name}ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
        except Exception as e:
            logger.error(f"íŒ€ ID ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return None

    def fetch_team_schedule(self, team_name, month):
        """íŒ€ì˜ ê²½ê¸° ì¼ì •ì„ ì¡°íšŒí•˜ê³  ì§€ì •ëœ ì›”ì— í•´ë‹¹í•˜ëŠ” ê²½ê¸°ë§Œ í•„í„°ë§"""
        cache_key = f"sports_schedule:{team_name}:{month}"
        cached_data = self.cache.get(cache_key)
        if cached_data:
            return cached_data

        team_id = self.fetch_team_id(team_name)
        if not team_id:
            return f"'{team_name}'ì˜ ê²½ê¸° ì¼ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ğŸ˜“"

        url = f"{self.base_url}/{self.api_key}/eventsnext.php"
        params = {'id': team_id}
        try:
            response = requests.get(url, params=params, timeout=5)
            response.raise_for_status()
            data = response.json()
            if not data['events']:
                return f"'{team_name}'ì˜ ì˜ˆì •ëœ ê²½ê¸°ê°€ ì—†ìŠµë‹ˆë‹¤. ğŸ˜“"

            league_color = {
                "UEFA Champions League": "color: #FFD700;",
                "LaLiga": "color: #FF4500;",
                "Copa del Rey": "color: #4169E1;",
            }

            result = f"**âš½ {team_name}ì˜ {month}ì›” ê²½ê¸° ì¼ì • âš½**\n\n"
            result += "-" * 75 + "\n\n"
            found = False
            today = datetime.now().date()
            for event in data['events']:
                event_date_str = event['dateEvent']
                event_date = datetime.strptime(event_date_str, '%Y-%m-%d')
                if event_date.month == month and event_date.year == 2025:
                    found = True
                    event_time = event['strTime'] if event['strTime'] else "ì‹œê°„ ë¯¸ì •"
                    timezone_info = "(CET)"

                    event_date_only = datetime.strptime(event_date_str, '%Y-%m-%d').date()
                    if event_date_only == today:
                        date_label = f"ğŸ“… ë‚ ì§œ: {event_date_str} {event_time} {timezone_info} (ì˜¤ëŠ˜! ğŸ”¥)"
                    elif event_date_only == today + timedelta(days=1):
                        date_label = f"ğŸ“… ë‚ ì§œ: {event_date_str} {event_time} {timezone_info} (ë‚´ì¼! ğŸ”¥)"
                    else:
                        date_label = f"ğŸ“… ë‚ ì§œ: {event_date_str} {event_time} {timezone_info}"

                    league_style = league_color.get(event['strLeague'], "color: #000000;")
                    league_display = f"<span style='{league_style}'>ğŸ† ë¦¬ê·¸: {event['strLeague']}</span>"

                    result += (
                        f"{date_label}\n\n"
                        f"ğŸŸï¸ íŒ€: {event['strEvent']} (í™ˆ: {event['strHomeTeam']} vs ì›ì •: {event['strAwayTeam']})\n\n"
                        f"{league_display}\n\n"
                        f"ğŸ“ ì¥ì†Œ: {event.get('strVenue', 'ë¯¸ì •')}\n\n"
                        f"{'-' * 75}\n\n"
                    )

            if not found:
                result = f"'{team_name}'ì˜ {month}ì›” ê²½ê¸° ì¼ì •ì´ ì—†ìŠµë‹ˆë‹¤. ğŸ˜“\n\n"
            result += "ë” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"
            self.cache.setex(cache_key, self.cache_ttl, result)
            return result

        except Exception as e:
            logger.error(f"ê²½ê¸° ì¼ì • ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return f"ê²½ê¸° ì¼ì •ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ğŸ˜“\n\në” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"

    def fetch_league_schedule(self, league_key, month, initial_limit=5):
        """ë¦¬ê·¸ì˜ ê²½ê¸° ì¼ì •ì„ ì¡°íšŒí•˜ê³  ì´ˆê¸°ì—ëŠ” ìµœëŒ€ initial_limit ê²½ê¸° í‘œì‹œ"""
        cache_key = f"league_schedule:{league_key}:{month}"
        cached_data = self.cache.get(cache_key)
        if cached_data:
            return cached_data

        if league_key not in LEAGUE_MAPPING:
            return f"'{league_key}' ë¦¬ê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ğŸ˜“"

        league_info = LEAGUE_MAPPING[league_key]
        league_id = league_info['id']
        league_name = league_info['name']

        url = f"{self.base_url}/{self.api_key}/eventsnextleague.php"
        params = {'id': league_id}
        try:
            response = requests.get(url, params=params, timeout=5)
            response.raise_for_status()
            data = response.json()
            if not data['events']:
                return f"'{league_name}'ì˜ ì˜ˆì •ëœ ê²½ê¸°ê°€ ì—†ìŠµë‹ˆë‹¤. ğŸ˜“"

            league_color = {
                "English Premier League": "color: #800080;",
                "German Bundesliga": "color: #FF0000;",
                "Italian Serie A": "color: #008000;",
                "French Ligue 1": "color: #0000FF;",
                "UEFA Europa League": "color: #FFA500;",
                "South Korean K League 1": "color: #FFD700;",
                "AFC Champions League Elite": "color: #00CED1;"
            }

            events = sorted(data['events'], key=lambda x: x['dateEvent'])
            filtered_events = [event for event in events if datetime.strptime(event['dateEvent'], '%Y-%m-%d').month == month and datetime.strptime(event['dateEvent'], '%Y-%m-%d').year == 2025]

            if not filtered_events:
                return f"'{league_name}'ì˜ {month}ì›” ê²½ê¸° ì¼ì •ì´ ì—†ìŠµë‹ˆë‹¤. ğŸ˜“\n\në” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"

            initial_events = filtered_events[:initial_limit]
            remaining_events = filtered_events[initial_limit:]

            if f"show_more_{league_key}_{month}" not in st.session_state:
                st.session_state[f"show_more_{league_key}_{month}"] = False

            result = f"**âš½ {league_name}ì˜ {month}ì›” ê²½ê¸° ì¼ì • âš½**\n\n"
            result += "-" * 75 + "\n\n"
            today = datetime.now().date()
            for event in initial_events:
                event_date_str = event['dateEvent']
                event_date = datetime.strptime(event_date_str, '%Y-%m-%d')
                event_time = event['strTime'] if event['strTime'] else "ì‹œê°„ ë¯¸ì •"

                timezone_info = "(CET)"
                if league_key in ["k league 1", "kleague1", "afc champions league elite", "afcchampionsleagueelite"]:
                    timezone_info = "(KST)"

                event_date_only = datetime.strptime(event_date_str, '%Y-%m-%d').date()
                if event_date_only == today:
                    date_label = f"ğŸ“… ë‚ ì§œ: {event_date_str} {event_time} {timezone_info} (ì˜¤ëŠ˜! ğŸ”¥)"
                elif event_date_only == today + timedelta(days=1):
                    date_label = f"ğŸ“… ë‚ ì§œ: {event_date_str} {event_time} {timezone_info} (ë‚´ì¼! ğŸ”¥)"
                else:
                    date_label = f"ğŸ“… ë‚ ì§œ: {event_date_str} {event_time} {timezone_info}"

                league_style = league_color.get(league_name, "color: #000000;")
                league_display = f"<span style='{league_style}'>ğŸ† ë¦¬ê·¸: {league_name}</span>"

                result += (
                    f"{date_label}\n\n"
                    f"ğŸŸï¸ íŒ€: {event['strEvent']} (í™ˆ: {event['strHomeTeam']} vs ì›ì •: {event['strAwayTeam']})\n\n"
                    f"{league_display}\n\n"
                    f"ğŸ“ ì¥ì†Œ: {event.get('strVenue', 'ë¯¸ì •')}\n\n"
                    f"{'-' * 75}\n\n"
                )

            if remaining_events:
                result += "ë” ë§ì€ ê²½ê¸°ë¥¼ ë³´ë ¤ë©´ ì•„ë˜ ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”:\n\n"
                if st.button("ë” ë³´ê¸°", key=f"show_more_{league_key}_{month}"):
                    st.session_state[f"show_more_{league_key}_{month}"] = True

                if st.session_state[f"show_more_{league_key}_{month}"]:
                    for event in remaining_events:
                        event_date_str = event['dateEvent']
                        event_date = datetime.strptime(event_date_str, '%Y-%m-%d')
                        event_time = event['strTime'] if event['strTime'] else "ì‹œê°„ ë¯¸ì •"

                        timezone_info = "(CET)"
                        if league_key in ["k league 1", "kleague1", "afc champions league elite", "afcchampionsleagueelite"]:
                            timezone_info = "(KST)"

                        event_date_only = datetime.strptime(event_date_str, '%Y-%m-%d').date()
                        if event_date_only == today:
                            date_label = f"ğŸ“… ë‚ ì§œ: {event_date_str} {event_time} {timezone_info} (ì˜¤ëŠ˜! ğŸ”¥)"
                        elif event_date_only == today + timedelta(days=1):
                            date_label = f"ğŸ“… ë‚ ì§œ: {event_date_str} {event_time} {timezone_info} (ë‚´ì¼! ğŸ”¥)"
                        else:
                            date_label = f"ğŸ“… ë‚ ì§œ: {event_date_str} {event_time} {timezone_info}"

                        league_style = league_color.get(league_name, "color: #000000;")
                        league_display = f"<span style='{league_style}'>ğŸ† ë¦¬ê·¸: {league_name}</span>"

                        result += (
                            f"{date_label}\n\n"
                            f"ğŸŸï¸ íŒ€: {event['strEvent']} (í™ˆ: {event['strHomeTeam']} vs ì›ì •: {event['strAwayTeam']})\n\n"
                            f"{league_display}\n\n"
                            f"ğŸ“ ì¥ì†Œ: {event.get('strVenue', 'ë¯¸ì •')}\n\n"
                            f"{'-' * 75}\n\n"
                        )

            result += "ë” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"
            self.cache.setex(cache_key, self.cache_ttl, result)
            return result

        except Exception as e:
            logger.error(f"ë¦¬ê·¸ ê²½ê¸° ì¼ì • ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return f"ë¦¬ê·¸ ê²½ê¸° ì¼ì •ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ğŸ˜“\n\në” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"

# ì´ˆê¸°í™”
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
client = Client()
weather_api = WeatherAPI()
sports_api = SportsAPI(api_key="3")  # TheSportsDB API í‚¤ ì„¤ì • (ì˜ˆì‹œ)
naver_request_count = 0
NAVER_DAILY_LIMIT = 25000
st.set_page_config(page_title="AI ì±—ë´‡", page_icon="ğŸ¤–")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” í•¨ìˆ˜
def init_session_state():
    if "is_logged_in" not in st.session_state:
        st.session_state.is_logged_in = False
    if "user_id" not in st.session_state:
        st.session_state.user_id = None
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    if "session_id" not in st.session_state:
        st.session_state.session_id = str(uuid.uuid4())

# ë„ì‹œ ì¶”ì¶œ
CITY_PATTERNS = [
    re.compile(r'(?:ì˜¤ëŠ˜|ë‚´ì¼|ëª¨ë ˆ|ì´ë²ˆ ì£¼|ì£¼ê°„)?\s*([ê°€-í£a-zA-Z\s]{2,20}(?:ì‹œ|êµ°|city)?)ì˜?\s*ë‚ ì”¨', re.IGNORECASE),
    re.compile(r'(?:ì˜¤ëŠ˜|ë‚´ì¼|ëª¨ë ˆ|ì´ë²ˆ ì£¼|ì£¼ê°„)?\s*([ê°€-í£a-zA-Z\s]{2,20}(?:ì‹œ|êµ°|city)?)\s*ë‚ ì”¨', re.IGNORECASE),
    re.compile(r'ë‚ ì”¨\s*(?:ì˜¤ëŠ˜|ë‚´ì¼|ëª¨ë ˆ|ì´ë²ˆ ì£¼|ì£¼ê°„)?\s*([ê°€-í£a-zA-Z\s]{2,20}(?:ì‹œ|êµ°|city)?)', re.IGNORECASE),
]
def extract_city_from_query(query):
    for pattern in CITY_PATTERNS:
        match = pattern.search(query)
        if match:
            city = match.group(1).strip()
            if city not in ["ì˜¤ëŠ˜", "ë‚´ì¼", "ëª¨ë ˆ", "ì´ë²ˆ ì£¼", "ì£¼ê°„", "í˜„ì¬"]:
                return city
    return "ì„œìš¸"

# íŒ€ ë° ë¦¬ê·¸ ì¶”ì¶œ
SPORTS_PATTERNS = [
    re.compile(r'([ê°€-í£a-zA-Z\s]{2,20}(?:ì‹œ|íŒ€)?)\s*(?:[0-9]{1,2}ì›”)?\s*(?:ê²½ê¸°ì¼ì •|ê²½ê¸° ì¼ì •|ìŠ¤ì¼€ì¤„|ì¼ì •)', re.IGNORECASE),
    re.compile(r'(?:[0-9]{1,2}ì›”)?\s*([ê°€-í£a-zA-Z\s]{2,20}(?:ì‹œ|íŒ€)?)\s*(?:ê²½ê¸°ì¼ì •|ê²½ê¸° ì¼ì •|ìŠ¤ì¼€ì¤„|ì¼ì •)', re.IGNORECASE),
]
MONTH_PATTERN = re.compile(r'([0-9]{1,2})ì›”')

LEAGUE_MAPPING = {
    "epl": {"name": "English Premier League", "id": 4328},
    "bundesliga": {"name": "German Bundesliga", "id": 4331},
    "serie a": {"name": "Italian Serie A", "id": 4332},
    "seriea": {"name": "Italian Serie A", "id": 4332},
    "ligue 1": {"name": "French Ligue 1", "id": 4334},
    "ligue1": {"name": "French Ligue 1", "id": 4334},
    "europa league": {"name": "UEFA Europa League", "id": 4480},
    "europaleague": {"name": "UEFA Europa League", "id": 4480},
    "k league 1": {"name": "South Korean K League 1", "id": 4356},
    "kleague1": {"name": "South Korean K League 1", "id": 4356},
    "afc champions league elite": {"name": "AFC Champions League Elite", "id": 4517},
    "afcchampionsleagueelite": {"name": "AFC Champions League Elite", "id": 4517},
}

def extract_team_from_query(query):
    for pattern in SPORTS_PATTERNS:
        match = pattern.search(query)
        if match:
            team = match.group(1).strip()
            return team
    return None

def extract_league_from_query(query):
    query_lower = query.strip().lower()
    for league_key in LEAGUE_MAPPING.keys():
        if league_key in query_lower:
            return league_key
    return None

def extract_month_from_query(query):
    match = MONTH_PATTERN.search(query)
    if match:
        return int(match.group(1))
    return datetime.now().month

TIME_CITY_PATTERNS = [
    re.compile(r'([ê°€-í£a-zA-Z]{2,20}(?:ì‹œ|êµ°)?)ì˜?\s*ì‹œê°„'),
    re.compile(r'([ê°€-í£a-zA-Z]{2,20}(?:ì‹œ|êµ°)?)\s*ì‹œê°„'),
    re.compile(r'ì‹œê°„\s*([ê°€-í£a-zA-Z\s]{2,20}(?:ì‹œ|êµ°)?)'),
]
def extract_city_from_time_query(query):
    for pattern in TIME_CITY_PATTERNS:
        match = pattern.search(query)
        if match:
            city = match.group(1).strip()
            if city != "í˜„ì¬":
                return city
    return "ì„œìš¸"

# ì‹œê°„ ì •ë³´
def get_time_by_city(city_name="ì„œìš¸"):
    city_info = weather_api.get_city_info(city_name)
    if not city_info:
        return f"'{city_name}'ì˜ ì‹œê°„ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    tf = TimezoneFinder()
    timezone_str = tf.timezone_at(lng=city_info["lon"], lat=city_info["lat"]) or "Asia/Seoul"
    timezone = pytz.timezone(timezone_str)
    city_time = datetime.now(timezone)
    am_pm = "ì˜¤ì „" if city_time.strftime("%p") == "AM" else "ì˜¤í›„"
    return f"í˜„ì¬ {city_name} ì‹œê°„: {city_time.strftime('%Yë…„ %mì›” %dì¼ %p %I:%M')} â°\në” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"

# ì‚¬ìš©ì ë° ì±„íŒ… ê¸°ë¡ ê´€ë¦¬
def create_or_get_user(nickname):
    user = supabase.table("users").select("*").eq("nickname", nickname).execute()
    if user.data:
        return user.data[0]["id"], True
    new_user = supabase.table("users").insert({"nickname": nickname, "created_at": datetime.now().isoformat()}).execute()
    return new_user.data[0]["id"], False

def save_chat_history(user_id, session_id, question, answer, time_taken):
    supabase.table("chat_history").insert({
        "user_id": user_id, "session_id": session_id, "question": question,
        "answer": answer, "time_taken": time_taken, "created_at": datetime.now().isoformat()
    }).execute()

def async_save_chat_history(user_id, session_id, question, answer, time_taken):
    threading.Thread(target=save_chat_history, args=(user_id, session_id, question, answer, time_taken)).start()

# ì˜ì•½í’ˆ ê²€ìƒ‰
def get_drug_info(drug_query):
    drug_name = drug_query.replace("ì•½í’ˆê²€ìƒ‰", "").strip()
    cache_key = f"drug:{drug_name}"
    cached = cache_handler.get(cache_key)
    if cached:
        return cached
    
    url = 'http://apis.data.go.kr/1471000/DrbEasyDrugInfoService/getDrbEasyDrugList'
    params = {'serviceKey': DRUG_API_KEY, 'pageNo': '1', 'numOfRows': '1', 'itemName': urllib.parse.quote(drug_name), 'type': 'json'}
    try:
        response = requests.get(url, params=params, timeout=5)
        response.raise_for_status()
        data = response.json()
        if 'body' in data and 'items' in data['body'] and data['body']['items']:
            item = data['body']['items'][0]
            efcy = item.get('efcyQesitm', 'ì •ë³´ ì—†ìŒ')[:150] + ("..." if len(item.get('efcyQesitm', '')) > 150 else "")
            use_method_raw = item.get('useMethodQesitm', 'ì •ë³´ ì—†ìŒ')
            atpn = item.get('atpnQesitm', 'ì •ë³´ ì—†ìŒ')[:150] + ("..." if len(item.get('atpnQesitm', '')) > 150 else "")
            use_method = use_method_raw[:150] + ("..." if len(use_method_raw) > 150 else "")
            
            result = (
                f"ğŸ’Š **ì˜ì•½í’ˆ ì •ë³´** ğŸ’Š\n\n"
                f"âœ… **ì•½í’ˆëª…**: {item.get('itemName', 'ì •ë³´ ì—†ìŒ')}\n\n"
                f"âœ… **ì œì¡°ì‚¬**: {item.get('entpName', 'ì •ë³´ ì—†ìŒ')}\n\n"
                f"âœ… **íš¨ëŠ¥**: {efcy}\n\n"
                f"âœ… **ìš©ë²•ìš©ëŸ‰**: {use_method}\n\n"
                f"âœ… **ì£¼ì˜ì‚¬í•­**: {atpn}\n\n"
                f"â„¹ï¸ ìì„¸í•œ ì •ë³´ëŠ” [ì•½í•™ì •ë³´ì›](https://www.health.kr/searchDrug/search_detail.asp)ì—ì„œ í™•ì¸í•˜ì„¸ìš”! ğŸ©º\n\n"
                f"ë” ê¶ê¸ˆí•œ ì  ìˆìœ¼ì‹ ê°€ìš”? ğŸ˜Š"
            )
            cache_handler.setex(cache_key, 86400, result)
            return result
        else:
            return search_and_summarize_drug(drug_name)
    except Exception as e:
        logger.error(f"ì•½í’ˆ API ì˜¤ë¥˜: {str(e)}")
        return search_and_summarize_drug(drug_name)

def search_and_summarize_drug(drug_name):
    search_results = search_and_summarize(f"{drug_name} ì˜ì•½í’ˆ ì •ë³´", num_results=5)
    if not search_results.empty:
        return f"'{drug_name}' ê³µì‹ ì •ë³´ ì—†ìŒ. ì›¹ ê²€ìƒ‰ ìš”ì•½:\n{get_ai_summary(search_results)}"
    return f"'{drug_name}' ì˜ì•½í’ˆ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

# Naver API ë° ì›¹ ê²€ìƒ‰
def get_naver_api_results(query):
    global naver_request_count
    if naver_request_count >= NAVER_DAILY_LIMIT:
        return search_and_summarize(query, num_results=5)
    enc_text = urllib.parse.quote(query)
    url = f"https://openapi.naver.com/v1/search/webkr?query={enc_text}&display=10&sort=date"
    request = urllib.request.Request(url)
    request.add_header("X-Naver-Client-Id", NAVER_CLIENT_ID)
    request.add_header("X-Naver-Client-Secret", NAVER_CLIENT_SECRET)
    results = []
    try:
        response = urllib.request.urlopen(request)
        naver_request_count += 1
        if response.getcode() == 200:
            data = json.loads(response.read().decode('utf-8'))
            for item in data.get('items', [])[:5]:
                title = re.sub(r'<b>|</b>', '', item['title'])
                contents = re.sub(r'<b>|</b>', '', item.get('description', 'ë‚´ìš© ì—†ìŒ'))[:100] + "..."
                results.append({"title": title, "contents": contents, "url": item.get('link', ''), "date": item.get('pubDate', '')})
    except Exception:
        return search_and_summarize(query, num_results=5)
    return pd.DataFrame(results)

def search_and_summarize(query, num_results=5):
    data = []
    with ThreadPoolExecutor() as executor:
        futures = [executor.submit(requests.get, link, timeout=5) for link in search(query, num_results=num_results)]
        for future in futures:
            try:
                response = future.result()
                soup = BeautifulSoup(response.text, 'html.parser')
                title = soup.title.get_text() if soup.title else "No title"
                description = ' '.join([p.get_text() for p in soup.find_all('p')[:3]])
                link = response.url if response.url else None
                if not link:
                    logger.warning(f"ë§í¬ ëˆ„ë½ ë°œìƒ: query={query}, title={title}, url=ì—†ìŒ")
                data.append({"title": title, "contents": description[:500], "link": link})
            except Exception as e:
                logger.error(f"ê²€ìƒ‰ ìš”ì²­ ì‹¤íŒ¨: query={query}, error={str(e)}")
                continue
    df = pd.DataFrame(data)
    if df.empty or 'link' not in df.columns or df['link'].isnull().all():
        logger.warning(f"ê²€ìƒ‰ ê²°ê³¼ ë¹„ì–´ìˆìŒ ë˜ëŠ” ëª¨ë“  ë§í¬ ëˆ„ë½: query={query}, data={data}")
    return df

def get_ai_summary(search_results):
    if search_results.empty:
        return "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    context = "\n".join([f"ì¶œì²˜: {row['title']}\në‚´ìš©: {row['contents']}" for _, row in search_results.iterrows()])
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": f"ê²€ìƒ‰ ê²°ê³¼ë¥¼ 2~3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½:\n{context}"}]
    )
    summary = response.choices[0].message.content
    sources = "\n\nğŸ“œ **ì¶œì²˜**\n" + "\n".join(
        [f"ğŸŒ [{row['title']}]({row.get('link', 'ë§í¬ ì—†ìŒ')})" 
         for _, row in search_results.iterrows()]
    )
    return f"{summary}{sources}\n\në” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"

# ë…¼ë¬¸ ê²€ìƒ‰ (ArXiv)
def fetch_arxiv_paper(paper):
    return {
        "title": paper.title,
        "authors": ", ".join(str(a) for a in paper.authors),
        "summary": paper.summary[:200],
        "entry_id": paper.entry_id,
        "pdf_url": paper.pdf_url,
        "published": paper.published.strftime('%Y-%m-%d')
    }

def get_arxiv_papers(query, max_results=3):
    cache_key = f"arxiv:{query}:{max_results}"
    cached = cache_handler.get(cache_key)
    if cached:
        return cached
    search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.SubmittedDate)
    with ThreadPoolExecutor() as executor:
        results = list(executor.map(fetch_arxiv_paper, search.results()))
    if not results:
        return "í•´ë‹¹ í‚¤ì›Œë“œë¡œ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    response = "ğŸ“š **Arxiv ë…¼ë¬¸ ê²€ìƒ‰ ê²°ê³¼** ğŸ“š\n\n"
    response += "\n\n".join(
        [f"**ë…¼ë¬¸ {i}**\n\n"
         f"ğŸ“„ **ì œëª©**: {r['title']}\n\n"
         f"ğŸ‘¥ **ì €ì**: {r['authors']}\n\n"
         f"ğŸ“ **ì´ˆë¡**: {r['summary']}...\n\n"
         f"ğŸ”— **ë…¼ë¬¸ í˜ì´ì§€**: {r['entry_id']}\n\n"
         f"ğŸ“¥ **PDF ë‹¤ìš´ë¡œë“œ**: [{r['pdf_url'].split('/')[-1]}]({r['pdf_url']})\n\n"
         f"ğŸ“… **ì¶œíŒì¼**: {r['published']}\n\n"
         f"{'-' * 50}"
         for i, r in enumerate(results, 1)]
    ) + "\n\në” ë§ì€ ë…¼ë¬¸ì„ ë³´ê³  ì‹¶ë‹¤ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”! ğŸ˜Š"
    cache_handler.setex(cache_key, 3600, response)
    return response

# PubMed ë…¼ë¬¸ ê²€ìƒ‰
base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/"

def search_pubmed(query, max_results=5):
    search_url = f"{base_url}esearch.fcgi"
    params = {
        "db": "pubmed",
        "term": query,
        "retmode": "json",
        "retmax": max_results,
        "api_key": NCBI_KEY
    }
    response = requests.get(search_url, params=params)
    return response.json()

def get_pubmed_summaries(id_list):
    summary_url = f"{base_url}esummary.fcgi"
    ids = ",".join(id_list)
    params = {
        "db": "pubmed",
        "id": ids,
        "retmode": "json",
        "api_key": NCBI_KEY
    }
    response = requests.get(summary_url, params=params)
    return response.json()

def get_pubmed_abstract(id_list):
    fetch_url = f"{base_url}efetch.fcgi"
    ids = ",".join(id_list)
    params = {
        "db": "pubmed",
        "id": ids,
        "retmode": "xml",
        "rettype": "abstract",
        "api_key": NCBI_KEY
    }
    response = requests.get(fetch_url, params=params)
    return response.text

def extract_first_two_sentences(abstract_text):
    if not abstract_text or abstract_text.isspace():
        return "No abstract available"
    sentences = [s.strip() for s in abstract_text.split('.') if s.strip()]
    return " ".join(sentences[:2]) + "." if sentences else "No abstract available"

def parse_abstracts(xml_text):
    abstract_dict = {}
    try:
        root = ET.fromstring(xml_text)
        for article in root.findall(".//PubmedArticle"):
            pmid_elem = article.find(".//MedlineCitation/PMID")
            abstract_elem = article.find(".//Abstract/AbstractText")
            pmid = pmid_elem.text if pmid_elem is not None else None
            abstract = abstract_elem.text if abstract_elem is not None else "No abstract available"
            if pmid:
                abstract_dict[pmid] = extract_first_two_sentences(abstract)
    except ET.ParseError as e:
        logger.error(f"XML íŒŒì‹± ì˜¤ë¥˜ ë°œìƒ: {e}")
    return abstract_dict

def get_pubmed_papers(query, max_results=5):
    cache_key = f"pubmed:{query}:{max_results}"
    cached = cache_handler.get(cache_key)
    if cached:
        return cached
    
    search_results = search_pubmed(query, max_results)
    pubmed_ids = search_results["esearchresult"]["idlist"]
    if not pubmed_ids:
        return "í•´ë‹¹ í‚¤ì›Œë“œë¡œ ì˜í•™ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    summaries = get_pubmed_summaries(pubmed_ids)
    abstracts_xml = get_pubmed_abstract(pubmed_ids)
    abstract_dict = parse_abstracts(abstracts_xml)
    
    response = "ğŸ“š **PubMed ì˜í•™ ë…¼ë¬¸ ê²€ìƒ‰ ê²°ê³¼** ğŸ“š\n\n"
    response += "\n\n".join(
        [f"**ë…¼ë¬¸ {i}**\n\n"
         f"ğŸ†” **PMID**: {pmid}\n\n"
         f"ğŸ“– **ì œëª©**: {summaries['result'][pmid].get('title', 'No title available')}\n\n"
         f"ğŸ“… **ì¶œíŒì¼**: {summaries['result'][pmid].get('pubdate', 'No date available')}\n\n"
         f"âœï¸ **ì €ì**: {', '.join([author.get('name', '') for author in summaries['result'][pmid].get('authors', [])])}\n\n"
         f"ğŸ”— **ë§í¬**: {'https://doi.org/' + next((aid['value'] for aid in summaries['result'][pmid].get('articleids', []) if aid['idtype'] == 'doi'), None) if next((aid['value'] for aid in summaries['result'][pmid].get('articleids', []) if aid['idtype'] == 'doi'), None) else f'https://pubmed.ncbi.nlm.nih.gov/{pmid}/'}\n\n"
         f"ğŸ“ **ì´ˆë¡**: {abstract_dict.get(pmid, 'No abstract available')}\n\n"
         f"{'-' * 50}"
         for i, pmid in enumerate(pubmed_ids, 1)]
    ) + "\n\në” ë§ì€ ì˜í•™ ë…¼ë¬¸ì„ ë³´ê³  ì‹¶ë‹¤ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”! ğŸ˜Š"
    cache_handler.setex(cache_key, 3600, response)
    return response

# ëŒ€í™”í˜• ì‘ë‹µ
conversation_cache = MemoryCache()
def get_conversational_response(query, chat_history):
    cache_key = f"conv:{needs_search(query)}:{query}:{hash(str(chat_history[-5:]))}"
    cached = conversation_cache.get(cache_key)
    if cached:
        return cached
    
    emoji_list = (
        "ì²´í¬ë¦¬ìŠ¤íŠ¸ ì´ëª¨ì§€:\n"
        "âœ… ì™„ë£Œëœ í•­ëª© | â˜‘ï¸ ì²´í¬ ìƒì (ì²´í¬ë¨) | âœ“ ì²´í¬ í‘œì‹œ | âœ”ï¸ êµµì€ ì²´í¬ í‘œì‹œ | âŒ ì·¨ì†Œ/ì‹¤íŒ¨ í•­ëª© | â¬œ ë¹ˆ ìƒì | âšª ë¹ˆ ì› | ğŸ”˜ ë¼ë””ì˜¤ ë²„íŠ¼ | ğŸ“Œ í•€ìœ¼ë¡œ ê³ ì •ëœ í•­ëª© | ğŸš€ ì‹œì‘/ì¶œì‹œ í•­ëª©\n\n"
        "ìƒíƒœ í‘œì‹œ ì´ëª¨ì§€:\n"
        "ğŸŸ¢ ë…¹ìƒ‰ ì› (ì„±ê³µ/í™œì„±) | ğŸ”´ ë¹¨ê°„ ì› (ì‹¤íŒ¨/ì¤‘ìš”) | ğŸŸ¡ ë…¸ë€ ì› (ì£¼ì˜/ì§„í–‰ ì¤‘) | ğŸ”„ ì—…ë°ì´íŠ¸/ì§„í–‰ ì¤‘ | â±ï¸ ëŒ€ê¸° ì¤‘/ì‹œê°„ ê´€ë ¨ | ğŸ” ê²€í†  ì¤‘/ê²€ìƒ‰\n\n"
        "ê°œë°œ ê´€ë ¨ ì´ëª¨ì§€:\n"
        "ğŸ’» ì½”ë“œ/í”„ë¡œê·¸ë˜ë° | ğŸ”§ ë„êµ¬/ì„¤ì • | ğŸ› ë²„ê·¸ | ğŸ“¦ íŒ¨í‚¤ì§€/ëª¨ë“ˆ | ğŸ“ ë¬¸ì„œ/ë…¸íŠ¸ | ğŸ—‚ï¸ í´ë”/ë¶„ë¥˜ | âš™ï¸ ì„¤ì •/êµ¬ì„± | ğŸ”’ ë³´ì•ˆ/ì ê¸ˆ | ğŸ“Š ë°ì´í„°/í†µê³„ | ğŸ“ˆ ì„±ì¥/ì¦ê°€\n\n"
        "ì„¹ì…˜ êµ¬ë¶„ ì´ëª¨ì§€:\n"
        "ğŸ“‹ ëª©ë¡/ì²´í¬ë¦¬ìŠ¤íŠ¸ | ğŸ“š ì±…/ë¬¸ì„œ | ğŸ’¡ ì•„ì´ë””ì–´/íŒ | âš ï¸ ì£¼ì˜/ê²½ê³  | ğŸ¯ ëª©í‘œ/íƒ€ê²Ÿ | ğŸ”— ë§í¬/ì—°ê²° | ğŸ‘¥ ì‚¬ìš©ì/íŒ€ | ğŸ“… ì¼ì •/ìº˜ë¦°ë”\n\n"
        "ê¸°íƒ€ ìœ ìš©í•œ ì´ëª¨ì§€:\n"
        "ğŸŒŸ í•˜ì´ë¼ì´íŠ¸/ì¤‘ìš” í•­ëª© | âœ¨ íŠ¹ë³„/ê°œì„  í•­ëª© | ğŸ“± ëª¨ë°”ì¼ | ğŸ–¥ï¸ ë°ìŠ¤í¬í†± | ğŸ—ï¸ ì•„í‚¤í…ì²˜ | ğŸš§ ì‘ì—… ì¤‘ | ğŸ’¬ ì˜ê²¬/ì½”ë©˜íŠ¸ | ğŸŒ ì›¹/ê¸€ë¡œë²Œ | ğŸ“¤ ë°°í¬/ì—…ë¡œë“œ | ğŸ“¥ ë‹¤ìš´ë¡œë“œ/ìˆ˜ì‹ "
    )
    
    messages = [
        {"role": "system", "content": (
            "ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ìƒí˜¸ì‘ìš©ì ì¸ AI ì±—ë´‡ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µí•˜ê³ , í•„ìš”í•˜ë©´ ì¶”ê°€ ì§ˆë¬¸ì„ ë˜ì ¸ ëŒ€í™”ë¥¼ ì´ì–´ê°€ì„¸ìš”. "
            "ì‘ë‹µ ë‚´ìš©ì— ë”°ë¼ ì ì ˆí•œ ì´ëª¨ì§€ë¥¼ ì•„ë˜ ëª©ë¡ì—ì„œ ì„ íƒí•´ ìì—°ìŠ¤ëŸ½ê²Œ ì‚½ì…í•˜ì„¸ìš”. ì´ëª¨ì§€ëŠ” ë¬¸ë§¥ì— ë§ê²Œ ì‚¬ìš©í•˜ê³ , ê³¼ë„í•˜ê²Œ ë§ì§€ ì•Šë„ë¡ í•˜ì„¸ìš”:\n\n" + emoji_list
        )}] + [
        {"role": msg["role"], "content": msg["content"]} for msg in chat_history[-5:]
    ] + [{"role": "user", "content": query}]
    
    response = client.chat.completions.create(model="gpt-4", messages=messages)
    result = response.choices[0].message.content
    conversation_cache.setex(cache_key, 600, result)
    return result

GREETING_RESPONSES = {
    "ì•ˆë…•": "ì•ˆë…•í•˜ì„¸ìš”! ë°˜ê°‘ìŠµë‹ˆë‹¤! ğŸ˜Š",
    "ì•ˆë…• ë°˜ê°€ì›Œ": "ì•ˆë…•í•˜ì„¸ìš”! ì €ë„ ë°˜ê°‘ìŠµë‹ˆë‹¤! ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì–´ë– ì‹ ê°€ìš”? ğŸ˜„",
    "í•˜ì´": "í•˜ì´! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ğŸ˜Š",
    "í—¬ë¡œ": "ì•ˆë…•í•˜ì„¸ìš”! ë°˜ê°‘ìŠµë‹ˆë‹¤! ğŸ˜Š",
    "í—¤ì´": "í—¤ì´! ì˜ ì§€ë‚´ì„¸ìš”? ğŸ˜„",
    "ì™“ì—…": "ì™“ì—…! ë­í•˜ê³  ê³„ì‹ ê°€ìš”? ğŸ˜Š",
    "ì™“ì¹": "ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì–´ë– ì‹ ê°€ìš”? ğŸ˜„",
}

# ì¿¼ë¦¬ ë¶„ë¥˜
def needs_search(query):
    query_lower = query.strip().lower()
    greeting_keywords = ["ì•ˆë…•", "í•˜ì´", "ë°˜ê°€ì›Œ", "ì•ˆë‡½", "ë­í•´", "í—¬ë¡œ", "í—¬ë¡±", "í•˜ì‡", "í—¤ì´", "í—¤ì´ìš”", "ì™“ì—…", "ì™“ì¹", "ì—ì´ìš”"]
    emotion_keywords = ["ë°°ê³ í”„ë‹¤", "ë°°ê³ í”„", "ì¡¸ë¦¬ë‹¤", "í”¼ê³¤í•˜ë‹¤", "í™”ë‚¨", "ì—´ë°›ìŒ", "ì§œì¦ë‚¨", "í”¼ê³¤í•¨"]
    if any(greeting in query_lower for greeting in greeting_keywords) or \
       any(emo in query_lower for emo in emotion_keywords) or \
       len(query_lower) <= 3 and "?" not in query_lower:
        return "conversation"
    intent_keywords = ["ì¶”ì²œí•´ì¤˜", "ë­ ë¨¹ì„ê¹Œ", "ë©”ë‰´", "ë­í• ê¹Œ"]
    if any(kw in query_lower for kw in intent_keywords):
        return "conversation"
    time_keywords = ["í˜„ì¬ ì‹œê°„", "ì‹œê°„", "ëª‡ ì‹œ", "ì§€ê¸ˆ", "ëª‡ì‹œ", "ëª‡ ì‹œì•¼", "ì§€ê¸ˆ ì‹œê°„", "í˜„ì¬", "ì‹œê³„"]
    if any(keyword in query_lower for keyword in time_keywords) and \
       any(timeword in query_lower for timeword in ["ì‹œê°„", "ëª‡ì‹œ", "ëª‡ ì‹œ", "ì‹œê³„"]):
        return "time"
    weather_keywords = ["ë‚ ì”¨", "ì˜¨ë„", "ê¸°ì˜¨"]
    if any(keyword in query_lower for keyword in weather_keywords) and "ë‚´ì¼" in query_lower:
        return "tomorrow_weather"
    elif any(keyword in query_lower for keyword in weather_keywords) and "ëª¨ë ˆ" in query_lower:
        return "day_after_tomorrow_weather"
    elif any(keyword in query_lower for keyword in weather_keywords) and ("ì£¼ê°„" in query_lower or any(kw in query_lower for kw in ["ì´ë²ˆ ì£¼", "ì£¼ê°„ ì˜ˆë³´", "ì£¼ê°„ ë‚ ì”¨"])):
        return "weekly_forecast"
    elif any(keyword in query_lower for keyword in weather_keywords):
        return "weather"
    drug_keywords = ["ì•½í’ˆê²€ìƒ‰"]
    drug_pattern = r'^ì•½í’ˆê²€ìƒ‰\s+[ê°€-í£a-zA-Z]{2,10}(?:ì•½|ì •|ì‹œëŸ½|ìº¡ìŠ)?$'
    if any(keyword in query_lower for keyword in drug_keywords) and re.match(drug_pattern, query_lower):
        return "drug"
    if query_lower == "mbti ê²€ì‚¬":
        return "mbti"
    if query_lower == "ë‹¤ì¤‘ì§€ëŠ¥ ê²€ì‚¬":
        return "multi_iq"
    arxiv_keywords = ["ë…¼ë¬¸ê²€ìƒ‰", "arxiv", "paper", "research"]
    if any(kw in query_lower for kw in arxiv_keywords) and len(query_lower) > 5:
        return "arxiv_search"
    pubmed_keywords = ["ì˜í•™ë…¼ë¬¸"]
    if any(kw in query_lower for kw in pubmed_keywords) and len(query_lower) > 5:
        return "pubmed_search"
    sports_keywords = ["ê²½ê¸°ì¼ì •", "ê²½ê¸° ì¼ì •", "ìŠ¤ì¼€ì¤„", "ì¼ì •"]
    league_keywords = list(LEAGUE_MAPPING.keys())
    if any(kw in query_lower for kw in sports_keywords):
        for league in league_keywords:
            if league in query_lower:
                return "league_schedule"
        for pattern in SPORTS_PATTERNS:
            match = pattern.search(query)
            if match:
                return "sports_schedule"
    search_keywords = ["ê²€ìƒ‰", "ì•Œë ¤ì¤˜", "ì •ë³´", "ë­ì•¼", "ë¬´ì—‡ì´ì•¼", "ë¬´ì—‡ì¸ì§€", "ì°¾ì•„ì„œ", "ì •ë¦¬í•´ì¤˜", "ì„¤ëª…í•´ì¤˜", "ì•Œê³ ì‹¶ì–´", "ì•Œë ¤ì¤„ë˜","ì•Œì•„","ë­ëƒ", "ì•Œë ¤ì¤˜", "ì°¾ì•„ì¤˜"]
    if any(kw in query_lower for kw in search_keywords) and len(query_lower) > 5:
        return "web_search"
    return "general_query"

# UI í•¨ìˆ˜
def show_login_page():
    st.title("ë¡œê·¸ì¸ ğŸ¤—")
    with st.form("login_form"):
        nickname = st.text_input("ë‹‰ë„¤ì„", placeholder="ì˜ˆ: í›„ì•ˆ")
        submit_button = st.form_submit_button("ì‹œì‘í•˜ê¸° ğŸš€")
        
        if submit_button:
            if nickname:
                try:
                    user_id, is_existing = create_or_get_user(nickname)
                    st.session_state.user_id = user_id
                    st.session_state.is_logged_in = True
                    st.session_state.chat_history = []
                    st.session_state.session_id = str(uuid.uuid4())
                    
                    if is_existing:
                        st.toast(f"í™˜ì˜í•©ë‹ˆë‹¤, {nickname}ë‹˜! ğŸ‰")
                    else:
                        st.toast(f"ìƒˆë¡œìš´ ì‚¬ìš©ìë¡œ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤. í™˜ì˜í•©ë‹ˆë‹¤, {nickname}ë‹˜! ğŸ‰")
                    time.sleep(1)
                    st.rerun()
                except Exception as e:
                    st.toast(f"ë¡œê·¸ì¸ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.", icon="âŒ")
            else:
                st.toast("ë‹‰ë„¤ì„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", icon="âš ï¸")

@st.cache_data(ttl=600)
def get_cached_response(query):
    return process_query(query)

def process_query(query):
    init_session_state()
    query_type = needs_search(query)
    if query_type == "mbti":
        return (
            "MBTI ê²€ì‚¬ë¥¼ ì›í•˜ì‹œë‚˜ìš”? âœ¨ ì•„ë˜ ì‚¬ì´íŠ¸ì—ì„œ ë¬´ë£Œë¡œ ì„±ê²© ìœ í˜• ê²€ì‚¬ë¥¼ í•  ìˆ˜ ìˆì–´ìš”! ğŸ˜Š\n"
            "[16Personalities MBTI ê²€ì‚¬](https://www.16personalities.com/ko/%EB%AC%B4%EB%A3%8C-%EC%84%B1%EA%B2%A9-%EC%9C%A0%ED%98%95-%EA%B2%80%EC%82%AC) ğŸŒŸ\n"
            "ì´ ì‚¬ì´íŠ¸ëŠ” 16ê°€ì§€ ì„±ê²© ìœ í˜•ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ í…ŒìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ë©°, ê²°ê³¼ì— ë”°ë¼ ì„±ê²© ì„¤ëª…ê³¼ ì¸ê°„ê´€ê³„ ì¡°ì–¸ ë“±ì„ í™•ì¸í•  ìˆ˜ ìˆì–´ìš”! ğŸ§ ğŸ’¡"
        )
    elif query_type == "multi_iq":
        return (
            "ë‹¤ì¤‘ì§€ëŠ¥ ê²€ì‚¬ë¥¼ ì›í•˜ì‹œë‚˜ìš”? ğŸ‰ ì•„ë˜ ì‚¬ì´íŠ¸ì—ì„œ ë¬´ë£Œë¡œ ë‹¤ì¤‘ì§€ëŠ¥ í…ŒìŠ¤íŠ¸ë¥¼ í•´ë³¼ ìˆ˜ ìˆì–´ìš”! ğŸ˜„\n"
            "[Multi IQ Test](https://multiiqtest.com/) ğŸš€\n"
            "ì´ ì‚¬ì´íŠ¸ëŠ” í•˜ì›Œë“œ ê°€ë“œë„ˆì˜ ë‹¤ì¤‘ì§€ëŠ¥ ì´ë¡ ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ í…ŒìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ë©°, ë‹¤ì–‘í•œ ì§€ëŠ¥ ì˜ì—­ì„ í‰ê°€í•´ì¤ë‹ˆë‹¤! ğŸ“šâœ¨"
        )
    elif query_type == "time":
        city = extract_city_from_time_query(query)
        return get_time_by_city(city)
    elif query_type == "weather":
        city = extract_city_from_query(query)
        return weather_api.get_city_weather(city)
    elif query_type == "tomorrow_weather":
        city = extract_city_from_query(query)
        return weather_api.get_forecast_by_day(city, days_from_today=1)
    elif query_type == "day_after_tomorrow_weather":
        city = extract_city_from_query(query)
        return weather_api.get_forecast_by_day(city, days_from_today=2)
    elif query_type == "weekly_forecast":
        city = extract_city_from_query(query)
        return weather_api.get_weekly_forecast(city)
    elif query_type == "drug":
        return get_drug_info(query)
    elif query_type == "sports_schedule":
        team = extract_team_from_query(query)
        month = extract_month_from_query(query)
        return sports_api.fetch_team_schedule(team, month)
    elif query_type == "league_schedule":
        league_key = extract_league_from_query(query)
        month = extract_month_from_query(query)
        return sports_api.fetch_league_schedule(league_key, month)
    elif query_type == "conversation":
        if query.strip() in GREETING_RESPONSES:
            return GREETING_RESPONSES[query.strip()]
        return get_conversational_response(query, st.session_state.chat_history)
    elif query_type == "web_search":
        language = detect(query)
        if language == 'ko' and naver_request_count < NAVER_DAILY_LIMIT:
            return get_ai_summary(get_naver_api_results(query))
        return get_ai_summary(search_and_summarize(query))
    elif query_type == "arxiv_search":
        keywords = query.replace("ë…¼ë¬¸ê²€ìƒ‰", "").replace("arxiv", "").replace("paper", "").replace("research", "").strip()
        return get_arxiv_papers(keywords)
    elif query_type == "pubmed_search":
        keywords = query.replace("ì˜í•™ë…¼ë¬¸", "").strip()
        return get_pubmed_papers(keywords)
    elif query_type == "general_query":
        return get_conversational_response(query, st.session_state.chat_history)
    return "ì•„ì§ ì§€ì›í•˜ì§€ ì•ŠëŠ” ê¸°ëŠ¥ì´ì—ìš”. ğŸ˜…"

def handle_error(e, context="ì‘ì—… ì¤‘", user_friendly_msg="ì‘ë‹µì„ ì¤€ë¹„í•˜ë‹¤ ë¬¸ì œê°€ ìƒê²¼ì–´ìš”. ğŸ˜“"):
    logger.error(f"{context} ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
    return user_friendly_msg

def show_chat_dashboard():
    st.title("AI ì±—ë´‡ ğŸ¤–")
    
    init_session_state()
    
    if st.button("ë„ì›€ë§ â„¹ï¸"):
        st.info(
            "ì±—ë´‡ê³¼ ë” ì‰½ê²Œ ëŒ€í™”í•˜ëŠ” ë°©ë²•ì´ì—ìš”! ğŸ‘‡:\n\n"
            "1. **ì•½í’ˆê²€ìƒ‰** ğŸ’Š: 'ì•½í’ˆê²€ìƒ‰ [ì•½ ì´ë¦„]' (ì˜ˆ: ì•½í’ˆê²€ìƒ‰ íƒ€ì´ë ˆë†€ì •)\n"
            "2. **ë…¼ë¬¸ê²€ìƒ‰ (ArXiv)** ğŸ“š: 'ë…¼ë¬¸ê²€ìƒ‰ [í‚¤ì›Œë“œ]' (ì˜ˆ: ë…¼ë¬¸ê²€ìƒ‰ machine learning)\n"
            "3. **ì˜í•™ë…¼ë¬¸ê²€ìƒ‰ (PubMed)** ğŸ©º: 'ì˜í•™ë…¼ë¬¸ [í‚¤ì›Œë“œ]' (ì˜ˆ: ì˜í•™ë…¼ë¬¸ gene therapy)\n"
            "4. **ë‚ ì”¨ê²€ìƒ‰** â˜€ï¸: '[ë„ì‹œëª…] ë‚ ì”¨' ë˜ëŠ” 'ë‚´ì¼ [ë„ì‹œëª…] ë‚ ì”¨' (ì˜ˆ: ì„œìš¸ ë‚ ì”¨, ë‚´ì¼ ì„œìš¸ ë‚ ì”¨)\n"
            "5. **ì‹œê°„ê²€ìƒ‰** â±ï¸: '[ë„ì‹œëª…] ì‹œê°„' (ì˜ˆ: íŒŒë¦¬ ì‹œê°„, ë‰´ìš• ì‹œê°„)\n"
            "6. **ê²½ê¸°ì¼ì • ê²€ìƒ‰** âš½: \n"
            "   - íŒ€ë³„ ì¼ì •: '[íŒ€ ì´ë¦„] [ì›”] ê²½ê¸°ì¼ì •' (ì˜ˆ: AT ë§ˆë“œë¦¬ë“œ 3ì›” ê²½ê¸°ì¼ì •)\n"
            "   - ë¦¬ê·¸ë³„ ì¼ì •: '[ë¦¬ê·¸ ì´ë¦„] [ì›”] ê²½ê¸°ì¼ì •' (ì˜ˆ: EPL 3ì›” ê²½ê¸°ì¼ì •)\n"
            "   - ì§€ì› ë¦¬ê·¸: EPL, Bundesliga, Serie A, Ligue 1, Europa League, K League 1, AFC Champions League Elite\n\n"
            "ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ ì–¸ì œë“  ì§ˆë¬¸í•´ì£¼ì„¸ìš”! ğŸ˜Š"
        )
    
    for msg in st.session_state.chat_history:
        with st.chat_message(msg['role']):
            st.markdown(msg['content'], unsafe_allow_html=True)
    
    if user_prompt := st.chat_input("ì§ˆë¬¸í•´ ì£¼ì„¸ìš”!"):
        st.chat_message("user").markdown(user_prompt)
        st.session_state.chat_history.append({"role": "user", "content": user_prompt})
        with st.chat_message("assistant"):
            try:
                with st.spinner("ì‘ë‹µì„ ì¤€ë¹„ ì¤‘ì´ì—ìš”.. â³"):
                    start_time = time.time()
                    with ThreadPoolExecutor() as executor:
                        future = executor.submit(get_cached_response, user_prompt)
                        response = future.result()
                    time_taken = round(time.time() - start_time, 2)
                
                st.markdown(response, unsafe_allow_html=True)
                st.session_state.chat_history.append({"role": "assistant", "content": response})
                async_save_chat_history(st.session_state.user_id, st.session_state.session_id, user_prompt, response, time_taken)
            
            except Exception as e:
                error_msg = handle_error(e, "ëŒ€í™” ì²˜ë¦¬ ì¤‘", "ì‘ë‹µì„ ì¤€ë¹„í•˜ë‹¤ ë¬¸ì œê°€ ìƒê²¼ì–´ìš”. ğŸ˜“")
                st.markdown(error_msg, unsafe_allow_html=True)
                st.session_state.chat_history.append({"role": "assistant", "content": error_msg})

# ë©”ì¸ ì‹¤í–‰
def main():
    init_session_state()
    if not st.session_state.is_logged_in:
        show_login_page()
    else:
        show_chat_dashboard()

if __name__ == "__main__":
    main()

# from config.imports import *
# from config.env import *

# # ë¡œê¹… ì„¤ì •
# logging.basicConfig(
#     level=logging.WARNING if os.getenv("ENV") == "production" else logging.INFO,
#     filename="chatbot_errors.log",
#     format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
# )
# logger = logging.getLogger("HybridChat")

# # ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
# MAX_WORKERS = min(multiprocessing.cpu_count() * 2, 8)

# # ìºì‹œ ì„¤ì •
# cache = Cache("cache_directory")

# class MemoryCache:
#     def __init__(self):
#         self.cache = {}
#         self.expiry = {}
    
#     def get(self, key):
#         if key in self.cache and time.time() < self.expiry[key]:
#             return self.cache[key]
#         return cache.get(key)
    
#     def setex(self, key, ttl, value):
#         self.cache[key] = value
#         self.expiry[key] = time.time() + ttl
#         cache.set(key, value, expire=ttl)

# cache_handler = MemoryCache()

# # ê³µí†µ ì—ëŸ¬ ì²˜ë¦¬ í•¨ìˆ˜
# def handle_error(e, context="ì‘ì—… ì¤‘", user_friendly_msg="ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”. ğŸ˜… ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”!"):
#     logger.error(f"{context} ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
#     return f"{user_friendly_msg}\n\nâš ï¸ ì˜¤ë¥˜ ë‚´ìš©: {str(e)}"

# # WeatherAPI í´ë˜ìŠ¤
# class WeatherAPI:
#     def __init__(self, cache_ttl=600):
#         self.cache = cache_handler
#         self.cache_ttl = cache_ttl

#     def fetch_weather(self, url, params):
#         try:
#             session = requests.Session()
#             retry_strategy = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
#             adapter = HTTPAdapter(max_retries=retry_strategy)
#             session.mount("https://", adapter)
#             logger.info(f"ë‚ ì”¨ API ìš”ì²­: {url} with params {params}")
#             response = session.get(url, params=params, timeout=3)
#             response.raise_for_status()
#             return response.json()
#         except requests.RequestException as e:
#             return handle_error(e, "ë‚ ì”¨ API í˜¸ì¶œ ì¤‘", f"ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆì–´ìš”. API í‚¤ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”. ğŸŒ§ï¸")

#     @lru_cache(maxsize=100)
#     def get_city_info(self, city_name):
#         cache_key = f"city_info:{city_name}"
#         cached = self.cache.get(cache_key)
#         if cached:
#             return cached
#         try:
#             url = "http://api.openweathermap.org/geo/1.0/direct"
#             params = {'q': city_name, 'limit': 1, 'appid': WEATHER_API_KEY}
#             with ThreadPoolExecutor(max_workers=1) as executor:
#                 future = executor.submit(self.fetch_weather, url, params)
#                 data = future.result()
#             if isinstance(data, str):
#                 return data
#             if data and len(data) > 0:
#                 city_info = {"name": data[0]["name"], "lat": data[0]["lat"], "lon": data[0]["lon"]}
#                 self.cache.setex(cache_key, 86400, city_info)
#                 return city_info
#             return None
#         except Exception as e:
#             return handle_error(e, "ë„ì‹œ ì •ë³´ ì¡°íšŒ ì¤‘", f"'{city_name}' ìœ„ì¹˜ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”. ğŸ—ºï¸")

#     def get_city_weather(self, city_name):
#         cache_key = f"weather:{city_name}"
#         cached_data = self.cache.get(cache_key)
#         if cached_data:
#             return cached_data
        
#         city_info = self.get_city_info(city_name)
#         if not city_info or isinstance(city_info, str):
#             return city_info if isinstance(city_info, str) else f"'{city_name}'ì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
#         try:
#             url = "https://api.openweathermap.org/data/2.5/weather"
#             params = {'lat': city_info["lat"], 'lon': city_info["lon"], 'appid': WEATHER_API_KEY, 'units': 'metric', 'lang': 'kr'}
#             with ThreadPoolExecutor(max_workers=1) as executor:
#                 future = executor.submit(self.fetch_weather, url, params)
#                 data = future.result()
#             if isinstance(data, str):
#                 return data
#             weather_emojis = {'Clear': 'â˜€ï¸', 'Clouds': 'â˜ï¸', 'Rain': 'ğŸŒ§ï¸', 'Snow': 'â„ï¸', 'Thunderstorm': 'â›ˆï¸', 'Drizzle': 'ğŸŒ¦ï¸', 'Mist': 'ğŸŒ«ï¸'}
#             weather_emoji = weather_emojis.get(data['weather'][0]['main'], 'ğŸŒ¤ï¸')
#             result = (
#                 f"í˜„ì¬ {data['name']}, {data['sys']['country']} ë‚ ì”¨ {weather_emoji}\n"
#                 f"ë‚ ì”¨: {data['weather'][0]['description']}\n"
#                 f"ì˜¨ë„: {data['main']['temp']}Â°C\n"
#                 f"ì²´ê°: {data['main']['feels_like']}Â°C\n"
#                 f"ìŠµë„: {data['main']['humidity']}%\n"
#                 f"í’ì†: {data['wind']['speed']}m/s\n"
#                 f"ë” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"
#             )
#             self.cache.setex(cache_key, self.cache_ttl, result)
#             return result
#         except Exception as e:
#             return handle_error(e, "í˜„ì¬ ë‚ ì”¨ ì¡°íšŒ ì¤‘", f"'{city_name}' ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆì–´ìš”. ğŸŒ¦ï¸")

#     def get_forecast_by_day(self, city_name, days_from_today=1):
#         cache_key = f"forecast:{city_name}:{days_from_today}"
#         cached_data = self.cache.get(cache_key)
#         if cached_data:
#             return cached_data
        
#         city_info = self.get_city_info(city_name)
#         if not city_info or isinstance(city_info, str):
#             return city_info if isinstance(city_info, str) else f"'{city_name}'ì˜ ë‚ ì”¨ ì˜ˆë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
#         try:
#             url = "https://api.openweathermap.org/data/2.5/forecast"
#             params = {'lat': city_info["lat"], 'lon': city_info["lon"], 'appid': WEATHER_API_KEY, 'units': 'metric', 'lang': 'kr'}
#             with ThreadPoolExecutor(max_workers=1) as executor:
#                 future = executor.submit(self.fetch_weather, url, params)
#                 data = future.result()
#             if isinstance(data, str):
#                 return data
#             target_date = (datetime.now() + timedelta(days=days_from_today)).strftime('%Y-%m-%d')
#             forecast_text = f"{city_info['name']}ì˜ {target_date} ë‚ ì”¨ ì˜ˆë³´ ğŸŒ¤ï¸\n\n"
#             weather_emojis = {'Clear': 'â˜€ï¸', 'Clouds': 'â˜ï¸', 'Rain': 'ğŸŒ§ï¸', 'Snow': 'â„ï¸', 'Thunderstorm': 'â›ˆï¸', 'Drizzle': 'ğŸŒ¦ï¸', 'Mist': 'ğŸŒ«ï¸'}
#             found = False

#             forecast_lines = []
#             for forecast in data['list']:
#                 dt = datetime.fromtimestamp(forecast['dt']).strftime('%Y-%m-%d')
#                 if dt == target_date:
#                     time_only = datetime.fromtimestamp(forecast['dt']).strftime('%H:%M')
#                     weather_emoji = weather_emojis.get(forecast['weather'][0]['main'], 'ğŸŒ¤ï¸')
#                     forecast_lines.append(
#                         f"â° {time_only} {forecast['weather'][0]['description']} {weather_emoji} "
#                         f"{forecast['main']['temp']}Â°C ğŸ’§{forecast['main']['humidity']}% ğŸŒ¬ï¸{forecast['wind']['speed']}m/s"
#                     )
#                     found = True
            
#             forecast_text += "\n\n".join(forecast_lines)
#             result = forecast_text + "\n\në” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š" if found else f"'{city_name}'ì˜ {target_date} ë‚ ì”¨ ì˜ˆë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
#             self.cache.setex(cache_key, self.cache_ttl, result)
#             return result
#         except Exception as e:
#             return handle_error(e, "ë‚ ì”¨ ì˜ˆë³´ ì¡°íšŒ ì¤‘", f"'{city_name}' ì˜ˆë³´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆì–´ìš”. â›…")

#     def get_weekly_forecast(self, city_name):
#         cache_key = f"weekly_forecast:{city_name}"
#         cached_data = self.cache.get(cache_key)
#         if cached_data:
#             return cached_data
        
#         city_info = self.get_city_info(city_name)
#         if not city_info or isinstance(city_info, str):
#             return city_info if isinstance(city_info, str) else f"'{city_name}'ì˜ ì£¼ê°„ ì˜ˆë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
#         try:
#             url = "https://api.openweathermap.org/data/2.5/forecast"
#             params = {'lat': city_info["lat"], 'lon': city_info["lon"], 'appid': WEATHER_API_KEY, 'units': 'metric', 'lang': 'kr'}
#             with ThreadPoolExecutor(max_workers=1) as executor:
#                 future = executor.submit(self.fetch_weather, url, params)
#                 data = future.result()
#             if isinstance(data, str):
#                 return data
            
#             today = datetime.now().date()
#             week_end = today + timedelta(days=6)
#             daily_forecast = {}
#             weekdays_kr = ["ì›”ìš”ì¼", "í™”ìš”ì¼", "ìˆ˜ìš”ì¼", "ëª©ìš”ì¼", "ê¸ˆìš”ì¼", "í† ìš”ì¼", "ì¼ìš”ì¼"]
#             today_weekday = today.weekday()

#             for forecast in data['list']:
#                 dt = datetime.fromtimestamp(forecast['dt']).date()
#                 if today <= dt <= week_end:
#                     dt_str = dt.strftime('%Y-%m-%d')
#                     weekday_idx = (today_weekday + (dt - today).days) % 7
#                     info = {
#                         'weekday': weekdays_kr[weekday_idx],
#                         'temp_min': forecast['main']['temp_min'],
#                         'temp_max': forecast['main']['temp_max'],
#                         'weather': forecast['weather'][0]['description']
#                     }
#                     if dt_str not in daily_forecast:
#                         daily_forecast[dt_str] = info
#                     else:
#                         daily_forecast[dt_str]['temp_min'] = min(daily_forecast[dt_str]['temp_min'], info['temp_min'])
#                         daily_forecast[dt_str]['temp_max'] = max(daily_forecast[dt_str]['temp_max'], info['temp_max'])
            
#             today_str = today.strftime('%Y-%m-%d')
#             today_weekday_str = weekdays_kr[today_weekday]
#             forecast_text = f"{today_str}({today_weekday_str}) ê¸°ì¤€ {city_info['name']}ì˜ ì£¼ê°„ ë‚ ì”¨ ì˜ˆë³´ ğŸŒ¤ï¸\n"
#             weather_emojis = {'Clear': 'â˜€ï¸', 'Clouds': 'â˜ï¸', 'Rain': 'ğŸŒ§ï¸', 'Snow': 'â„ï¸', 'Thunderstorm': 'â›ˆï¸', 'Drizzle': 'ğŸŒ¦ï¸', 'Mist': 'ğŸŒ«ï¸'}
            
#             for date, info in daily_forecast.items():
#                 weather_emoji = weather_emojis.get(info['weather'].split()[0], 'ğŸŒ¤ï¸')
#                 forecast_text += (
#                     f"{info['weekday']}: {info['weather']} {weather_emoji} "
#                     f"ìµœì € {info['temp_min']}Â°C ìµœê³  {info['temp_max']}Â°C\n"
#                 )
            
#             result = forecast_text + "\në” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"
#             self.cache.setex(cache_key, self.cache_ttl, result)
#             return result
#         except Exception as e:
#             return handle_error(e, "ì£¼ê°„ ë‚ ì”¨ ì˜ˆë³´ ì¡°íšŒ ì¤‘", f"'{city_name}' ì£¼ê°„ ì˜ˆë³´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆì–´ìš”. â˜”")

# # ì´ˆê¸°í™”
# supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
# client = Client()
# weather_api = WeatherAPI()
# naver_request_count = 0
# NAVER_DAILY_LIMIT = 25000
# st.set_page_config(page_title="AI ì±—ë´‡", page_icon="ğŸ¤–")

# # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” í•¨ìˆ˜
# def init_session_state():
#     if "is_logged_in" not in st.session_state:
#         st.session_state.is_logged_in = False
#     if "user_id" not in st.session_state:
#         st.session_state.user_id = None
#     if "chat_history" not in st.session_state:
#         st.session_state.chat_history = []
#     if "session_id" not in st.session_state:
#         st.session_state.session_id = str(uuid.uuid4())

# # ë„ì‹œ ì¶”ì¶œ
# CITY_PATTERNS = [
#     re.compile(r'(?:ì˜¤ëŠ˜|ë‚´ì¼|ëª¨ë ˆ|ì´ë²ˆ ì£¼|ì£¼ê°„)?\s*([ê°€-í£a-zA-Z\s]{2,20}(?:ì‹œ|êµ°|city)?)ì˜?\s*ë‚ ì”¨', re.IGNORECASE),
#     re.compile(r'(?:ì˜¤ëŠ˜|ë‚´ì¼|ëª¨ë ˆ|ì´ë²ˆ ì£¼|ì£¼ê°„)?\s*([ê°€-í£a-zA-Z\s]{2,20}(?:ì‹œ|êµ°|city)?)\s*ë‚ ì”¨', re.IGNORECASE),
#     re.compile(r'ë‚ ì”¨\s*(?:ì˜¤ëŠ˜|ë‚´ì¼|ëª¨ë ˆ|ì´ë²ˆ ì£¼|ì£¼ê°„)?\s*([ê°€-í£a-zA-Z\s]{2,20}(?:ì‹œ|êµ°|city)?)', re.IGNORECASE),
# ]
# def extract_city_from_query(query):
#     for pattern in CITY_PATTERNS:
#         match = pattern.search(query)
#         if match:
#             city = match.group(1).strip()
#             if city not in ["ì˜¤ëŠ˜", "ë‚´ì¼", "ëª¨ë ˆ", "ì´ë²ˆ ì£¼", "ì£¼ê°„", "í˜„ì¬"]:
#                 return city
#     return "ì„œìš¸"

# TIME_CITY_PATTERNS = [
#     re.compile(r'([ê°€-í£a-zA-Z]{2,20}(?:ì‹œ|êµ°)?)ì˜?\s*ì‹œê°„'),
#     re.compile(r'([ê°€-í£a-zA-Z]{2,20}(?:ì‹œ|êµ°)?)\s*ì‹œê°„'),
#     re.compile(r'ì‹œê°„\s*([ê°€-í£a-zA-Z\s]{2,20}(?:ì‹œ|êµ°)?)'),
# ]
# def extract_city_from_time_query(query):
#     for pattern in TIME_CITY_PATTERNS:
#         match = pattern.search(query)
#         if match:
#             city = match.group(1).strip()
#             if city != "í˜„ì¬":
#                 return city
#     return "ì„œìš¸"

# # ì‹œê°„ ì •ë³´
# def get_time_by_city(city_name="ì„œìš¸"):
#     try:
#         city_info = weather_api.get_city_info(city_name)
#         if not city_info or isinstance(city_info, str):
#             return city_info if isinstance(city_info, str) else f"'{city_name}'ì˜ ì‹œê°„ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
#         tf = TimezoneFinder()
#         timezone_str = tf.timezone_at(lng=city_info["lon"], lat=city_info["lat"]) or "Asia/Seoul"
#         timezone = pytz.timezone(timezone_str)
#         city_time = datetime.now(timezone)
#         am_pm = "ì˜¤ì „" if city_time.strftime("%p") == "AM" else "ì˜¤í›„"
#         return f"í˜„ì¬ {city_name} ì‹œê°„: {city_time.strftime('%Yë…„ %mì›” %dì¼ %p %I:%M')} â°\në” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"
#     except Exception as e:
#         return handle_error(e, "ì‹œê°„ ì •ë³´ ì¡°íšŒ ì¤‘", f"'{city_name}'ì˜ ì‹œê°„ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆì–´ìš”. â±ï¸")

# # ì‚¬ìš©ì ë° ì±„íŒ… ê¸°ë¡ ê´€ë¦¬
# def create_or_get_user(nickname):
#     try:
#         user = supabase.table("users").select("*").eq("nickname", nickname).execute()
#         if user.data:
#             return user.data[0]["id"], True
#         new_user = supabase.table("users").insert({"nickname": nickname, "created_at": datetime.now().isoformat()}).execute()
#         return new_user.data[0]["id"], False
#     except Exception as e:
#         return handle_error(e, "ì‚¬ìš©ì ìƒì„±/ì¡°íšŒ ì¤‘", "ì‚¬ìš©ì ì •ë³´ë¥¼ ì²˜ë¦¬í•˜ì§€ ëª»í–ˆì–´ìš”. ğŸ˜“"), False

# def save_chat_history(user_id, session_id, question, answer, time_taken):
#     try:
#         supabase.table("chat_history").insert({
#             "user_id": user_id, "session_id": session_id, "question": question,
#             "answer": answer, "time_taken": time_taken, "created_at": datetime.now().isoformat()
#         }).execute()
#     except Exception as e:
#         logger.error(f"ì±„íŒ… ê¸°ë¡ ì €ì¥ ì‹¤íŒ¨: {str(e)}")

# def async_save_chat_history(user_id, session_id, question, answer, time_taken):
#     threading.Thread(target=save_chat_history, args=(user_id, session_id, question, answer, time_taken)).start()

# # ì˜ì•½í’ˆ ê²€ìƒ‰
# def get_drug_info(drug_query):
#     drug_name = drug_query.replace("ì•½í’ˆê²€ìƒ‰", "").strip()
#     cache_key = f"drug:{drug_name}"
#     cached = cache_handler.get(cache_key)
#     if cached:
#         return cached
    
#     url = 'http://apis.data.go.kr/1471000/DrbEasyDrugInfoService/getDrbEasyDrugList'
#     params = {'serviceKey': DRUG_API_KEY, 'pageNo': '1', 'numOfRows': '1', 'itemName': urllib.parse.quote(drug_name), 'type': 'json'}
    
#     try:
#         with ThreadPoolExecutor(max_workers=1) as executor:
#             future = executor.submit(requests.get, url, params=params, timeout=3)
#             response = future.result()
#         response.raise_for_status()
#         data = response.json()
#         if 'body' in data and 'items' in data['body'] and data['body']['items']:
#             item = data['body']['items'][0]
#             efcy = item.get('efcyQesitm', 'ì •ë³´ ì—†ìŒ')[:150] + ("..." if len(item.get('efcyQesitm', '')) > 150 else "")
#             use_method_raw = item.get('useMethodQesitm', 'ì •ë³´ ì—†ìŒ')
#             atpn = item.get('atpnQesitm', 'ì •ë³´ ì—†ìŒ')[:150] + ("..." if len(item.get('atpnQesitm', '')) > 150 else "")
#             use_method = use_method_raw[:150] + ("..." if len(use_method_raw) > 150 else "")
#             result = (
#                 f"ğŸ’Š **ì˜ì•½í’ˆ ì •ë³´** ğŸ’Š\n\n"
#                 f"âœ… **ì•½í’ˆëª…**: {item.get('itemName', 'ì •ë³´ ì—†ìŒ')}\n\n"
#                 f"âœ… **ì œì¡°ì‚¬**: {item.get('entpName', 'ì •ë³´ ì—†ìŒ')}\n\n"
#                 f"âœ… **íš¨ëŠ¥**: {efcy}\n\n"
#                 f"âœ… **ìš©ë²•ìš©ëŸ‰**: {use_method}\n\n"
#                 f"âœ… **ì£¼ì˜ì‚¬í•­**: {atpn}\n\n"
#                 f"â„¹ï¸ ìì„¸í•œ ì •ë³´ëŠ” [ì•½í•™ì •ë³´ì›](https://www.health.kr/searchDrug/search_detail.asp)ì—ì„œ í™•ì¸í•˜ì„¸ìš”! ğŸ©º\n\n"
#                 f"ë” ê¶ê¸ˆí•œ ì  ìˆìœ¼ì‹ ê°€ìš”? ğŸ˜Š"
#             )
#             cache_handler.setex(cache_key, 86400, result)
#             return result
#         else:
#             return search_and_summarize_drug(drug_name)
#     except requests.RequestException as e:
#         logger.warning(f"ì•½í’ˆ API í˜¸ì¶œ ì‹¤íŒ¨, ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì „í™˜: {drug_name}")
#         return search_and_summarize_drug(drug_name)
#     except Exception as e:
#         return handle_error(e, "ì•½í’ˆ ì •ë³´ ì¡°íšŒ ì¤‘", f"'{drug_name}' ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆì–´ìš”. ğŸ’Š")

# def search_and_summarize_drug(drug_name):
#     try:
#         search_results = search_and_summarize(f"{drug_name} ì˜ì•½í’ˆ ì •ë³´", num_results=5)
#         if not search_results.empty:
#             return f"'{drug_name}' ê³µì‹ ì •ë³´ ì—†ìŒ. ì›¹ ê²€ìƒ‰ ìš”ì•½:\n{get_ai_summary(search_results)}"
#         return f"'{drug_name}' ì˜ì•½í’ˆ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
#     except Exception as e:
#         return handle_error(e, "ì•½í’ˆ ì›¹ ê²€ìƒ‰ ì¤‘", f"'{drug_name}' ê²€ìƒ‰ì— ì‹¤íŒ¨í–ˆì–´ìš”. ğŸ”")

# # Naver API ë° ì›¹ ê²€ìƒ‰
# def get_naver_api_results(query):
#     global naver_request_count
#     if naver_request_count >= NAVER_DAILY_LIMIT:
#         return search_and_summarize(query, num_results=5)
    
#     enc_text = urllib.parse.quote(query)
#     url = f"https://openapi.naver.com/v1/search/webkr?query={enc_text}&display=10&sort=date"
#     request = urllib.request.Request(url)
#     request.add_header("X-Naver-Client-Id", NAVER_CLIENT_ID)
#     request.add_header("X-Naver-Client-Secret", NAVER_CLIENT_SECRET)
#     results = []
    
#     try:
#         with ThreadPoolExecutor(max_workers=1) as executor:
#             future = executor.submit(urllib.request.urlopen, request)
#             response = future.result()
#         naver_request_count += 1
#         if response.getcode() == 200:
#             data = json.loads(response.read().decode('utf-8'))
#             for item in data.get('items', [])[:5]:
#                 title = re.sub(r'<b>|</b>', '', item['title'])
#                 contents = re.sub(r'<b>|</b>', '', item.get('description', 'ë‚´ìš© ì—†ìŒ'))[:100] + "..."
#                 results.append({"title": title, "contents": contents, "url": item.get('link', ''), "date": item.get('pubDate', '')})
#     except Exception:
#         logger.warning(f"Naver API í˜¸ì¶œ ì‹¤íŒ¨, ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì „í™˜: {query}")
#         return search_and_summarize(query, num_results=5)
#     return pd.DataFrame(results)

# def search_and_summarize(query, num_results=5):
#     data = []
#     with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
#         futures = [executor.submit(requests.get, link, timeout=3) for link in search(query, num_results=num_results)]
#         for future in futures:
#             try:
#                 response = future.result()
#                 soup = BeautifulSoup(response.text, 'html.parser')
#                 title = soup.title.get_text() if soup.title else "No title"
#                 description = ' '.join([p.get_text() for p in soup.find_all('p')[:3]])
#                 data.append({"title": title, "contents": description[:500], "link": response.url})
#             except Exception:
#                 continue
#     return pd.DataFrame(data)

# def process_search_result(row):
#     try:
#         return f"ì¶œì²˜: {row['title']}\në‚´ìš©: {row['contents']}"
#     except Exception:
#         return ""

# def get_ai_summary(search_results):
#     try:
#         if search_results.empty:
#             return "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
#         context = "\n".join(process_search_result(row) for _, row in search_results.iterrows())
#         with ThreadPoolExecutor(max_workers=1) as executor:
#             future = executor.submit(client.chat.completions.create, model="gpt-4o", messages=[{"role": "user", "content": f"ê²€ìƒ‰ ê²°ê³¼ë¥¼ 2~3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½:\n{context}"}])
#             response = future.result()
#         summary = response.choices[0].message.content
#         sources = "\n\nğŸ“œ **ì¶œì²˜**\n" + "\n".join(
#             [f"ğŸŒ [{row['title']}]({row.get('link', 'ë§í¬ ì—†ìŒ')})" for _, row in search_results.iterrows()]
#         )
#         return f"{summary}{sources}\n\në” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"
#     except Exception as e:
#         return handle_error(e, "ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ì¤‘", "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš”ì•½í•˜ì§€ ëª»í–ˆì–´ìš”. ğŸ˜“")

# # ë…¼ë¬¸ ê²€ìƒ‰ (ArXiv)
# def fetch_arxiv_paper(paper):
#     try:
#         return {
#             "title": paper.title,
#             "authors": ", ".join(str(a) for a in paper.authors),
#             "summary": paper.summary[:200],
#             "entry_id": paper.entry_id,
#             "pdf_url": paper.pdf_url,
#             "published": paper.published.strftime('%Y-%m-%d')
#         }
#     except Exception as e:
#         return handle_error(e, "ArXiv ë…¼ë¬¸ ì²˜ë¦¬ ì¤‘", "ë…¼ë¬¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆì–´ìš”. ğŸ“„")

# def get_arxiv_papers(query, max_results=3):
#     cache_key = f"arxiv:{query}:{max_results}"
#     cached = cache_handler.get(cache_key)
#     if cached:
#         return cached
    
#     try:
#         search_obj = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.SubmittedDate)
#         with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
#             results = list(executor.map(fetch_arxiv_paper, search_obj.results()))
#         if not results:
#             return "í•´ë‹¹ í‚¤ì›Œë“œë¡œ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
#         response = "ğŸ“š **Arxiv ë…¼ë¬¸ ê²€ìƒ‰ ê²°ê³¼** ğŸ“š\n\n"
#         response += "\n\n".join(
#             [f"**ë…¼ë¬¸ {i}**\n\n"
#              f"ğŸ“„ **ì œëª©**: {r['title']}\n\n"
#              f"ğŸ‘¥ **ì €ì**: {r['authors']}\n\n"
#              f"ğŸ“ **ì´ˆë¡**: {r['summary']}...\n\n"
#              f"ğŸ”— **ë…¼ë¬¸ í˜ì´ì§€**: {r['entry_id']}\n\n"
#              f"ğŸ“¥ **PDF ë‹¤ìš´ë¡œë“œ**: [{r['pdf_url'].split('/')[-1]}]({r['pdf_url']})\n\n"
#              f"ğŸ“… **ì¶œíŒì¼**: {r['published']}\n\n"
#              f"{'-' * 50}"
#              for i, r in enumerate(results, 1) if not isinstance(r, str)]
#         ) + "\n\në” ë§ì€ ë…¼ë¬¸ì„ ë³´ê³  ì‹¶ë‹¤ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”! ğŸ˜Š"
#         cache_handler.setex(cache_key, 3600, response)
#         return response
#     except Exception as e:
#         return handle_error(e, "ArXiv ë…¼ë¬¸ ê²€ìƒ‰ ì¤‘", "ë…¼ë¬¸ ê²€ìƒ‰ì— ì‹¤íŒ¨í–ˆì–´ìš”. ğŸ“š")

# # PubMed ë…¼ë¬¸ ê²€ìƒ‰
# base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/"

# def search_pubmed(query, max_results=5):
#     search_url = f"{base_url}esearch.fcgi"
#     params = {"db": "pubmed", "term": query, "retmode": "json", "retmax": max_results, "api_key": NCBI_KEY}
#     try:
#         with ThreadPoolExecutor(max_workers=1) as executor:
#             future = executor.submit(requests.get, search_url, params=params)
#             response = future.result()
#         response.raise_for_status()
#         return response.json()
#     except requests.RequestException as e:
#         return handle_error(e, "PubMed ê²€ìƒ‰ ì¤‘", "ì˜í•™ ë…¼ë¬¸ ê²€ìƒ‰ì— ì‹¤íŒ¨í–ˆì–´ìš”. ğŸ©º")

# def get_pubmed_summaries(id_list):
#     summary_url = f"{base_url}esummary.fcgi"
#     ids = ",".join(id_list)
#     params = {"db": "pubmed", "id": ids, "retmode": "json", "api_key": NCBI_KEY}
#     try:
#         with ThreadPoolExecutor(max_workers=1) as executor:
#             future = executor.submit(requests.get, summary_url, params=params)
#             response = future.result()
#         response.raise_for_status()
#         return response.json()
#     except requests.RequestException as e:
#         return handle_error(e, "PubMed ìš”ì•½ ì¡°íšŒ ì¤‘", "ë…¼ë¬¸ ìš”ì•½ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆì–´ìš”. ğŸ“–")

# def get_pubmed_abstract(id_list):
#     fetch_url = f"{base_url}efetch.fcgi"
#     ids = ",".join(id_list)
#     params = {"db": "pubmed", "id": ids, "retmode": "xml", "rettype": "abstract", "api_key": NCBI_KEY}
#     try:
#         with ThreadPoolExecutor(max_workers=1) as executor:
#             future = executor.submit(requests.get, fetch_url, params=params)
#             response = future.result()
#         response.raise_for_status()
#         return response.text
#     except requests.RequestException as e:
#         return handle_error(e, "PubMed ì´ˆë¡ ì¡°íšŒ ì¤‘", "ë…¼ë¬¸ ì´ˆë¡ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆì–´ìš”. ğŸ“")

# def extract_first_two_sentences(abstract_text):
#     if not abstract_text or abstract_text.isspace():
#         return "No abstract available"
#     sentences = [s.strip() for s in abstract_text.split('.') if s.strip()]
#     return " ".join(sentences[:2]) + "." if sentences else "No abstract available"

# def parse_abstracts(xml_text):
#     abstract_dict = {}
#     try:
#         root = ET.fromstring(xml_text)
#         for article in root.findall(".//PubmedArticle"):
#             pmid_elem = article.find(".//MedlineCitation/PMID")
#             abstract_elem = article.find(".//Abstract/AbstractText")
#             pmid = pmid_elem.text if pmid_elem is not None else None
#             abstract = abstract_elem.text if abstract_elem is not None else "No abstract available"
#             if pmid:
#                 abstract_dict[pmid] = extract_first_two_sentences(abstract)
#     except ET.ParseError as e:
#         logger.error(f"PubMed XML íŒŒì‹± ì˜¤ë¥˜: {e}")
#         return {}
#     return abstract_dict

# def get_pubmed_papers(query, max_results=5):
#     if "tissuge" in query.lower():
#         query = query.replace("tissuge", "tissue")
#         logger.info(f"ì˜¤íƒ€ ë³´ì •: {query}")
    
#     cache_key = f"pubmed:{query}:{max_results}"
#     cached = cache_handler.get(cache_key)
#     if cached:
#         return cached
    
#     try:
#         search_results = search_pubmed(query, max_results)
#         if isinstance(search_results, str):
#             return search_results
#         pubmed_ids = search_results["esearchresult"]["idlist"]
#         if not pubmed_ids:
#             return f"'{query}'ë¡œ ì˜í•™ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í‚¤ì›Œë“œë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”."
        
#         summaries = get_pubmed_summaries(pubmed_ids)
#         if isinstance(summaries, str):
#             return summaries
        
#         abstracts_xml = get_pubmed_abstract(pubmed_ids)
#         if isinstance(abstracts_xml, str):
#             return abstracts_xml
#         abstract_dict = parse_abstracts(abstracts_xml)
        
#         response = "ğŸ“š **PubMed ì˜í•™ ë…¼ë¬¸ ê²€ìƒ‰ ê²°ê³¼** ğŸ“š\n\n"
#         for i, pmid in enumerate(pubmed_ids, 1):
#             summary = summaries['result'].get(pmid, {})
#             title = summary.get('title', 'No title available')
#             pubdate = summary.get('pubdate', 'No date available')
#             authors = ", ".join(author.get('name', '') for author in summary.get('authors', [])) or "No authors available"
#             doi = next((aid['value'] for aid in summary.get('articleids', []) if aid['idtype'] == 'doi'), None)
#             link = f"https://doi.org/{doi}" if doi else f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/"
#             abstract = abstract_dict.get(pmid, "No abstract available")
            
#             response += (
#                 f"**ë…¼ë¬¸ {i}**\n\n"
#                 f"ğŸ†” **PMID**: {pmid}\n\n"
#                 f"ğŸ“– **ì œëª©**: {title}\n\n"
#                 f"ğŸ“… **ì¶œíŒì¼**: {pubdate}\n\n"
#                 f"âœï¸ **ì €ì**: {authors}\n\n"
#                 f"ğŸ”— **ë§í¬**: {link}\n\n"
#                 f"ğŸ“ **ì´ˆë¡**: {abstract}\n\n"
#                 f"{'-' * 50}\n\n"
#             )
        
#         response += "ë” ë§ì€ ì˜í•™ ë…¼ë¬¸ì„ ë³´ê³  ì‹¶ë‹¤ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”! ğŸ˜Š"
#         cache_handler.setex(cache_key, 3600, response)
#         return response
#     except Exception as e:
#         return handle_error(e, "PubMed ë…¼ë¬¸ ê²€ìƒ‰ ì¤‘", "ì˜í•™ ë…¼ë¬¸ ê²€ìƒ‰ì— ì‹¤íŒ¨í–ˆì–´ìš”. ğŸ©º")

# # ëŒ€í™”í˜• ì‘ë‹µ
# conversation_cache = MemoryCache()
# def get_conversational_response(query, chat_history):
#     cache_key = f"conv:{needs_search(query)}:{query}:{hash(str(chat_history[-5:]))}"
#     cached = conversation_cache.get(cache_key)
#     if cached:
#         return cached
    
#     emoji_list = (
#         "ì²´í¬ë¦¬ìŠ¤íŠ¸ ì´ëª¨ì§€:\nâœ… ì™„ë£Œëœ í•­ëª© | â˜‘ï¸ ì²´í¬ ìƒì (ì²´í¬ë¨) | âœ“ ì²´í¬ í‘œì‹œ | âœ”ï¸ êµµì€ ì²´í¬ í‘œì‹œ | âŒ ì·¨ì†Œ/ì‹¤íŒ¨ í•­ëª© | â¬œ ë¹ˆ ìƒì | âšª ë¹ˆ ì› | ğŸ”˜ ë¼ë””ì˜¤ ë²„íŠ¼ | ğŸ“Œ í•€ìœ¼ë¡œ ê³ ì •ëœ í•­ëª© | ğŸš€ ì‹œì‘/ì¶œì‹œ í•­ëª©\n\n"
#         "ìƒíƒœ í‘œì‹œ ì´ëª¨ì§€:\nğŸŸ¢ ë…¹ìƒ‰ ì› (ì„±ê³µ/í™œì„±) | ğŸ”´ ë¹¨ê°„ ì› (ì‹¤íŒ¨/ì¤‘ìš”) | ğŸŸ¡ ë…¸ë€ ì› (ì£¼ì˜/ì§„í–‰ ì¤‘) | ğŸ”„ ì—…ë°ì´íŠ¸/ì§„í–‰ ì¤‘ | â±ï¸ ëŒ€ê¸° ì¤‘/ì‹œê°„ ê´€ë ¨ | ğŸ” ê²€í†  ì¤‘/ê²€ìƒ‰\n\n"
#         "ê°œë°œ ê´€ë ¨ ì´ëª¨ì§€:\nğŸ’» ì½”ë“œ/í”„ë¡œê·¸ë˜ë° | ğŸ”§ ë„êµ¬/ì„¤ì • | ğŸ› ë²„ê·¸ | ğŸ“¦ íŒ¨í‚¤ì§€/ëª¨ë“ˆ | ğŸ“ ë¬¸ì„œ/ë…¸íŠ¸ | ğŸ—‚ï¸ í´ë”/ë¶„ë¥˜ | âš™ï¸ ì„¤ì •/êµ¬ì„± | ğŸ”’ ë³´ì•ˆ/ì ê¸ˆ | ğŸ“Š ë°ì´í„°/í†µê³„ | ğŸ“ˆ ì„±ì¥/ì¦ê°€\n\n"
#         "ì„¹ì…˜ êµ¬ë¶„ ì´ëª¨ì§€:\nğŸ“‹ ëª©ë¡/ì²´í¬ë¦¬ìŠ¤íŠ¸ | ğŸ“š ì±…/ë¬¸ì„œ | ğŸ’¡ ì•„ì´ë””ì–´/íŒ | âš ï¸ ì£¼ì˜/ê²½ê³  | ğŸ¯ ëª©í‘œ/íƒ€ê²Ÿ | ğŸ”— ë§í¬/ì—°ê²° | ğŸ‘¥ ì‚¬ìš©ì/íŒ€ | ğŸ“… ì¼ì •/ìº˜ë¦°ë”\n\n"
#         "ê¸°íƒ€ ìœ ìš©í•œ ì´ëª¨ì§€:\nğŸŒŸ í•˜ì´ë¼ì´íŠ¸/ì¤‘ìš” í•­ëª© | âœ¨ íŠ¹ë³„/ê°œì„  í•­ëª© | ğŸ“± ëª¨ë°”ì¼ | ğŸ–¥ï¸ ë°ìŠ¤í¬í†± | ğŸ—ï¸ ì•„í‚¤í…ì²˜ | ğŸš§ ì‘ì—… ì¤‘ | ğŸ’¬ ì˜ê²¬/ì½”ë©˜íŠ¸ | ğŸŒ ì›¹/ê¸€ë¡œë²Œ | ğŸ“¤ ë°°í¬/ì—…ë¡œë“œ | ğŸ“¥ ë‹¤ìš´ë¡œë“œ/ìˆ˜ì‹ "
#     )
    
#     messages = [
#         {"role": "system", "content": (
#             "ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ìƒí˜¸ì‘ìš©ì ì¸ AI ì±—ë´‡ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µí•˜ê³ , í•„ìš”í•˜ë©´ ì¶”ê°€ ì§ˆë¬¸ì„ ë˜ì ¸ ëŒ€í™”ë¥¼ ì´ì–´ê°€ì„¸ìš”. "
#             "ì‘ë‹µ ë‚´ìš©ì— ë”°ë¼ ì ì ˆí•œ ì´ëª¨ì§€ë¥¼ ì•„ë˜ ëª©ë¡ì—ì„œ ì„ íƒí•´ ìì—°ìŠ¤ëŸ½ê²Œ ì‚½ì…í•˜ì„¸ìš”. ì´ëª¨ì§€ëŠ” ë¬¸ë§¥ì— ë§ê²Œ ì‚¬ìš©í•˜ê³ , ê³¼ë„í•˜ê²Œ ë§ì§€ ì•Šë„ë¡ í•˜ì„¸ìš”:\n\n" + emoji_list
#         )}] + [
#         {"role": msg["role"], "content": msg["content"]} for msg in chat_history[-5:]
#     ] + [{"role": "user", "content": query}]
    
#     try:
#         response = client.chat.completions.create(model="gpt-4", messages=messages)
#         result = response.choices[0].message.content
#         conversation_cache.setex(cache_key, 3600, result)
#         return result
#     except Exception as e:
#         return handle_error(e, "GPT ëŒ€í™” ìƒì„± ì¤‘", "ëŒ€í™”ë¥¼ ìƒì„±í•˜ëŠ” ë° ë¬¸ì œê°€ ìƒê²¼ì–´ìš”. ğŸ˜“ ì ì‹œ í›„ ë‹¤ì‹œ ë¬¼ì–´ë³´ì„¸ìš”!")

# GREETING_RESPONSES = {
#     "ì•ˆë…•": "ì•ˆë…•í•˜ì„¸ìš”! ë°˜ê°‘ìŠµë‹ˆë‹¤! ğŸ˜Š",
#     "ì•ˆë…• ë°˜ê°€ì›Œ": "ì•ˆë…•í•˜ì„¸ìš”! ì €ë„ ë°˜ê°‘ìŠµë‹ˆë‹¤! ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì–´ë– ì‹ ê°€ìš”? ğŸ˜„",
#     "í•˜ì´": "í•˜ì´! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ğŸ˜Š",
#     "í—¬ë¡œ": "ì•ˆë…•í•˜ì„¸ìš”! ë°˜ê°‘ìŠµë‹ˆë‹¤! ğŸ˜Š",
#     "í—¤ì´": "í—¤ì´! ì˜ ì§€ë‚´ì„¸ìš”? ğŸ˜„",
#     "ì™“ì—…": "ì™“ì—…! ë­í•˜ê³  ê³„ì‹ ê°€ìš”? ğŸ˜Š",
#     "ì™“ì¹": "ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì–´ë– ì‹ ê°€ìš”? ğŸ˜„",
# }

# # ì¿¼ë¦¬ ë¶„ë¥˜
# def needs_search(query):
#     query_lower = query.strip().lower()
#     greeting_keywords = ["ì•ˆë…•", "í•˜ì´", "ë°˜ê°€ì›Œ", "ì•ˆë‡½", "ë­í•´", "í—¬ë¡œ", "í—¬ë¡±", "í•˜ì‡", "í—¤ì´", "í—¤ì´ìš”", "ì™“ì—…", "ì™“ì¹", "ì—ì´ìš”"]
#     emotion_keywords = ["ë°°ê³ í”„ë‹¤", "ë°°ê³ í”„", "ì¡¸ë¦¬ë‹¤", "í”¼ê³¤í•˜ë‹¤", "í™”ë‚¨", "ì—´ë°›ìŒ", "ì§œì¦ë‚¨", "í”¼ê³¤í•¨"]
#     if any(greeting in query_lower for greeting in greeting_keywords) or \
#        any(emo in query_lower for emo in emotion_keywords) or \
#        len(query_lower) <= 3 and "?" not in query_lower:
#         return "conversation"
#     intent_keywords = ["ì¶”ì²œí•´ì¤˜", "ë­ ë¨¹ì„ê¹Œ", "ë©”ë‰´", "ë­í• ê¹Œ"]
#     if any(kw in query_lower for kw in intent_keywords):
#         return "conversation"
#     time_keywords = ["í˜„ì¬ ì‹œê°„", "ì‹œê°„", "ëª‡ ì‹œ", "ì§€ê¸ˆ", "ëª‡ì‹œ", "ëª‡ ì‹œì•¼", "ì§€ê¸ˆ ì‹œê°„", "í˜„ì¬", "ì‹œê³„"]
#     if any(keyword in query_lower for keyword in time_keywords) and \
#        any(timeword in query_lower for timeword in ["ì‹œê°„", "ëª‡ì‹œ", "ëª‡ ì‹œ", "ì‹œê³„"]):
#         return "time"
#     weather_keywords = ["ë‚ ì”¨", "ì˜¨ë„", "ê¸°ì˜¨"]
#     if any(keyword in query_lower for keyword in weather_keywords) and "ë‚´ì¼" in query_lower:
#         return "tomorrow_weather"
#     elif any(keyword in query_lower for keyword in weather_keywords) and "ëª¨ë ˆ" in query_lower:
#         return "day_after_tomorrow_weather"
#     elif any(keyword in query_lower for keyword in weather_keywords) and ("ì£¼ê°„" in query_lower or any(kw in query_lower for kw in ["ì´ë²ˆ ì£¼", "ì£¼ê°„ ì˜ˆë³´", "ì£¼ê°„ ë‚ ì”¨"])):
#         return "weekly_forecast"
#     elif any(keyword in query_lower for keyword in weather_keywords):
#         return "weather"
#     drug_keywords = ["ì•½í’ˆê²€ìƒ‰"]
#     drug_pattern = r'^ì•½í’ˆê²€ìƒ‰\s+[ê°€-í£a-zA-Z]{2,10}(?:ì•½|ì •|ì‹œëŸ½|ìº¡ìŠ)?$'
#     if any(keyword in query_lower for keyword in drug_keywords) and re.match(drug_pattern, query_lower):
#         return "drug"
#     if query_lower == "mbti ê²€ì‚¬":
#         return "mbti"
#     if query_lower == "ë‹¤ì¤‘ì§€ëŠ¥ ê²€ì‚¬":
#         return "multi_iq"
#     arxiv_keywords = ["ë…¼ë¬¸ê²€ìƒ‰", "arxiv", "paper", "research"]
#     if any(kw in query_lower for kw in arxiv_keywords) and len(query_lower) > 5:
#         return "arxiv_search"
#     pubmed_keywords = ["ì˜í•™ë…¼ë¬¸"]
#     if any(kw in query_lower for kw in pubmed_keywords) and len(query_lower) > 5:
#         return "pubmed_search"
#     search_keywords = ["ê²€ìƒ‰", "ì•Œë ¤ì¤˜", "ì •ë³´", "ë­ì•¼", "ë¬´ì—‡ì´ì•¼", "ë¬´ì—‡ì¸ì§€", "ì°¾ì•„ì„œ", "ì •ë¦¬í•´ì¤˜", "ì„¤ëª…í•´ì¤˜", "ì•Œê³ ì‹¶ì–´", "ì•Œë ¤ì¤„ë˜", "ì•Œì•„", "ë­ëƒ", "ì•Œë ¤ì¤˜", "ì°¾ì•„ì¤˜"]
#     if any(kw in query_lower for kw in search_keywords) and len(query_lower) > 5:
#         return "web_search"
#     return "general_query"

# # UI í•¨ìˆ˜
# def show_login_page():
#     st.title("ë¡œê·¸ì¸ ğŸ¤—")
#     with st.form("login_form"):
#         nickname = st.text_input("ë‹‰ë„¤ì„", placeholder="ì˜ˆ: í›„ì•ˆ")
#         submit_button = st.form_submit_button("ì‹œì‘í•˜ê¸° ğŸš€")
        
#         if submit_button:
#             if nickname:
#                 try:
#                     user_id, is_existing = create_or_get_user(nickname)
#                     if isinstance(user_id, str):
#                         st.toast(user_id, icon="âŒ")
#                         return
#                     st.session_state.user_id = user_id
#                     st.session_state.is_logged_in = True
#                     st.session_state.chat_history = []
#                     st.session_state.session_id = str(uuid.uuid4())
                    
#                     if is_existing:
#                         st.toast(f"í™˜ì˜í•©ë‹ˆë‹¤, {nickname}ë‹˜! ğŸ‰")
#                     else:
#                         st.toast(f"ìƒˆë¡œìš´ ì‚¬ìš©ìë¡œ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤. í™˜ì˜í•©ë‹ˆë‹¤, {nickname}ë‹˜! ğŸ‰")
#                     time.sleep(1)
#                     st.rerun()
#                 except Exception as e:
#                     st.toast(handle_error(e, "ë¡œê·¸ì¸ ì²˜ë¦¬ ì¤‘", "ë¡œê·¸ì¸ì— ì‹¤íŒ¨í–ˆì–´ìš”. ğŸ˜“"), icon="âŒ")
#             else:
#                 st.toast("ë‹‰ë„¤ì„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", icon="âš ï¸")

# @st.cache_data(ttl=3600, show_spinner=False)
# def get_cached_response(query):
#     return process_query(query)

# def fetch_city_weather(city, weather_api):
#     return weather_api.get_city_weather(city)

# def process_query(query):
#     init_session_state()
#     query_type = needs_search(query)
    
#     try:
#         if query_type == "mbti":
#             return (
#                 "MBTI ê²€ì‚¬ë¥¼ ì›í•˜ì‹œë‚˜ìš”? âœ¨ ì•„ë˜ ì‚¬ì´íŠ¸ì—ì„œ ë¬´ë£Œë¡œ ì„±ê²© ìœ í˜• ê²€ì‚¬ë¥¼ í•  ìˆ˜ ìˆì–´ìš”! ğŸ˜Š\n"
#                 "[16Personalities MBTI ê²€ì‚¬](https://www.16personalities.com/ko/%EB%AC%B4%EB%A3%8C-%EC%84%B1%EA%B2%A9-%EC%9C%A0%ED%98%95-%EA%B2%80%EC%82%AC) ğŸŒŸ\n"
#                 "ì´ ì‚¬ì´íŠ¸ëŠ” 16ê°€ì§€ ì„±ê²© ìœ í˜•ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ í…ŒìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ë©°, ê²°ê³¼ì— ë”°ë¼ ì„±ê²© ì„¤ëª…ê³¼ ì¸ê°„ê´€ê³„ ì¡°ì–¸ ë“±ì„ í™•ì¸í•  ìˆ˜ ìˆì–´ìš”! ğŸ§ ğŸ’¡"
#             )
#         elif query_type == "multi_iq":
#             return (
#                 "ë‹¤ì¤‘ì§€ëŠ¥ ê²€ì‚¬ë¥¼ ì›í•˜ì‹œë‚˜ìš”? ğŸ‰ ì•„ë˜ ì‚¬ì´íŠ¸ì—ì„œ ë¬´ë£Œë¡œ ë‹¤ì¤‘ì§€ëŠ¥ í…ŒìŠ¤íŠ¸ë¥¼ í•´ë³¼ ìˆ˜ ìˆì–´ìš”! ğŸ˜„\n"
#                 "[Multi IQ Test](https://multiiqtest.com/) ğŸš€\n"
#                 "ì´ ì‚¬ì´íŠ¸ëŠ” í•˜ì›Œë“œ ê°€ë“œë„ˆì˜ ë‹¤ì¤‘ì§€ëŠ¥ ì´ë¡ ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ í…ŒìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ë©°, ë‹¤ì–‘í•œ ì§€ëŠ¥ ì˜ì—­ì„ í‰ê°€í•´ì¤ë‹ˆë‹¤! ğŸ“šâœ¨"
#             )
#         elif query_type == "time":
#             city = extract_city_from_time_query(query)
#             return get_time_by_city(city)
#         elif query_type == "weather":
#             cities = [city.strip() for city in query.split("ì™€") if "ë‚ ì”¨" in city] or [extract_city_from_query(query)]
#             results = []
#             q = queue.Queue()
#             for city in cities:
#                 q.put(city)
            
#             def worker():
#                 while not q.empty():
#                     try:
#                         city = q.get()
#                         result = fetch_city_weather(city, weather_api)
#                         results.append(result)
#                         q.task_done()
#                     except Exception as e:
#                         results.append(handle_error(e, f"{city} ë‚ ì”¨ ì¡°íšŒ ì¤‘", f"'{city}' ë‚ ì”¨ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆì–´ìš”. ğŸŒ§ï¸"))
            
#             with ThreadPoolExecutor(max_workers=min(len(cities), MAX_WORKERS)) as executor:
#                 for _ in range(min(len(cities), MAX_WORKERS)):
#                     executor.submit(worker)
#             q.join()
#             return "\n\n".join(results)
#         elif query_type == "tomorrow_weather":
#             city = extract_city_from_query(query)
#             return weather_api.get_forecast_by_day(city, days_from_today=1)
#         elif query_type == "day_after_tomorrow_weather":
#             city = extract_city_from_query(query)
#             return weather_api.get_forecast_by_day(city, days_from_today=2)
#         elif query_type == "weekly_forecast":
#             city = extract_city_from_query(query)
#             return weather_api.get_weekly_forecast(city)
#         elif query_type == "drug":
#             return get_drug_info(query)
#         elif query_type == "conversation":
#             if query.strip() in GREETING_RESPONSES:
#                 return GREETING_RESPONSES[query.strip()]
#             return get_conversational_response(query, st.session_state.chat_history)
#         elif query_type == "web_search":
#             language = detect(query)
#             if language == 'ko' and naver_request_count < NAVER_DAILY_LIMIT:
#                 return get_ai_summary(get_naver_api_results(query))
#             return get_ai_summary(search_and_summarize(query))
#         elif query_type == "arxiv_search":
#             keywords = query.replace("ë…¼ë¬¸ê²€ìƒ‰", "").replace("arxiv", "").replace("paper", "").replace("research", "").strip()
#             return get_arxiv_papers(keywords)
#         elif query_type == "pubmed_search":
#             keywords = query.replace("ì˜í•™ë…¼ë¬¸", "").strip()
#             return get_pubmed_papers(keywords)
#         elif query_type == "general_query":
#             return get_conversational_response(query, st.session_state.chat_history)
#         return "ì•„ì§ ì§€ì›í•˜ì§€ ì•ŠëŠ” ê¸°ëŠ¥ì´ì—ìš”. ğŸ˜…"
#     except Exception as e:
#         return handle_error(e, "ì¿¼ë¦¬ ì²˜ë¦¬ ì¤‘", "ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ìƒê²¼ì–´ìš”. ğŸ˜¥ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”!")

# def show_chat_dashboard():
#     st.title("AI ì±—ë´‡ ğŸ¤–")
#     init_session_state()
    
#     if st.button("ë„ì›€ë§ â„¹ï¸"):
#         st.info(
#             "ì±—ë´‡ê³¼ ë” ì‰½ê²Œ ëŒ€í™”í•˜ëŠ” ë°©ë²•ì´ì—ìš”! ğŸ‘‡:\n\n"
#             "1. **ì•½í’ˆê²€ìƒ‰** ğŸ’Š: 'ì•½í’ˆê²€ìƒ‰ [ì•½ ì´ë¦„]' (ì˜ˆ: ì•½í’ˆê²€ìƒ‰ íƒ€ì´ë ˆë†€ì •)\n"
#             "2. **ë…¼ë¬¸ê²€ìƒ‰ (ArXiv)** ğŸ“š: 'ë…¼ë¬¸ê²€ìƒ‰ [í‚¤ì›Œë“œ]' (ì˜ˆ: ë…¼ë¬¸ê²€ìƒ‰ machine learning)\n"
#             "3. **ì˜í•™ë…¼ë¬¸ê²€ìƒ‰ (PubMed)** ğŸ©º: 'ì˜í•™ë…¼ë¬¸ [í‚¤ì›Œë“œ]' (ì˜ˆ: ì˜í•™ë…¼ë¬¸ gene therapy)\n"
#             "4. **ë‚ ì”¨ê²€ìƒ‰** â˜€ï¸: '[ë„ì‹œëª…] ë‚ ì”¨' ë˜ëŠ” 'ë‚´ì¼ [ë„ì‹œëª…] ë‚ ì”¨' (ì˜ˆ: ì„œìš¸ ë‚ ì”¨, ë‚´ì¼ ì„œìš¸ ë‚ ì”¨)\n"
#             "5. **ì‹œê°„ê²€ìƒ‰** â±ï¸: '[ë„ì‹œëª…] ì‹œê°„' (ì˜ˆ: íŒŒë¦¬ ì‹œê°„, ë‰´ìš• ì‹œê°„)\n\n"
#             "ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ ì–¸ì œë“  ì§ˆë¬¸í•´ì£¼ì„¸ìš”! ğŸ˜Š"
#         )
    
#     for msg in st.session_state.chat_history[-10:]:
#         with st.chat_message(msg['role']):
#             st.markdown(msg['content'], unsafe_allow_html=True)
    
#     if user_prompt := st.chat_input("ì§ˆë¬¸í•´ ì£¼ì„¸ìš”!"):
#         st.chat_message("user").markdown(user_prompt)
#         st.session_state.chat_history.append({"role": "user", "content": user_prompt})
#         with st.chat_message("assistant"):
#             with st.spinner("ì‘ë‹µì„ ì¤€ë¹„ ì¤‘ì´ì—ìš”..."):
#                 start_time = time.time()
#                 try:
#                     response = get_cached_response(user_prompt)
#                     time_taken = round(time.time() - start_time, 2)
#                     st.markdown(response, unsafe_allow_html=True)
#                     st.session_state.chat_history.append({"role": "assistant", "content": response})
#                     async_save_chat_history(st.session_state.user_id, st.session_state.session_id, user_prompt, response, time_taken)
#                 except Exception as e:
#                     error_msg = handle_error(e, "ëŒ€í™” ì²˜ë¦¬ ì¤‘", "ì‘ë‹µì„ ì¤€ë¹„í•˜ë‹¤ ë¬¸ì œê°€ ìƒê²¼ì–´ìš”. ğŸ˜“")
#                     st.markdown(error_msg, unsafe_allow_html=True)
#                     st.session_state.chat_history.append({"role": "assistant", "content": error_msg})

# # ë©”ì¸ ì‹¤í–‰
# def main():
#     init_session_state()
#     if not st.session_state.is_logged_in:
#         show_login_page()
#     else:
#         show_chat_dashboard()

# if __name__ == "__main__":
#     main()
